{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chap12.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/varinen/handson-unsupervised-learning/blob/master/Chap12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMUZNdZ68zLn",
        "colab_type": "text"
      },
      "source": [
        "# Generative Adversarial Networks\n",
        "\n",
        "## GANs, the Concept\n",
        "\n",
        "GANs were introduced by Ian Goodfellow and colleagues at the University of Montreal in 2014. In the core of the idea are two neural networks. One is known as the *generator*. It produces data based on the original samples provided as an input. The other network known as the *discriminator* tries to guess which of the supplied data samples belong to the original distribution and which have been generated. The two networks are locked in a zero-sum game, one network trying to fool the discriminator and the other trying to call out the data generated by the generator as fake.\n",
        "\n",
        "GANs is an unsupervised approach. The generator is capable of learning the original distribution without any labels. The learning is done by using fewer parameters than the input data has. This forces the generator to learn the most salient properties of the input. Each subsequent layer in the generator learns a representation of the data coming from the preceding layer and in each step more complex and abstract properties are discovered and learned. \n",
        "\n",
        "The discriminator forces the generator to learn a representation that will allow generating of data almost identical to the original.\n",
        "\n",
        "## The Power of GANs\n",
        "\n",
        "In the previous chapters, we have already use DBN to generate additional training data for a supervised algorithm. GANs are particularly effective in generating synthetic data that appears real. So,  in order to efficiently train a supervised image recognition  algorithm, we need a lot of training data. We can take an available smaller dataset, run it through a GAN and then use the generator to produce as much training data as we need.\n",
        "\n",
        "GANs can be powerful in anomaly  detection. We can set up a GAN so that the fakes caught by the discriminator as \"likely synthetic data\" are considered  anomalies. This can be applied to, for example, credit fraud card detection.\n",
        "\n",
        "## Deep Convolutional GANs\n",
        "\n",
        "In this  chapter, we will use the MNIST dataset to generate synthetic data using GANs. We then will apply a supervised learning model to perform image classification.\n",
        "\n",
        "This version of GAN is called a *deep convolutional generative adversarial  network (DCGAN)*, DCGANs have been first introduced in late 2015 by Alec Radform, Luke Metz, and Soumith Chinala.\n",
        "\n",
        "## Convolutional Neural Networks\n",
        "\n",
        "Compared to text and numerical data, images and video are computationally much more intensive. A single 4k Ultra HD video frame has dimensions of $4096 \\times 2160 \\times 3$ (26_452_080). Training a network on an image like thin in full resolution will require a network of millions of neurons and will take a lot of time.\n",
        "\n",
        "Instead of building a network for raw images, we can use images reduced in dimensions. The idea behind such a reduction is that pixels in an image relate to pixels nearest to them and not to pixels elsewhere.\n",
        "\n",
        "*Convolution* is a process that filters the image to decrease the size of an image without losing the relationships between pixels.\n",
        "\n",
        "The convolution process applies a  filter  to a small image area known as *kernel size* and moving this area with a small step (*stride*). In the end, the max pixel is taken from each of the areas to build a much smaller version of an image. This last step is known as *max pooling*.\n",
        "\n",
        "Let's build a CNN and use it to perform image classification on the MNIST dataset. First, we need to input the required libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lKtO5klEuBb",
        "colab_type": "code",
        "outputId": "399e48b7-80c1-4950-d7e3-be4ea4d827b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "'''Main'''\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, time, re\n",
        "import pickle, gzip, datetime\n",
        "\n",
        "'''Data Viz'''\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "color = sns.color_palette()\n",
        "import matplotlib as mpl\n",
        "from mpl_toolkits.axes_grid1 import Grid\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "'''Data Prep and Model Evaluation'''\n",
        "from sklearn import preprocessing as pp\n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.model_selection import StratifiedKFold \n",
        "from sklearn.metrics import log_loss, accuracy_score\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score, mean_squared_error\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "'''Algos'''\n",
        "import lightgbm as lgb\n",
        "\n",
        "'''TensorFlow and Keras'''\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Activation, Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
        "from keras.layers import LeakyReLU, Reshape, UpSampling2D, Conv2DTranspose\n",
        "from keras.layers import BatchNormalization, Input, Lambda\n",
        "from keras.layers import Embedding, Flatten, dot\n",
        "from keras import regularizers\n",
        "from keras.losses import mse, binary_crossentropy\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_r0rD--FCWq",
        "colab_type": "text"
      },
      "source": [
        "Next, we will load the MNIST data and store it in a 4D tensor that Keras needs to work with. We will take labels and convert them to one-hot-vectors using the `to_categorical` function in Keras.\n",
        "For later use, we will store the dataset in Pandas DataFrames. We will also reuse the `show_digit` function from previous chapters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dd4FRtGvE4FY",
        "colab_type": "code",
        "outputId": "2325096d-a130-475a-d96a-3c6d3bbd261d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swwLVi_2Fjma",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "current_path = 'gdrive/My Drive/Colab Notebooks/datasets/mnist/'\n",
        "file  = 'mnist.pkl.gz'\n",
        "\n",
        "f = gzip.open(current_path+file, 'rb')\n",
        "train_set, validation_set, test_set = pickle.load(f, encoding='latin1')\n",
        "f.close()\n",
        "\n",
        "X_train, y_train = train_set[0], train_set[1]\n",
        "X_validation, y_validation = validation_set[0], validation_set[1]\n",
        "X_test, y_test = test_set[0], test_set[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p16HkXY9F7rS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_keras = X_train.reshape(50000,28,28,1)\n",
        "X_validation_keras = X_validation.reshape(10000,28,28,1)\n",
        "X_test_keras = X_test.reshape(10000,28,28,1)\n",
        "\n",
        "y_train_keras = to_categorical(y_train)\n",
        "y_validation_keras = to_categorical(y_validation)\n",
        "y_test_keras = to_categorical(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6BDntosGDza",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create Pandas DataFrames from the datasets\n",
        "train_index = range(0,len(X_train))\n",
        "validation_index = range(len(X_train),len(X_train)+len(X_validation))\n",
        "test_index = range(len(X_train)+len(X_validation),len(X_train)+ \\\n",
        "                   len(X_validation)+len(X_test))\n",
        "\n",
        "X_train = pd.DataFrame(data=X_train,index=train_index)\n",
        "y_train = pd.Series(data=y_train,index=train_index)\n",
        "\n",
        "X_validation = pd.DataFrame(data=X_validation,index=validation_index)\n",
        "y_validation = pd.Series(data=y_validation,index=validation_index)\n",
        "\n",
        "X_test = pd.DataFrame(data=X_test,index=test_index)\n",
        "y_test = pd.Series(data=y_test,index=test_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guuBaLsrGRja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def view_digit(X, y, example):\n",
        "    label = y.loc[example]\n",
        "    image = X.loc[example,:].values.reshape([28,28])\n",
        "    plt.title('Example: %d  Label: %d' % (example, label))\n",
        "    plt.imshow(image, cmap=plt.get_cmap('gray'))\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OD98J53sGTGA",
        "colab_type": "code",
        "outputId": "5c8e8adc-b42f-440d-b61a-ec90990ccaa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "# View the first digit\n",
        "view_digit(X_train, y_train, 0)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5fCbZer/ARUPJ6ewVFZ+AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeK9r9u9GncZ",
        "colab_type": "text"
      },
      "source": [
        "In order to use the GPU, we need to change the Colab Runtime Type to \"Hardware Accelerator: GPU\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1a6zXQOtGUMy",
        "colab_type": "code",
        "outputId": "e20c365f-aee3-4e31-b9e5-bd9a57c320a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Confirm use of GPU\n",
        "import tensorflow as tf\n",
        "if tf.test.gpu_device_name():\n",
        "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
        "else: print(\"Please install GPU version of TF\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Default GPU Device: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVT-ZmjIG2P-",
        "colab_type": "text"
      },
      "source": [
        "### GAN Design\n",
        "\n",
        "We start building the GAN by calling `sequential()` in Keras. We will add two convolutional layers, each with 32 filters and a kernel size of 5 by 5, a default stride of 1, and ReLU activation.\n",
        "\n",
        "We will also perform dropout to safeguard against overfitting. The dropout will be 25%.\n",
        "\n",
        "In the next stage, we will add two more convolutional layers, each with 64 filters and a kernel size of 3 by 3. We will also do a max pooling with a window of 2 by 2 and a stride of 2. We will add a dropout layer with a dropout value of 25% of the input units.\n",
        "\n",
        "Finally, we will flatten the images, add a regular neural network with 256 hidden units, perform dropout of 50% and perform a 10-class classification using the softmax function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgAWKvIiGWLR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "# First stage\n",
        "model.add(Conv2D(filters = 32, kernel_size = (5,5), padding = 'Same', \n",
        "                 activation ='relu', input_shape = (28,28,1)))\n",
        "model.add(Conv2D(filters = 32, kernel_size = (5,5), padding = 'Same', \n",
        "                 activation ='relu'))\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "\n",
        "# Second stage\n",
        "model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', \n",
        "                 activation ='relu'))\n",
        "model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', \n",
        "                 activation ='relu'))\n",
        "\n",
        "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# Flatten images and feed to a dense layer:\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation = \"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10, activation = \"softmax\"))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZFz08g9K2t0",
        "colab_type": "text"
      },
      "source": [
        "To train this CNN we will use the *Adam optimizer* and minimize cross-entropy. We will also store the image classification accuracy as the evaluation metric.\n",
        "\n",
        "Let's train the model for 100 epochs and evaluate the results on the validation set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWROOSLLKdDg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train CNN\n",
        "model.compile(optimizer='adam', \n",
        "              loss='categorical_crossentropy', \n",
        "              metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-ykEqn1LPHH",
        "colab_type": "code",
        "outputId": "3c63cb39-94c1-4a62-88c9-a8eb475b883f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "cnn_history = model.fit(X_train_keras, y_train_keras, \n",
        "          validation_data=(X_validation_keras, y_validation_keras), \\\n",
        "          epochs=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0708 15:21:29.757631 140216533751680 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "50000/50000 [==============================] - 30s 593us/step - loss: 0.1930 - acc: 0.9394 - val_loss: 0.0422 - val_acc: 0.9879\n",
            "Epoch 2/100\n",
            "50000/50000 [==============================] - 24s 473us/step - loss: 0.0709 - acc: 0.9789 - val_loss: 0.0337 - val_acc: 0.9897\n",
            "Epoch 3/100\n",
            "50000/50000 [==============================] - 23s 470us/step - loss: 0.0545 - acc: 0.9838 - val_loss: 0.0318 - val_acc: 0.9915\n",
            "Epoch 4/100\n",
            "50000/50000 [==============================] - 23s 469us/step - loss: 0.0440 - acc: 0.9870 - val_loss: 0.0315 - val_acc: 0.9913\n",
            "Epoch 5/100\n",
            "50000/50000 [==============================] - 23s 466us/step - loss: 0.0401 - acc: 0.9877 - val_loss: 0.0257 - val_acc: 0.9928\n",
            "Epoch 6/100\n",
            "50000/50000 [==============================] - 23s 469us/step - loss: 0.0364 - acc: 0.9893 - val_loss: 0.0272 - val_acc: 0.9924\n",
            "Epoch 7/100\n",
            "50000/50000 [==============================] - 23s 468us/step - loss: 0.0323 - acc: 0.9904 - val_loss: 0.0272 - val_acc: 0.9922\n",
            "Epoch 8/100\n",
            "50000/50000 [==============================] - 24s 477us/step - loss: 0.0302 - acc: 0.9909 - val_loss: 0.0272 - val_acc: 0.9926\n",
            "Epoch 9/100\n",
            "50000/50000 [==============================] - 24s 477us/step - loss: 0.0282 - acc: 0.9916 - val_loss: 0.0308 - val_acc: 0.9929\n",
            "Epoch 10/100\n",
            "50000/50000 [==============================] - 24s 475us/step - loss: 0.0275 - acc: 0.9914 - val_loss: 0.0274 - val_acc: 0.9938\n",
            "Epoch 11/100\n",
            "50000/50000 [==============================] - 23s 467us/step - loss: 0.0265 - acc: 0.9917 - val_loss: 0.0332 - val_acc: 0.9923\n",
            "Epoch 12/100\n",
            "50000/50000 [==============================] - 23s 463us/step - loss: 0.0232 - acc: 0.9931 - val_loss: 0.0281 - val_acc: 0.9934\n",
            "Epoch 13/100\n",
            "50000/50000 [==============================] - 23s 461us/step - loss: 0.0240 - acc: 0.9924 - val_loss: 0.0291 - val_acc: 0.9936\n",
            "Epoch 14/100\n",
            "50000/50000 [==============================] - 23s 462us/step - loss: 0.0226 - acc: 0.9933 - val_loss: 0.0285 - val_acc: 0.9933\n",
            "Epoch 15/100\n",
            "50000/50000 [==============================] - 23s 461us/step - loss: 0.0207 - acc: 0.9937 - val_loss: 0.0332 - val_acc: 0.9920\n",
            "Epoch 16/100\n",
            "50000/50000 [==============================] - 23s 460us/step - loss: 0.0219 - acc: 0.9937 - val_loss: 0.0293 - val_acc: 0.9934\n",
            "Epoch 17/100\n",
            "50000/50000 [==============================] - 23s 464us/step - loss: 0.0204 - acc: 0.9937 - val_loss: 0.0290 - val_acc: 0.9934\n",
            "Epoch 18/100\n",
            "50000/50000 [==============================] - 23s 461us/step - loss: 0.0223 - acc: 0.9935 - val_loss: 0.0256 - val_acc: 0.9936\n",
            "Epoch 19/100\n",
            "50000/50000 [==============================] - 23s 460us/step - loss: 0.0198 - acc: 0.9941 - val_loss: 0.0306 - val_acc: 0.9928\n",
            "Epoch 20/100\n",
            "50000/50000 [==============================] - 23s 467us/step - loss: 0.0194 - acc: 0.9942 - val_loss: 0.0251 - val_acc: 0.9938\n",
            "Epoch 21/100\n",
            "50000/50000 [==============================] - 23s 468us/step - loss: 0.0181 - acc: 0.9946 - val_loss: 0.0295 - val_acc: 0.9939\n",
            "Epoch 22/100\n",
            "50000/50000 [==============================] - 23s 465us/step - loss: 0.0208 - acc: 0.9942 - val_loss: 0.0309 - val_acc: 0.9940\n",
            "Epoch 23/100\n",
            "50000/50000 [==============================] - 23s 466us/step - loss: 0.0188 - acc: 0.9949 - val_loss: 0.0238 - val_acc: 0.9937\n",
            "Epoch 24/100\n",
            "50000/50000 [==============================] - 24s 474us/step - loss: 0.0189 - acc: 0.9944 - val_loss: 0.0291 - val_acc: 0.9939\n",
            "Epoch 25/100\n",
            "50000/50000 [==============================] - 24s 473us/step - loss: 0.0200 - acc: 0.9943 - val_loss: 0.0305 - val_acc: 0.9938\n",
            "Epoch 26/100\n",
            "50000/50000 [==============================] - 23s 466us/step - loss: 0.0177 - acc: 0.9948 - val_loss: 0.0297 - val_acc: 0.9943\n",
            "Epoch 27/100\n",
            "50000/50000 [==============================] - 23s 469us/step - loss: 0.0194 - acc: 0.9945 - val_loss: 0.0303 - val_acc: 0.9937\n",
            "Epoch 28/100\n",
            "50000/50000 [==============================] - 23s 466us/step - loss: 0.0193 - acc: 0.9946 - val_loss: 0.0342 - val_acc: 0.9931\n",
            "Epoch 29/100\n",
            "50000/50000 [==============================] - 23s 466us/step - loss: 0.0157 - acc: 0.9956 - val_loss: 0.0376 - val_acc: 0.9939\n",
            "Epoch 30/100\n",
            "50000/50000 [==============================] - 23s 467us/step - loss: 0.0207 - acc: 0.9946 - val_loss: 0.0382 - val_acc: 0.9931\n",
            "Epoch 31/100\n",
            "50000/50000 [==============================] - 24s 472us/step - loss: 0.0172 - acc: 0.9953 - val_loss: 0.0343 - val_acc: 0.9936\n",
            "Epoch 32/100\n",
            "50000/50000 [==============================] - 24s 471us/step - loss: 0.0202 - acc: 0.9946 - val_loss: 0.0277 - val_acc: 0.9948\n",
            "Epoch 33/100\n",
            "50000/50000 [==============================] - 24s 475us/step - loss: 0.0170 - acc: 0.9954 - val_loss: 0.0318 - val_acc: 0.9936\n",
            "Epoch 34/100\n",
            "50000/50000 [==============================] - 23s 468us/step - loss: 0.0186 - acc: 0.9949 - val_loss: 0.0271 - val_acc: 0.9944\n",
            "Epoch 35/100\n",
            "50000/50000 [==============================] - 24s 473us/step - loss: 0.0176 - acc: 0.9954 - val_loss: 0.0269 - val_acc: 0.9946\n",
            "Epoch 36/100\n",
            "50000/50000 [==============================] - 23s 464us/step - loss: 0.0176 - acc: 0.9952 - val_loss: 0.0307 - val_acc: 0.9942\n",
            "Epoch 37/100\n",
            "50000/50000 [==============================] - 23s 466us/step - loss: 0.0190 - acc: 0.9948 - val_loss: 0.0312 - val_acc: 0.9946\n",
            "Epoch 38/100\n",
            "50000/50000 [==============================] - 23s 464us/step - loss: 0.0223 - acc: 0.9940 - val_loss: 0.0289 - val_acc: 0.9929\n",
            "Epoch 39/100\n",
            "50000/50000 [==============================] - 23s 463us/step - loss: 0.0174 - acc: 0.9954 - val_loss: 0.0317 - val_acc: 0.9940\n",
            "Epoch 40/100\n",
            "50000/50000 [==============================] - 24s 470us/step - loss: 0.0193 - acc: 0.9949 - val_loss: 0.0388 - val_acc: 0.9933\n",
            "Epoch 41/100\n",
            "50000/50000 [==============================] - 23s 464us/step - loss: 0.0185 - acc: 0.9951 - val_loss: 0.0513 - val_acc: 0.9916\n",
            "Epoch 42/100\n",
            "50000/50000 [==============================] - 23s 458us/step - loss: 0.0225 - acc: 0.9949 - val_loss: 0.0305 - val_acc: 0.9942\n",
            "Epoch 43/100\n",
            "50000/50000 [==============================] - 23s 456us/step - loss: 0.0162 - acc: 0.9959 - val_loss: 0.0395 - val_acc: 0.9930\n",
            "Epoch 44/100\n",
            "50000/50000 [==============================] - 23s 457us/step - loss: 0.0198 - acc: 0.9951 - val_loss: 0.0340 - val_acc: 0.9940\n",
            "Epoch 45/100\n",
            "50000/50000 [==============================] - 23s 461us/step - loss: 0.0170 - acc: 0.9954 - val_loss: 0.0383 - val_acc: 0.9938\n",
            "Epoch 46/100\n",
            "50000/50000 [==============================] - 23s 463us/step - loss: 0.0186 - acc: 0.9954 - val_loss: 0.0471 - val_acc: 0.9918\n",
            "Epoch 47/100\n",
            "50000/50000 [==============================] - 23s 457us/step - loss: 0.0168 - acc: 0.9954 - val_loss: 0.0439 - val_acc: 0.9930\n",
            "Epoch 48/100\n",
            "50000/50000 [==============================] - 23s 465us/step - loss: 0.0179 - acc: 0.9951 - val_loss: 0.0352 - val_acc: 0.9946\n",
            "Epoch 49/100\n",
            "50000/50000 [==============================] - 23s 465us/step - loss: 0.0173 - acc: 0.9955 - val_loss: 0.0285 - val_acc: 0.9948\n",
            "Epoch 50/100\n",
            "50000/50000 [==============================] - 23s 459us/step - loss: 0.0206 - acc: 0.9953 - val_loss: 0.0329 - val_acc: 0.9939\n",
            "Epoch 51/100\n",
            "50000/50000 [==============================] - 23s 465us/step - loss: 0.0218 - acc: 0.9952 - val_loss: 0.0350 - val_acc: 0.9927\n",
            "Epoch 52/100\n",
            "50000/50000 [==============================] - 23s 464us/step - loss: 0.0171 - acc: 0.9957 - val_loss: 0.0389 - val_acc: 0.9937\n",
            "Epoch 53/100\n",
            "50000/50000 [==============================] - 23s 463us/step - loss: 0.0190 - acc: 0.9954 - val_loss: 0.0311 - val_acc: 0.9937\n",
            "Epoch 54/100\n",
            "50000/50000 [==============================] - 23s 466us/step - loss: 0.0183 - acc: 0.9951 - val_loss: 0.0382 - val_acc: 0.9939\n",
            "Epoch 55/100\n",
            "50000/50000 [==============================] - 23s 466us/step - loss: 0.0231 - acc: 0.9948 - val_loss: 0.0379 - val_acc: 0.9934\n",
            "Epoch 56/100\n",
            "50000/50000 [==============================] - 23s 470us/step - loss: 0.0149 - acc: 0.9963 - val_loss: 0.0438 - val_acc: 0.9934\n",
            "Epoch 57/100\n",
            "50000/50000 [==============================] - 23s 467us/step - loss: 0.0192 - acc: 0.9956 - val_loss: 0.0346 - val_acc: 0.9945\n",
            "Epoch 58/100\n",
            "50000/50000 [==============================] - 23s 466us/step - loss: 0.0194 - acc: 0.9952 - val_loss: 0.0332 - val_acc: 0.9941\n",
            "Epoch 59/100\n",
            "50000/50000 [==============================] - 24s 472us/step - loss: 0.0166 - acc: 0.9958 - val_loss: 0.0409 - val_acc: 0.9942\n",
            "Epoch 60/100\n",
            "50000/50000 [==============================] - 24s 471us/step - loss: 0.0182 - acc: 0.9955 - val_loss: 0.0342 - val_acc: 0.9944\n",
            "Epoch 61/100\n",
            "50000/50000 [==============================] - 23s 465us/step - loss: 0.0184 - acc: 0.9957 - val_loss: 0.0437 - val_acc: 0.9941\n",
            "Epoch 62/100\n",
            "50000/50000 [==============================] - 23s 470us/step - loss: 0.0190 - acc: 0.9957 - val_loss: 0.0370 - val_acc: 0.9940\n",
            "Epoch 63/100\n",
            "50000/50000 [==============================] - 24s 478us/step - loss: 0.0208 - acc: 0.9951 - val_loss: 0.0481 - val_acc: 0.9943\n",
            "Epoch 64/100\n",
            "50000/50000 [==============================] - 24s 471us/step - loss: 0.0209 - acc: 0.9954 - val_loss: 0.0415 - val_acc: 0.9934\n",
            "Epoch 65/100\n",
            "50000/50000 [==============================] - 23s 467us/step - loss: 0.0193 - acc: 0.9952 - val_loss: 0.0329 - val_acc: 0.9940\n",
            "Epoch 66/100\n",
            "50000/50000 [==============================] - 23s 467us/step - loss: 0.0206 - acc: 0.9950 - val_loss: 0.0424 - val_acc: 0.9939\n",
            "Epoch 67/100\n",
            "50000/50000 [==============================] - 23s 467us/step - loss: 0.0236 - acc: 0.9950 - val_loss: 0.0351 - val_acc: 0.9943\n",
            "Epoch 68/100\n",
            "50000/50000 [==============================] - 23s 466us/step - loss: 0.0163 - acc: 0.9963 - val_loss: 0.0383 - val_acc: 0.9941\n",
            "Epoch 69/100\n",
            "50000/50000 [==============================] - 23s 466us/step - loss: 0.0213 - acc: 0.9956 - val_loss: 0.0417 - val_acc: 0.9940\n",
            "Epoch 70/100\n",
            "50000/50000 [==============================] - 23s 469us/step - loss: 0.0223 - acc: 0.9951 - val_loss: 0.0461 - val_acc: 0.9937\n",
            "Epoch 71/100\n",
            "50000/50000 [==============================] - 23s 461us/step - loss: 0.0181 - acc: 0.9958 - val_loss: 0.0386 - val_acc: 0.9942\n",
            "Epoch 72/100\n",
            "50000/50000 [==============================] - 23s 468us/step - loss: 0.0223 - acc: 0.9954 - val_loss: 0.0455 - val_acc: 0.9919\n",
            "Epoch 73/100\n",
            "50000/50000 [==============================] - 23s 462us/step - loss: 0.0235 - acc: 0.9954 - val_loss: 0.0490 - val_acc: 0.9935\n",
            "Epoch 74/100\n",
            "50000/50000 [==============================] - 23s 461us/step - loss: 0.0227 - acc: 0.9954 - val_loss: 0.0383 - val_acc: 0.9935\n",
            "Epoch 75/100\n",
            "50000/50000 [==============================] - 23s 465us/step - loss: 0.0214 - acc: 0.9958 - val_loss: 0.0339 - val_acc: 0.9945\n",
            "Epoch 76/100\n",
            "50000/50000 [==============================] - 23s 464us/step - loss: 0.0217 - acc: 0.9952 - val_loss: 0.0387 - val_acc: 0.9943\n",
            "Epoch 77/100\n",
            "50000/50000 [==============================] - 23s 467us/step - loss: 0.0235 - acc: 0.9957 - val_loss: 0.0530 - val_acc: 0.9934\n",
            "Epoch 78/100\n",
            "50000/50000 [==============================] - 24s 471us/step - loss: 0.0214 - acc: 0.9961 - val_loss: 0.0425 - val_acc: 0.9949\n",
            "Epoch 79/100\n",
            "50000/50000 [==============================] - 23s 465us/step - loss: 0.0218 - acc: 0.9957 - val_loss: 0.0423 - val_acc: 0.9934\n",
            "Epoch 80/100\n",
            "50000/50000 [==============================] - 23s 465us/step - loss: 0.0272 - acc: 0.9950 - val_loss: 0.0423 - val_acc: 0.9949\n",
            "Epoch 81/100\n",
            "50000/50000 [==============================] - 23s 457us/step - loss: 0.0316 - acc: 0.9947 - val_loss: 0.0511 - val_acc: 0.9928\n",
            "Epoch 82/100\n",
            "50000/50000 [==============================] - 23s 466us/step - loss: 0.0216 - acc: 0.9958 - val_loss: 0.0408 - val_acc: 0.9948\n",
            "Epoch 83/100\n",
            "50000/50000 [==============================] - 23s 461us/step - loss: 0.0216 - acc: 0.9957 - val_loss: 0.0465 - val_acc: 0.9933\n",
            "Epoch 84/100\n",
            "50000/50000 [==============================] - 23s 462us/step - loss: 0.0246 - acc: 0.9955 - val_loss: 0.0523 - val_acc: 0.9942\n",
            "Epoch 85/100\n",
            "50000/50000 [==============================] - 22s 449us/step - loss: 0.0280 - acc: 0.9956 - val_loss: 0.0593 - val_acc: 0.9928\n",
            "Epoch 86/100\n",
            "50000/50000 [==============================] - 22s 441us/step - loss: 0.0287 - acc: 0.9950 - val_loss: 0.0445 - val_acc: 0.9934\n",
            "Epoch 87/100\n",
            "50000/50000 [==============================] - 22s 439us/step - loss: 0.0246 - acc: 0.9953 - val_loss: 0.0448 - val_acc: 0.9932\n",
            "Epoch 88/100\n",
            "50000/50000 [==============================] - 21s 430us/step - loss: 0.0202 - acc: 0.9957 - val_loss: 0.0433 - val_acc: 0.9938\n",
            "Epoch 89/100\n",
            "50000/50000 [==============================] - 21s 423us/step - loss: 0.0211 - acc: 0.9954 - val_loss: 0.0477 - val_acc: 0.9944\n",
            "Epoch 90/100\n",
            "50000/50000 [==============================] - 21s 412us/step - loss: 0.0202 - acc: 0.9959 - val_loss: 0.0579 - val_acc: 0.9935\n",
            "Epoch 91/100\n",
            "50000/50000 [==============================] - 21s 413us/step - loss: 0.0253 - acc: 0.9954 - val_loss: 0.0502 - val_acc: 0.9944\n",
            "Epoch 92/100\n",
            "50000/50000 [==============================] - 21s 413us/step - loss: 0.0294 - acc: 0.9953 - val_loss: 0.0482 - val_acc: 0.9942\n",
            "Epoch 93/100\n",
            "50000/50000 [==============================] - 20s 410us/step - loss: 0.0278 - acc: 0.9950 - val_loss: 0.0474 - val_acc: 0.9936\n",
            "Epoch 94/100\n",
            "50000/50000 [==============================] - 20s 400us/step - loss: 0.0257 - acc: 0.9954 - val_loss: 0.0450 - val_acc: 0.9944\n",
            "Epoch 95/100\n",
            "50000/50000 [==============================] - 21s 415us/step - loss: 0.0268 - acc: 0.9952 - val_loss: 0.0497 - val_acc: 0.9936\n",
            "Epoch 96/100\n",
            "50000/50000 [==============================] - 20s 410us/step - loss: 0.0237 - acc: 0.9955 - val_loss: 0.0375 - val_acc: 0.9939\n",
            "Epoch 97/100\n",
            "50000/50000 [==============================] - 21s 414us/step - loss: 0.0269 - acc: 0.9953 - val_loss: 0.0510 - val_acc: 0.9943\n",
            "Epoch 98/100\n",
            "50000/50000 [==============================] - 21s 411us/step - loss: 0.0295 - acc: 0.9944 - val_loss: 0.0445 - val_acc: 0.9934\n",
            "Epoch 99/100\n",
            "50000/50000 [==============================] - 21s 430us/step - loss: 0.0246 - acc: 0.9952 - val_loss: 0.0392 - val_acc: 0.9948\n",
            "Epoch 100/100\n",
            "50000/50000 [==============================] - 22s 434us/step - loss: 0.0255 - acc: 0.9956 - val_loss: 0.0535 - val_acc: 0.9940\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZfgBdn4UM7-",
        "colab_type": "code",
        "outputId": "676e4b23-9c8a-43ed-a3f0-295521922ad0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(cnn_history.history.keys())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUR2XRIiUPTa",
        "colab_type": "code",
        "outputId": "a5896819-ffc9-4e1a-947a-c22d0bd31e44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "# Plot Accuracy of CNN\n",
        "print(\"CNN Final Accuracy\", cnn_history.history['acc'][-1])\n",
        "pd.Series(cnn_history.history['acc']).plot(logy=False)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNN Final Accuracy 0.99558\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBB3R8AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iz-4rvw1UUBd",
        "colab_type": "text"
      },
      "source": [
        "As you can see,  the trained CNN has an accuracy of 99.56%, better than any other MNIST image classification methods we trained before."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lgcLcepNIXy",
        "colab_type": "text"
      },
      "source": [
        "## DCGANs Revisited\n",
        "\n",
        "Now, we will return to deep convolutional generative adversarial networks. We will build a generative model to generate synthetic MNIST data that are very similar to the MNIST dataset.\n",
        "\n",
        "To produce near-realistic images, we will have to train a generator that generates new images from the original MNIST images and a discriminator to judge where those images are believably similar to the original ones or not.\n",
        "\n",
        "For the generator we will use the architecture proposed by Radford, Metz, and Chintala paper presented at the ICLR 2016 conference.\n",
        "\n",
        "![alt text](https://www.divisionlab.com/solvingmagento/wp-content/uploads/2019/DCGAN.png)\n",
        "\n",
        "The generator takes in an initial $100 \\times 1$ *noise* vector denoted here as $z$. It then projects and reshapes it to a tensor with dimensions $1024 \\times 4 \\times 4$. This *project and reshape* operation is opposite of convolution and is called *transposed convolution* (or *deconvolution* in some cases). In transposed convolution, the original process of convolution is reversed, mapping a reduced tensor to a larger one.\n",
        "\n",
        "After the initial transposed deconvolutions, the generator applies four additiional deconvolutions to map a final $64 \\times 3 \\times 3$ tensor.\n",
        "\n",
        "The stages can be shown like this:\n",
        "$$\n",
        "100 \\times 1 \\rightarrow 1024 \\times 4 \\times 4 \\rightarrow 256 \\times 16 \\times 16 \\rightarrow 128 \\times 32 \\times 32 \\rightarrow 64 \\times 3 \\times 3\n",
        "$$\n",
        "\n",
        "\n",
        "Our DCGAN will apply a similar (but not exact) approach.\n",
        "\n",
        "### Generator of the DCGAN\n",
        "\n",
        "This design of a DCGAN leverages he work done by Rowel Atienza (https://github.com/roatienza/Deep-Learning-Experiments) and builds upon it. We will create a class called DCGAN that we will use to build the generator, discriminator, and adversarial models.\n",
        "\n",
        "Let's start with the generator. We will set up parameters such as:\n",
        " - dropout percentage: 0.3\n",
        " - depth of the tensor: 256\n",
        " - size of the other dimensions: 7 by 7\n",
        " - momentum for the batch normalization: 0.8\n",
        " - initial dimensions: 100\n",
        " \n",
        "The final output dimensions are $28 \\times 28 \\times 1$\n",
        "\n",
        "We need to use both dropout and batch normalization to avoid overfitting.\n",
        "\n",
        "We start building the generator by calling `Sequential()` in Keras. We will add a dense, fully connected network layer with input dimensions of 100 and output dimensions of $7 \\times 7 \\times 256$. We will perform batch normalization, use ReLU for activation and perform dropout.\n",
        " \n",
        "\n",
        "Next, we will perform *unsampling* and *transposed convolutions* three times. Each time, we will halve the depth of the output space: from 256 to  128, to 64, to 32 while increasing other dimensions. We will keep the convolution window (kernel size) of 5 by 5 and a default stride of 1.\n",
        "\n",
        "During each transposed deconvolution we will perform batch normalizations and use the ReLU activation function.\n",
        "\n",
        "The process will look like this:\n",
        "\n",
        "$$\n",
        "100 \\rightarrow 7 \\times 7 \\times 256 \\rightarrow 14 \\times 14 \\times 128 \\rightarrow 28 \\times 28 \\times 64 \\rightarrow 28 \\times 28 \\times 32 \\rightarrow 28 \\times 28 \\times 1\n",
        "$$\n",
        "\n",
        "\n",
        "The generator will output a $28 \\times 28 \\times 1$ image - the same dimensions as in the MNIST dataset.\n",
        "\n",
        "### Discriminator of the DCGAN\n",
        "\n",
        "The parameters for the discriminators are:\n",
        " - dropout 0.3\n",
        " - depth 64,\n",
        " - alpha for the `LeakyReLU` function: 0.3\n",
        " \n",
        "First, we will load a 28 by 28 by 1 image and perform convolution using 64 channels, a filter of 5 by 5, and a stride of 2. The activation function will be `LeakyReLU`. After it we will perform dropout.\n",
        " \n",
        "We repeat this process three more times each time doubling the depth of the output while decreasing other dimensions.\n",
        " \n",
        "Finally, we flatten the images and use the sigmoind function to output a probability. The probability denotes the discriminator's confidence in calling the input image a fake (0.0 is fake, 1.0 is real)\n",
        "\n",
        "\n",
        "The process looks like this:\n",
        "\n",
        "$$\n",
        "28 \\times 28 \\times 1 \\rightarrow 14 \\times 14 \\times 64 \\rightarrow 7 \\times 7 \\times 128 \\rightarrow  4 \\times 4 \\times 512 \\rightarrow 1\n",
        "$$\n",
        "\n",
        "\n",
        "### Discriminator and Adversarial Models\n",
        "\n",
        "Next, we define the discriminator model (to detect fakes) and the adversarial model (the counterfeiter learning from the discriminator). For both models, we will use the RMSprop optimizer, define the loss function as binary cross-entropy, and use accuracy as our reported metric.\n",
        "\n",
        "For the adversarial model, we will use the generator and discriminator networks sefined earlier.\n",
        "\n",
        "For the discriminator model, we just use the discriminator network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzbZ-HOkLRin",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ElapsedTimer(object):\n",
        "    def __init__(self):\n",
        "        self.start_time = time.time()\n",
        "    def elapsed(self,sec):\n",
        "        if sec < 60:\n",
        "            return str(sec) + \" sec\"\n",
        "        elif sec < (60 * 60):\n",
        "            return str(sec / 60) + \" min\"\n",
        "        else:\n",
        "            return str(sec / (60 * 60)) + \" hr\"\n",
        "    def elapsed_time(self):\n",
        "        print(\"Elapsed: %s \" % self.elapsed(time.time() - self.start_time))\n",
        "\n",
        "class DCGAN(object):\n",
        "    def __init__(self, img_rows=28, img_cols=28, channel=1):\n",
        "\n",
        "        self.img_rows = img_rows\n",
        "        self.img_cols = img_cols\n",
        "        self.channel = channel\n",
        "        self.D = None   # discriminator\n",
        "        self.G = None   # generator\n",
        "        self.AM = None  # adversarial model\n",
        "        self.DM = None  # discriminator model\n",
        "        \n",
        "    def generator(self, depth=256, dim=7, dropout=0.3, momentum=0.8, \\\n",
        "                  window=5, input_dim=100, output_depth=1):\n",
        "        \"\"\"Produce a generator.\n",
        "        :param depth: initial depth of the output tensor\n",
        "        :param dim: initial width and length of the output tensor\n",
        "        :param dropout: dropout percentage\n",
        "        :param momentum: momentum for the batch normalization\n",
        "        :param window: kernel size\n",
        "        :param input_dim: dimensions of the input vector\n",
        "        :param output_depth: final dimensionality of the output space\n",
        "        \"\"\"\n",
        "        \n",
        "        # if generator is already set up, return it\n",
        "        if self.G:\n",
        "            return self.G\n",
        "          \n",
        "        # Set up a sequential model  \n",
        "        self.G = Sequential()\n",
        "        \n",
        "        \"\"\"Add a dense  layer with the input dimension equals the initial \n",
        "        input dimension. The output dimension is the multiplication of the\n",
        "        depth and dim parameters (7 * 7 * 256 = 12544).\n",
        "        \"\"\"\n",
        "        self.G.add(Dense(dim*dim*depth, input_dim=input_dim))\n",
        "        self.G.add(BatchNormalization(momentum=momentum))\n",
        "        self.G.add(Activation('relu'))\n",
        "        self.G.add(Reshape((dim, dim, depth)))\n",
        "        self.G.add(Dropout(dropout))\n",
        "        \n",
        "        # Unsampling and transposed convolution on 128 depth\n",
        "        self.G.add(UpSampling2D())\n",
        "        self.G.add(Conv2DTranspose(int(depth/2), window, padding='same'))\n",
        "        self.G.add(BatchNormalization(momentum=momentum))\n",
        "        self.G.add(Activation('relu'))\n",
        "\n",
        "        # Unsampling and transposed convolution on 64 depth\n",
        "        self.G.add(UpSampling2D())\n",
        "        self.G.add(Conv2DTranspose(int(depth/4), window, padding='same'))\n",
        "        self.G.add(BatchNormalization(momentum=momentum))\n",
        "        self.G.add(Activation('relu'))\n",
        "\n",
        "        # Unsampling and transposed convolution on 32 depth\n",
        "        self.G.add(Conv2DTranspose(int(depth/8), window, padding='same'))\n",
        "        self.G.add(BatchNormalization(momentum=momentum))\n",
        "        self.G.add(Activation('relu'))\n",
        "\n",
        "        # Unsampling and transposed convolution on 1 depth\n",
        "        self.G.add(Conv2DTranspose(output_depth, window, padding='same'))\n",
        "        self.G.add(Activation('sigmoid'))\n",
        "        self.G.summary()\n",
        "        return self.G\n",
        "\n",
        "    def discriminator(self, depth=64, dropout=0.3, alpha=0.3):\n",
        "        \"\"\"Return adiscriminator instance.\"\"\"\n",
        "        \n",
        "        if self.D:\n",
        "            return self.D\n",
        "        self.D = Sequential()\n",
        "        \n",
        "        # Identify the input shape of an image\n",
        "        input_shape = (self.img_rows, self.img_cols, self.channel)\n",
        "        \n",
        "        \"\"\"\n",
        "        Add a convolutional layer with the initial depth, activation function \n",
        "        and dropout\n",
        "        \"\"\"\n",
        "        self.D.add(Conv2D(depth*1, 5, strides=2, input_shape=input_shape,\n",
        "            padding='same'))\n",
        "        self.D.add(LeakyReLU(alpha=alpha))\n",
        "        self.D.add(Dropout(dropout))\n",
        "\n",
        "        # Double-depth convolutions layer\n",
        "        self.D.add(Conv2D(depth*2, 5, strides=2, padding='same'))\n",
        "        self.D.add(LeakyReLU(alpha=alpha))\n",
        "        self.D.add(Dropout(dropout))\n",
        "\n",
        "        # Quadruple-depth convolutions layer\n",
        "        self.D.add(Conv2D(depth*4, 5, strides=2, padding='same'))\n",
        "        self.D.add(LeakyReLU(alpha=alpha))\n",
        "        self.D.add(Dropout(dropout))\n",
        "\n",
        "        # Times-8-depth convolutions layer\n",
        "        self.D.add(Conv2D(depth*8, 5, strides=1, padding='same'))\n",
        "        self.D.add(LeakyReLU(alpha=alpha))\n",
        "        self.D.add(Dropout(dropout))\n",
        "\n",
        "        # Flatten images and push them through a dense layer\n",
        "        self.D.add(Flatten())\n",
        "        self.D.add(Dense(1))\n",
        "        self.D.add(Activation('sigmoid'))\n",
        "        self.D.summary()\n",
        "        return self.D\n",
        "\n",
        "    def discriminator_model(self):\n",
        "        \"\"\"Define the discriminator model consising of a layer taken\n",
        "        by the previously defined discriminator.\"\"\"\n",
        "        if self.DM:\n",
        "            return self.DM\n",
        "          \n",
        "        optimizer = RMSprop(lr=0.0002, decay=6e-8)\n",
        "        \n",
        "        self.DM = Sequential()\n",
        "        self.DM.add(self.discriminator())\n",
        "        self.DM.compile(loss='binary_crossentropy', \\\n",
        "                        optimizer=optimizer, metrics=['accuracy'])\n",
        "        return self.DM\n",
        "\n",
        "    def adversarial_model(self):\n",
        "        \"\"\"Define the adversarial model as a sequence of generator and \n",
        "        discriminator instances.\"\"\"\n",
        "        if self.AM:\n",
        "            return self.AM\n",
        "          \n",
        "        optimizer = RMSprop(lr=0.0001, decay=3e-8)\n",
        "        \n",
        "        self.AM = Sequential()\n",
        "        self.AM.add(self.generator())\n",
        "        self.AM.add(self.discriminator())\n",
        "        self.AM.compile(loss='binary_crossentropy', \\\n",
        "                        optimizer=optimizer, metrics=['accuracy'])\n",
        "        return self.AM\n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29CmBdXwfDHB",
        "colab_type": "text"
      },
      "source": [
        "## DCGAN for the MNIST Dataset\n",
        "\n",
        "Now, let's define the DCGAN for the MNIST dataset. First, we initialize the `MNIST_DCGAN` class for the $28 \\times 28  \\times 1$ MNIST images and use the generator, discriminator model, and adversarial model from earlier:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyvGMdGdUkWW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MNIST_DCGAN(object):\n",
        "    def __init__(self, x_train):\n",
        "        self.img_rows = 28\n",
        "        self.img_cols = 28\n",
        "        self.channel = 1\n",
        "\n",
        "        self.x_train = x_train\n",
        "\n",
        "        self.DCGAN = DCGAN()\n",
        "        self.discriminator =  self.DCGAN.discriminator_model()\n",
        "        self.adversarial = self.DCGAN.adversarial_model()\n",
        "        self.generator = self.DCGAN.generator()\n",
        "\n",
        "    def train(self, train_steps=2000, batch_size=256, save_interval=0):\n",
        "        \"\"\"Perform iterative improvement on the adversarial and discriminator models\n",
        "        by making the former to generate ever better fake images from noise, while \n",
        "        teaching the latter to recognize fake images ever better.\n",
        "        \"\"\"\n",
        "        noise_input = None\n",
        "        \n",
        "        if save_interval>0:\n",
        "            noise_input = np.random.uniform(-1.0, 1.0, size=[16, 100])\n",
        "            \n",
        "        for i in range(train_steps):\n",
        "            # Randomly sample a batch from the training data\n",
        "            images_train = self.x_train[np.random.randint(0,\n",
        "                self.x_train.shape[0], size=batch_size), :, :, :]\n",
        "            \n",
        "            # Generate random uniform noise\n",
        "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
        "            \n",
        "            # Generate totally fake images out of the noise\n",
        "            images_fake = self.generator.predict(noise)\n",
        "\n",
        "            # The generated fake images have the same dimensions as the training images\n",
        "            x = np.concatenate((images_train, images_fake))\n",
        "            \n",
        "            # Label the first half of the input as genuine, the second half as fakes\n",
        "            y = np.ones([2*batch_size, 1])\n",
        "            y[batch_size:, :] = 0\n",
        "            \n",
        "            # train_on_batch: Runs a single gradient update on a single batch of data.\n",
        "            d_loss = self.discriminator.train_on_batch(x, y)\n",
        "\n",
        "            y = np.ones([batch_size, 1])\n",
        "            \n",
        "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
        "            \n",
        "            # Train the adversarial model to generate believable \n",
        "            # hand-written digits from random noise.\n",
        "            # Note that the discriminator is trained at the same time to recognize \n",
        "            # images generated by the generator from random noise as fakes\n",
        "            \n",
        "            a_loss = self.adversarial.train_on_batch(noise, y)\n",
        "            log_mesg = \"%d: [D loss: %f, acc: %f]\" % (i, d_loss[0], d_loss[1])\n",
        "            log_mesg = \"%s  [A loss: %f, acc: %f]\" % (log_mesg, a_loss[0], \\\n",
        "                                                      a_loss[1])\n",
        "            print(log_mesg)\n",
        "            if save_interval>0:\n",
        "                if (i+1)%save_interval==0:\n",
        "                    self.plot_images(save2file=True, \\\n",
        "                        samples=noise_input.shape[0],\\\n",
        "                        noise=noise_input, step=(i+1))\n",
        "\n",
        "    def plot_images(self, save2file=False, fake=True, samples=16, \\\n",
        "                    noise=None, step=0):\n",
        "        current_path = 'gdrive/My Drive/Colab Notebooks/datasets/mnist/'\n",
        "        file = 'synthetic/'\n",
        "        filename = 'mnist.png'\n",
        "        if fake:\n",
        "            # Let's generate some fake images\n",
        "            if noise is None:\n",
        "                # If noise is not given, generate our own\n",
        "                noise = np.random.uniform(-1.0, 1.0, size=[samples, 100])\n",
        "            else:\n",
        "                # If noise is given, prepare a filename to save the fakes into\n",
        "                filename = \"mnist_%d.png\" % step\n",
        "            images = self.generator.predict(noise)\n",
        "        else:\n",
        "            # When plotting non-fakes, sample a few images from the training data\n",
        "            i = np.random.randint(0, self.x_train.shape[0], samples)\n",
        "            images = self.x_train[i, :, :, :]\n",
        "\n",
        "        # Plot the images (fakes or not)    \n",
        "        plt.figure(figsize=(10,10))\n",
        "        for i in range(images.shape[0]):\n",
        "            plt.subplot(4, 4, i+1)\n",
        "            image = images[i, :, :, :]\n",
        "            image = np.reshape(image, [self.img_rows, self.img_cols])\n",
        "            plt.imshow(image, cmap='gray')\n",
        "            plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "        \n",
        "        # If necessary, save the images\n",
        "        if save2file:\n",
        "            plt.savefig(current_path+file+filename)\n",
        "            plt.close('all')\n",
        "        else:\n",
        "            plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1HyvMsld7PW",
        "colab_type": "code",
        "outputId": "bcce7750-ed9e-4304-e8c0-811bbd613a6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Initialize MNIST DCGAN and train\n",
        "mnist_dcgan = MNIST_DCGAN(X_train_keras)\n",
        "timer = ElapsedTimer()\n",
        "mnist_dcgan.train(train_steps=1000, batch_size=256, save_interval=500)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_5 (Conv2D)            (None, 14, 14, 64)        1664      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 7, 7, 128)         204928    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 4, 4, 256)         819456    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)    (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 4, 4, 512)         3277312   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)    (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 8192)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 8193      \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 4,311,553\n",
            "Trainable params: 4,311,553\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_4 (Dense)              (None, 12544)             1266944   \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 12544)             50176     \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 12544)             0         \n",
            "_________________________________________________________________\n",
            "reshape_2 (Reshape)          (None, 7, 7, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 7, 7, 256)         0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_3 (UpSampling2 (None, 14, 14, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_5 (Conv2DTr (None, 14, 14, 128)       819328    \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 14, 14, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_4 (UpSampling2 (None, 28, 28, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_6 (Conv2DTr (None, 28, 28, 64)        204864    \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 28, 28, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 28, 28, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_7 (Conv2DTr (None, 28, 28, 32)        51232     \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 28, 28, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 28, 28, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_8 (Conv2DTr (None, 28, 28, 1)         801       \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 28, 28, 1)         0         \n",
            "=================================================================\n",
            "Total params: 2,394,241\n",
            "Trainable params: 2,368,705\n",
            "Non-trainable params: 25,536\n",
            "_________________________________________________________________\n",
            "0: [D loss: 0.696324, acc: 0.378906]  [A loss: 1.193757, acc: 0.000000]\n",
            "1: [D loss: 0.702037, acc: 0.500000]  [A loss: 1.048513, acc: 0.000000]\n",
            "2: [D loss: 0.627817, acc: 0.992188]  [A loss: 1.193056, acc: 0.000000]\n",
            "3: [D loss: 0.556299, acc: 0.980469]  [A loss: 1.365726, acc: 0.000000]\n",
            "4: [D loss: 0.515430, acc: 0.894531]  [A loss: 1.869962, acc: 0.000000]\n",
            "5: [D loss: 0.564745, acc: 0.910156]  [A loss: 0.998653, acc: 0.003906]\n",
            "6: [D loss: 0.513429, acc: 0.525391]  [A loss: 1.656686, acc: 0.000000]\n",
            "7: [D loss: 0.477268, acc: 0.912109]  [A loss: 1.363387, acc: 0.000000]\n",
            "8: [D loss: 0.425299, acc: 0.953125]  [A loss: 1.669370, acc: 0.000000]\n",
            "9: [D loss: 0.401797, acc: 0.960938]  [A loss: 1.959309, acc: 0.000000]\n",
            "10: [D loss: 0.371222, acc: 0.998047]  [A loss: 1.623594, acc: 0.000000]\n",
            "11: [D loss: 0.380316, acc: 0.779297]  [A loss: 2.567199, acc: 0.000000]\n",
            "12: [D loss: 0.446988, acc: 0.890625]  [A loss: 1.360009, acc: 0.000000]\n",
            "13: [D loss: 0.331473, acc: 0.927734]  [A loss: 1.814063, acc: 0.000000]\n",
            "14: [D loss: 0.252057, acc: 1.000000]  [A loss: 1.580759, acc: 0.000000]\n",
            "15: [D loss: 0.335281, acc: 0.833984]  [A loss: 2.432505, acc: 0.000000]\n",
            "16: [D loss: 0.306271, acc: 0.984375]  [A loss: 1.544365, acc: 0.000000]\n",
            "17: [D loss: 0.272792, acc: 0.955078]  [A loss: 2.098313, acc: 0.000000]\n",
            "18: [D loss: 0.198762, acc: 1.000000]  [A loss: 1.735261, acc: 0.000000]\n",
            "19: [D loss: 0.241796, acc: 0.972656]  [A loss: 2.345381, acc: 0.000000]\n",
            "20: [D loss: 0.185193, acc: 0.998047]  [A loss: 1.813883, acc: 0.000000]\n",
            "21: [D loss: 0.240180, acc: 0.966797]  [A loss: 2.613624, acc: 0.000000]\n",
            "22: [D loss: 0.193987, acc: 0.994141]  [A loss: 1.783625, acc: 0.000000]\n",
            "23: [D loss: 0.265001, acc: 0.937500]  [A loss: 2.688536, acc: 0.000000]\n",
            "24: [D loss: 0.189649, acc: 0.982422]  [A loss: 1.758826, acc: 0.000000]\n",
            "25: [D loss: 0.261444, acc: 0.927734]  [A loss: 2.581200, acc: 0.000000]\n",
            "26: [D loss: 0.156224, acc: 0.992188]  [A loss: 1.909137, acc: 0.000000]\n",
            "27: [D loss: 0.193183, acc: 0.988281]  [A loss: 2.415245, acc: 0.000000]\n",
            "28: [D loss: 0.125516, acc: 1.000000]  [A loss: 2.143512, acc: 0.000000]\n",
            "29: [D loss: 0.152557, acc: 0.998047]  [A loss: 2.400193, acc: 0.000000]\n",
            "30: [D loss: 0.135791, acc: 1.000000]  [A loss: 2.305100, acc: 0.000000]\n",
            "31: [D loss: 0.132417, acc: 1.000000]  [A loss: 2.285500, acc: 0.000000]\n",
            "32: [D loss: 0.152557, acc: 0.996094]  [A loss: 2.633377, acc: 0.000000]\n",
            "33: [D loss: 0.127390, acc: 0.996094]  [A loss: 1.605660, acc: 0.007812]\n",
            "34: [D loss: 0.705562, acc: 0.519531]  [A loss: 4.258557, acc: 0.000000]\n",
            "35: [D loss: 0.906261, acc: 0.576172]  [A loss: 1.288228, acc: 0.003906]\n",
            "36: [D loss: 0.397501, acc: 0.732422]  [A loss: 1.468344, acc: 0.000000]\n",
            "37: [D loss: 0.273176, acc: 0.927734]  [A loss: 1.547285, acc: 0.000000]\n",
            "38: [D loss: 0.277963, acc: 0.925781]  [A loss: 1.654887, acc: 0.000000]\n",
            "39: [D loss: 0.273216, acc: 0.931641]  [A loss: 1.721511, acc: 0.000000]\n",
            "40: [D loss: 0.275678, acc: 0.951172]  [A loss: 1.706625, acc: 0.000000]\n",
            "41: [D loss: 0.290843, acc: 0.919922]  [A loss: 1.808695, acc: 0.000000]\n",
            "42: [D loss: 0.273065, acc: 0.951172]  [A loss: 1.786034, acc: 0.000000]\n",
            "43: [D loss: 0.288825, acc: 0.929688]  [A loss: 1.814002, acc: 0.000000]\n",
            "44: [D loss: 0.285696, acc: 0.941406]  [A loss: 1.874963, acc: 0.000000]\n",
            "45: [D loss: 0.270280, acc: 0.958984]  [A loss: 1.846536, acc: 0.000000]\n",
            "46: [D loss: 0.291839, acc: 0.925781]  [A loss: 1.974147, acc: 0.000000]\n",
            "47: [D loss: 0.261394, acc: 0.978516]  [A loss: 1.671918, acc: 0.000000]\n",
            "48: [D loss: 0.389183, acc: 0.742188]  [A loss: 2.544624, acc: 0.000000]\n",
            "49: [D loss: 0.392650, acc: 0.871094]  [A loss: 0.713667, acc: 0.500000]\n",
            "50: [D loss: 1.175893, acc: 0.500000]  [A loss: 2.029997, acc: 0.000000]\n",
            "51: [D loss: 0.341114, acc: 0.960938]  [A loss: 1.136942, acc: 0.003906]\n",
            "52: [D loss: 0.504578, acc: 0.568359]  [A loss: 1.752138, acc: 0.000000]\n",
            "53: [D loss: 0.316190, acc: 0.966797]  [A loss: 1.505871, acc: 0.000000]\n",
            "54: [D loss: 0.401989, acc: 0.763672]  [A loss: 1.861985, acc: 0.000000]\n",
            "55: [D loss: 0.332767, acc: 0.949219]  [A loss: 1.535417, acc: 0.000000]\n",
            "56: [D loss: 0.418080, acc: 0.730469]  [A loss: 1.997843, acc: 0.000000]\n",
            "57: [D loss: 0.329715, acc: 0.949219]  [A loss: 1.335235, acc: 0.003906]\n",
            "58: [D loss: 0.565334, acc: 0.548828]  [A loss: 2.571910, acc: 0.000000]\n",
            "59: [D loss: 0.461004, acc: 0.841797]  [A loss: 0.629322, acc: 0.632812]\n",
            "60: [D loss: 0.984101, acc: 0.500000]  [A loss: 1.743666, acc: 0.000000]\n",
            "61: [D loss: 0.390634, acc: 0.958984]  [A loss: 1.106927, acc: 0.019531]\n",
            "62: [D loss: 0.539414, acc: 0.533203]  [A loss: 1.809617, acc: 0.000000]\n",
            "63: [D loss: 0.383182, acc: 0.933594]  [A loss: 1.231723, acc: 0.003906]\n",
            "64: [D loss: 0.537898, acc: 0.548828]  [A loss: 2.005285, acc: 0.000000]\n",
            "65: [D loss: 0.384500, acc: 0.941406]  [A loss: 1.058383, acc: 0.031250]\n",
            "66: [D loss: 0.646966, acc: 0.507812]  [A loss: 2.267807, acc: 0.000000]\n",
            "67: [D loss: 0.424465, acc: 0.910156]  [A loss: 0.749862, acc: 0.386719]\n",
            "68: [D loss: 0.752880, acc: 0.500000]  [A loss: 1.978238, acc: 0.000000]\n",
            "69: [D loss: 0.392875, acc: 0.962891]  [A loss: 0.971251, acc: 0.058594]\n",
            "70: [D loss: 0.582798, acc: 0.505859]  [A loss: 1.860741, acc: 0.000000]\n",
            "71: [D loss: 0.403640, acc: 0.923828]  [A loss: 1.162341, acc: 0.007812]\n",
            "72: [D loss: 0.529082, acc: 0.541016]  [A loss: 2.051105, acc: 0.000000]\n",
            "73: [D loss: 0.390350, acc: 0.941406]  [A loss: 0.998240, acc: 0.035156]\n",
            "74: [D loss: 0.634397, acc: 0.505859]  [A loss: 2.369901, acc: 0.000000]\n",
            "75: [D loss: 0.408421, acc: 0.912109]  [A loss: 0.790429, acc: 0.308594]\n",
            "76: [D loss: 0.705756, acc: 0.500000]  [A loss: 2.128535, acc: 0.000000]\n",
            "77: [D loss: 0.441593, acc: 0.908203]  [A loss: 0.730210, acc: 0.425781]\n",
            "78: [D loss: 0.687354, acc: 0.500000]  [A loss: 1.954160, acc: 0.000000]\n",
            "79: [D loss: 0.424257, acc: 0.927734]  [A loss: 0.962947, acc: 0.050781]\n",
            "80: [D loss: 0.603820, acc: 0.500000]  [A loss: 2.089060, acc: 0.000000]\n",
            "81: [D loss: 0.414442, acc: 0.951172]  [A loss: 0.911775, acc: 0.121094]\n",
            "82: [D loss: 0.632517, acc: 0.503906]  [A loss: 2.215234, acc: 0.000000]\n",
            "83: [D loss: 0.415832, acc: 0.941406]  [A loss: 0.815934, acc: 0.230469]\n",
            "84: [D loss: 0.672052, acc: 0.501953]  [A loss: 2.117356, acc: 0.000000]\n",
            "85: [D loss: 0.418719, acc: 0.941406]  [A loss: 0.822436, acc: 0.210938]\n",
            "86: [D loss: 0.679955, acc: 0.500000]  [A loss: 2.099669, acc: 0.000000]\n",
            "87: [D loss: 0.421625, acc: 0.957031]  [A loss: 0.864659, acc: 0.171875]\n",
            "88: [D loss: 0.654904, acc: 0.503906]  [A loss: 2.107565, acc: 0.000000]\n",
            "89: [D loss: 0.426794, acc: 0.927734]  [A loss: 0.795165, acc: 0.320312]\n",
            "90: [D loss: 0.677474, acc: 0.500000]  [A loss: 2.017917, acc: 0.000000]\n",
            "91: [D loss: 0.434391, acc: 0.960938]  [A loss: 0.911275, acc: 0.136719]\n",
            "92: [D loss: 0.628347, acc: 0.513672]  [A loss: 2.045361, acc: 0.000000]\n",
            "93: [D loss: 0.428735, acc: 0.953125]  [A loss: 0.880928, acc: 0.160156]\n",
            "94: [D loss: 0.651759, acc: 0.507812]  [A loss: 2.217401, acc: 0.000000]\n",
            "95: [D loss: 0.450706, acc: 0.912109]  [A loss: 0.737969, acc: 0.421875]\n",
            "96: [D loss: 0.707148, acc: 0.500000]  [A loss: 2.053749, acc: 0.000000]\n",
            "97: [D loss: 0.446172, acc: 0.929688]  [A loss: 0.830672, acc: 0.207031]\n",
            "98: [D loss: 0.661951, acc: 0.503906]  [A loss: 2.031167, acc: 0.000000]\n",
            "99: [D loss: 0.435235, acc: 0.933594]  [A loss: 0.892650, acc: 0.113281]\n",
            "100: [D loss: 0.645639, acc: 0.511719]  [A loss: 2.096513, acc: 0.000000]\n",
            "101: [D loss: 0.442438, acc: 0.931641]  [A loss: 0.792940, acc: 0.292969]\n",
            "102: [D loss: 0.689782, acc: 0.500000]  [A loss: 2.196311, acc: 0.000000]\n",
            "103: [D loss: 0.471994, acc: 0.869141]  [A loss: 0.663042, acc: 0.605469]\n",
            "104: [D loss: 0.744015, acc: 0.500000]  [A loss: 2.030803, acc: 0.000000]\n",
            "105: [D loss: 0.482400, acc: 0.888672]  [A loss: 0.792432, acc: 0.339844]\n",
            "106: [D loss: 0.652323, acc: 0.503906]  [A loss: 1.982677, acc: 0.000000]\n",
            "107: [D loss: 0.461185, acc: 0.904297]  [A loss: 0.854133, acc: 0.191406]\n",
            "108: [D loss: 0.661221, acc: 0.505859]  [A loss: 2.150198, acc: 0.000000]\n",
            "109: [D loss: 0.499240, acc: 0.853516]  [A loss: 0.612256, acc: 0.687500]\n",
            "110: [D loss: 0.774614, acc: 0.500000]  [A loss: 2.020391, acc: 0.000000]\n",
            "111: [D loss: 0.510402, acc: 0.857422]  [A loss: 0.698712, acc: 0.539062]\n",
            "112: [D loss: 0.710047, acc: 0.501953]  [A loss: 1.915217, acc: 0.000000]\n",
            "113: [D loss: 0.476731, acc: 0.884766]  [A loss: 0.871239, acc: 0.238281]\n",
            "114: [D loss: 0.676955, acc: 0.507812]  [A loss: 2.177821, acc: 0.000000]\n",
            "115: [D loss: 0.491200, acc: 0.857422]  [A loss: 0.687795, acc: 0.503906]\n",
            "116: [D loss: 0.744353, acc: 0.501953]  [A loss: 2.188318, acc: 0.000000]\n",
            "117: [D loss: 0.525605, acc: 0.806641]  [A loss: 0.611414, acc: 0.695312]\n",
            "118: [D loss: 0.768852, acc: 0.496094]  [A loss: 1.857921, acc: 0.000000]\n",
            "119: [D loss: 0.518032, acc: 0.875000]  [A loss: 0.779674, acc: 0.328125]\n",
            "120: [D loss: 0.694596, acc: 0.501953]  [A loss: 2.031533, acc: 0.000000]\n",
            "121: [D loss: 0.503672, acc: 0.867188]  [A loss: 0.771229, acc: 0.359375]\n",
            "122: [D loss: 0.727185, acc: 0.501953]  [A loss: 2.231654, acc: 0.000000]\n",
            "123: [D loss: 0.535798, acc: 0.785156]  [A loss: 0.603774, acc: 0.722656]\n",
            "124: [D loss: 0.749687, acc: 0.500000]  [A loss: 1.795209, acc: 0.000000]\n",
            "125: [D loss: 0.535827, acc: 0.814453]  [A loss: 0.837835, acc: 0.175781]\n",
            "126: [D loss: 0.671736, acc: 0.496094]  [A loss: 1.973464, acc: 0.000000]\n",
            "127: [D loss: 0.516968, acc: 0.880859]  [A loss: 0.800903, acc: 0.261719]\n",
            "128: [D loss: 0.727851, acc: 0.500000]  [A loss: 2.214093, acc: 0.000000]\n",
            "129: [D loss: 0.565048, acc: 0.757812]  [A loss: 0.586433, acc: 0.808594]\n",
            "130: [D loss: 0.785825, acc: 0.498047]  [A loss: 1.714645, acc: 0.000000]\n",
            "131: [D loss: 0.540936, acc: 0.853516]  [A loss: 0.810765, acc: 0.230469]\n",
            "132: [D loss: 0.672472, acc: 0.505859]  [A loss: 1.755307, acc: 0.000000]\n",
            "133: [D loss: 0.535080, acc: 0.828125]  [A loss: 0.987419, acc: 0.054688]\n",
            "134: [D loss: 0.647693, acc: 0.503906]  [A loss: 2.062565, acc: 0.000000]\n",
            "135: [D loss: 0.548306, acc: 0.804688]  [A loss: 0.613481, acc: 0.742188]\n",
            "136: [D loss: 0.810206, acc: 0.496094]  [A loss: 2.000622, acc: 0.000000]\n",
            "137: [D loss: 0.573603, acc: 0.722656]  [A loss: 0.618980, acc: 0.750000]\n",
            "138: [D loss: 0.738894, acc: 0.498047]  [A loss: 1.557346, acc: 0.000000]\n",
            "139: [D loss: 0.549358, acc: 0.792969]  [A loss: 1.012661, acc: 0.039062]\n",
            "140: [D loss: 0.622400, acc: 0.523438]  [A loss: 1.816820, acc: 0.000000]\n",
            "141: [D loss: 0.548146, acc: 0.814453]  [A loss: 0.739812, acc: 0.390625]\n",
            "142: [D loss: 0.705148, acc: 0.501953]  [A loss: 1.923018, acc: 0.000000]\n",
            "143: [D loss: 0.573705, acc: 0.755859]  [A loss: 0.632308, acc: 0.699219]\n",
            "144: [D loss: 0.744179, acc: 0.498047]  [A loss: 1.824749, acc: 0.000000]\n",
            "145: [D loss: 0.547858, acc: 0.824219]  [A loss: 0.716094, acc: 0.468750]\n",
            "146: [D loss: 0.711373, acc: 0.498047]  [A loss: 1.800823, acc: 0.000000]\n",
            "147: [D loss: 0.550712, acc: 0.808594]  [A loss: 0.656697, acc: 0.593750]\n",
            "148: [D loss: 0.711933, acc: 0.500000]  [A loss: 1.698781, acc: 0.000000]\n",
            "149: [D loss: 0.542277, acc: 0.841797]  [A loss: 0.756727, acc: 0.347656]\n",
            "150: [D loss: 0.691837, acc: 0.500000]  [A loss: 1.837392, acc: 0.000000]\n",
            "151: [D loss: 0.554271, acc: 0.775391]  [A loss: 0.642669, acc: 0.664062]\n",
            "152: [D loss: 0.731602, acc: 0.500000]  [A loss: 1.702653, acc: 0.000000]\n",
            "153: [D loss: 0.557483, acc: 0.824219]  [A loss: 0.738980, acc: 0.355469]\n",
            "154: [D loss: 0.698267, acc: 0.503906]  [A loss: 1.752411, acc: 0.000000]\n",
            "155: [D loss: 0.574556, acc: 0.769531]  [A loss: 0.691647, acc: 0.507812]\n",
            "156: [D loss: 0.690175, acc: 0.503906]  [A loss: 1.667647, acc: 0.000000]\n",
            "157: [D loss: 0.550076, acc: 0.832031]  [A loss: 0.739523, acc: 0.371094]\n",
            "158: [D loss: 0.679520, acc: 0.500000]  [A loss: 1.750286, acc: 0.000000]\n",
            "159: [D loss: 0.555845, acc: 0.822266]  [A loss: 0.714246, acc: 0.429688]\n",
            "160: [D loss: 0.677003, acc: 0.500000]  [A loss: 1.746759, acc: 0.000000]\n",
            "161: [D loss: 0.547091, acc: 0.812500]  [A loss: 0.681077, acc: 0.523438]\n",
            "162: [D loss: 0.717141, acc: 0.501953]  [A loss: 1.784953, acc: 0.000000]\n",
            "163: [D loss: 0.587246, acc: 0.742188]  [A loss: 0.665011, acc: 0.621094]\n",
            "164: [D loss: 0.710398, acc: 0.501953]  [A loss: 1.504804, acc: 0.000000]\n",
            "165: [D loss: 0.552657, acc: 0.744141]  [A loss: 1.182709, acc: 0.007812]\n",
            "166: [D loss: 0.618236, acc: 0.613281]  [A loss: 1.603954, acc: 0.000000]\n",
            "167: [D loss: 0.585286, acc: 0.771484]  [A loss: 0.809862, acc: 0.273438]\n",
            "168: [D loss: 0.672537, acc: 0.511719]  [A loss: 1.847282, acc: 0.000000]\n",
            "169: [D loss: 0.558200, acc: 0.802734]  [A loss: 0.671446, acc: 0.542969]\n",
            "170: [D loss: 0.775017, acc: 0.500000]  [A loss: 1.934575, acc: 0.000000]\n",
            "171: [D loss: 0.639451, acc: 0.605469]  [A loss: 0.577168, acc: 0.839844]\n",
            "172: [D loss: 0.800020, acc: 0.500000]  [A loss: 1.431833, acc: 0.000000]\n",
            "173: [D loss: 0.556038, acc: 0.808594]  [A loss: 0.946501, acc: 0.109375]\n",
            "174: [D loss: 0.697016, acc: 0.529297]  [A loss: 1.628268, acc: 0.000000]\n",
            "175: [D loss: 0.582738, acc: 0.757812]  [A loss: 0.718932, acc: 0.410156]\n",
            "176: [D loss: 0.696405, acc: 0.505859]  [A loss: 1.504462, acc: 0.000000]\n",
            "177: [D loss: 0.590550, acc: 0.734375]  [A loss: 0.792718, acc: 0.261719]\n",
            "178: [D loss: 0.684661, acc: 0.515625]  [A loss: 1.510329, acc: 0.000000]\n",
            "179: [D loss: 0.612241, acc: 0.703125]  [A loss: 0.756158, acc: 0.355469]\n",
            "180: [D loss: 0.705822, acc: 0.505859]  [A loss: 1.709235, acc: 0.000000]\n",
            "181: [D loss: 0.613142, acc: 0.675781]  [A loss: 0.700933, acc: 0.488281]\n",
            "182: [D loss: 0.698432, acc: 0.509766]  [A loss: 1.412755, acc: 0.000000]\n",
            "183: [D loss: 0.599791, acc: 0.736328]  [A loss: 0.812453, acc: 0.246094]\n",
            "184: [D loss: 0.678164, acc: 0.509766]  [A loss: 1.557712, acc: 0.000000]\n",
            "185: [D loss: 0.593468, acc: 0.746094]  [A loss: 0.760865, acc: 0.316406]\n",
            "186: [D loss: 0.730619, acc: 0.505859]  [A loss: 1.686255, acc: 0.000000]\n",
            "187: [D loss: 0.679193, acc: 0.537109]  [A loss: 0.625561, acc: 0.726562]\n",
            "188: [D loss: 0.764219, acc: 0.501953]  [A loss: 1.584596, acc: 0.000000]\n",
            "189: [D loss: 0.653680, acc: 0.628906]  [A loss: 0.914271, acc: 0.128906]\n",
            "190: [D loss: 0.648917, acc: 0.562500]  [A loss: 1.106630, acc: 0.007812]\n",
            "191: [D loss: 0.635078, acc: 0.613281]  [A loss: 1.089002, acc: 0.027344]\n",
            "192: [D loss: 0.638230, acc: 0.578125]  [A loss: 1.434088, acc: 0.000000]\n",
            "193: [D loss: 0.608430, acc: 0.716797]  [A loss: 0.773189, acc: 0.335938]\n",
            "194: [D loss: 0.710990, acc: 0.511719]  [A loss: 1.779413, acc: 0.000000]\n",
            "195: [D loss: 0.633154, acc: 0.625000]  [A loss: 0.511610, acc: 0.906250]\n",
            "196: [D loss: 0.803524, acc: 0.498047]  [A loss: 1.650460, acc: 0.000000]\n",
            "197: [D loss: 0.640978, acc: 0.623047]  [A loss: 0.614803, acc: 0.707031]\n",
            "198: [D loss: 0.737155, acc: 0.496094]  [A loss: 1.372737, acc: 0.000000]\n",
            "199: [D loss: 0.615049, acc: 0.738281]  [A loss: 0.791859, acc: 0.222656]\n",
            "200: [D loss: 0.682884, acc: 0.523438]  [A loss: 1.362100, acc: 0.000000]\n",
            "201: [D loss: 0.629084, acc: 0.708984]  [A loss: 0.757304, acc: 0.351562]\n",
            "202: [D loss: 0.691005, acc: 0.505859]  [A loss: 1.545839, acc: 0.000000]\n",
            "203: [D loss: 0.623378, acc: 0.707031]  [A loss: 0.724317, acc: 0.437500]\n",
            "204: [D loss: 0.695255, acc: 0.507812]  [A loss: 1.529090, acc: 0.000000]\n",
            "205: [D loss: 0.630379, acc: 0.685547]  [A loss: 0.621844, acc: 0.671875]\n",
            "206: [D loss: 0.764719, acc: 0.500000]  [A loss: 1.707689, acc: 0.000000]\n",
            "207: [D loss: 0.684052, acc: 0.541016]  [A loss: 0.669500, acc: 0.593750]\n",
            "208: [D loss: 0.750203, acc: 0.498047]  [A loss: 1.353395, acc: 0.000000]\n",
            "209: [D loss: 0.638168, acc: 0.652344]  [A loss: 0.877177, acc: 0.078125]\n",
            "210: [D loss: 0.649912, acc: 0.546875]  [A loss: 1.112123, acc: 0.000000]\n",
            "211: [D loss: 0.640881, acc: 0.607422]  [A loss: 0.986940, acc: 0.050781]\n",
            "212: [D loss: 0.645705, acc: 0.554688]  [A loss: 1.301322, acc: 0.000000]\n",
            "213: [D loss: 0.619590, acc: 0.703125]  [A loss: 0.873905, acc: 0.121094]\n",
            "214: [D loss: 0.664964, acc: 0.523438]  [A loss: 1.705081, acc: 0.000000]\n",
            "215: [D loss: 0.609819, acc: 0.728516]  [A loss: 0.569235, acc: 0.808594]\n",
            "216: [D loss: 0.792862, acc: 0.500000]  [A loss: 1.713764, acc: 0.000000]\n",
            "217: [D loss: 0.658146, acc: 0.576172]  [A loss: 0.562067, acc: 0.816406]\n",
            "218: [D loss: 0.773607, acc: 0.492188]  [A loss: 1.486544, acc: 0.000000]\n",
            "219: [D loss: 0.638158, acc: 0.675781]  [A loss: 0.706355, acc: 0.460938]\n",
            "220: [D loss: 0.698125, acc: 0.509766]  [A loss: 1.305542, acc: 0.011719]\n",
            "221: [D loss: 0.654283, acc: 0.619141]  [A loss: 0.941923, acc: 0.035156]\n",
            "222: [D loss: 0.678394, acc: 0.521484]  [A loss: 1.067044, acc: 0.023438]\n",
            "223: [D loss: 0.659287, acc: 0.603516]  [A loss: 1.254813, acc: 0.000000]\n",
            "224: [D loss: 0.641666, acc: 0.656250]  [A loss: 0.985434, acc: 0.023438]\n",
            "225: [D loss: 0.668364, acc: 0.541016]  [A loss: 1.337340, acc: 0.000000]\n",
            "226: [D loss: 0.613242, acc: 0.744141]  [A loss: 0.851196, acc: 0.191406]\n",
            "227: [D loss: 0.715017, acc: 0.509766]  [A loss: 1.835165, acc: 0.000000]\n",
            "228: [D loss: 0.689741, acc: 0.525391]  [A loss: 0.488483, acc: 0.984375]\n",
            "229: [D loss: 0.845199, acc: 0.500000]  [A loss: 1.459639, acc: 0.000000]\n",
            "230: [D loss: 0.637717, acc: 0.673828]  [A loss: 0.691452, acc: 0.535156]\n",
            "231: [D loss: 0.710674, acc: 0.505859]  [A loss: 1.322621, acc: 0.000000]\n",
            "232: [D loss: 0.652301, acc: 0.656250]  [A loss: 0.693250, acc: 0.546875]\n",
            "233: [D loss: 0.715012, acc: 0.503906]  [A loss: 1.229430, acc: 0.000000]\n",
            "234: [D loss: 0.644332, acc: 0.640625]  [A loss: 0.772373, acc: 0.265625]\n",
            "235: [D loss: 0.690073, acc: 0.531250]  [A loss: 1.218378, acc: 0.000000]\n",
            "236: [D loss: 0.654371, acc: 0.634766]  [A loss: 0.776443, acc: 0.289062]\n",
            "237: [D loss: 0.679105, acc: 0.517578]  [A loss: 1.376529, acc: 0.000000]\n",
            "238: [D loss: 0.649398, acc: 0.626953]  [A loss: 0.726951, acc: 0.425781]\n",
            "239: [D loss: 0.713646, acc: 0.501953]  [A loss: 1.377640, acc: 0.000000]\n",
            "240: [D loss: 0.661617, acc: 0.626953]  [A loss: 0.661576, acc: 0.617188]\n",
            "241: [D loss: 0.726761, acc: 0.494141]  [A loss: 1.364629, acc: 0.003906]\n",
            "242: [D loss: 0.654998, acc: 0.613281]  [A loss: 0.803882, acc: 0.238281]\n",
            "243: [D loss: 0.684935, acc: 0.525391]  [A loss: 1.189481, acc: 0.000000]\n",
            "244: [D loss: 0.669584, acc: 0.583984]  [A loss: 0.848917, acc: 0.136719]\n",
            "245: [D loss: 0.684443, acc: 0.509766]  [A loss: 1.261835, acc: 0.023438]\n",
            "246: [D loss: 0.692967, acc: 0.539062]  [A loss: 0.988413, acc: 0.007812]\n",
            "247: [D loss: 0.671112, acc: 0.564453]  [A loss: 0.998813, acc: 0.011719]\n",
            "248: [D loss: 0.672023, acc: 0.585938]  [A loss: 1.131993, acc: 0.015625]\n",
            "249: [D loss: 0.654321, acc: 0.607422]  [A loss: 0.875033, acc: 0.128906]\n",
            "250: [D loss: 0.674225, acc: 0.544922]  [A loss: 1.398234, acc: 0.000000]\n",
            "251: [D loss: 0.659535, acc: 0.599609]  [A loss: 0.585354, acc: 0.839844]\n",
            "252: [D loss: 0.761830, acc: 0.498047]  [A loss: 1.658578, acc: 0.000000]\n",
            "253: [D loss: 0.693855, acc: 0.511719]  [A loss: 0.492019, acc: 0.980469]\n",
            "254: [D loss: 0.790223, acc: 0.500000]  [A loss: 1.071205, acc: 0.000000]\n",
            "255: [D loss: 0.668541, acc: 0.623047]  [A loss: 0.709688, acc: 0.433594]\n",
            "256: [D loss: 0.697223, acc: 0.498047]  [A loss: 1.083161, acc: 0.007812]\n",
            "257: [D loss: 0.662864, acc: 0.623047]  [A loss: 0.747815, acc: 0.328125]\n",
            "258: [D loss: 0.693990, acc: 0.507812]  [A loss: 1.044696, acc: 0.007812]\n",
            "259: [D loss: 0.663930, acc: 0.621094]  [A loss: 0.847739, acc: 0.121094]\n",
            "260: [D loss: 0.672111, acc: 0.523438]  [A loss: 1.193741, acc: 0.000000]\n",
            "261: [D loss: 0.658150, acc: 0.636719]  [A loss: 0.721302, acc: 0.437500]\n",
            "262: [D loss: 0.710977, acc: 0.500000]  [A loss: 1.391148, acc: 0.000000]\n",
            "263: [D loss: 0.662919, acc: 0.589844]  [A loss: 0.617366, acc: 0.726562]\n",
            "264: [D loss: 0.742376, acc: 0.498047]  [A loss: 1.374691, acc: 0.000000]\n",
            "265: [D loss: 0.698927, acc: 0.537109]  [A loss: 0.664481, acc: 0.628906]\n",
            "266: [D loss: 0.703185, acc: 0.505859]  [A loss: 1.192332, acc: 0.000000]\n",
            "267: [D loss: 0.664909, acc: 0.595703]  [A loss: 0.727647, acc: 0.367188]\n",
            "268: [D loss: 0.691770, acc: 0.509766]  [A loss: 1.015028, acc: 0.000000]\n",
            "269: [D loss: 0.658941, acc: 0.625000]  [A loss: 0.808085, acc: 0.250000]\n",
            "270: [D loss: 0.697765, acc: 0.515625]  [A loss: 1.200850, acc: 0.000000]\n",
            "271: [D loss: 0.677166, acc: 0.583984]  [A loss: 0.749950, acc: 0.339844]\n",
            "272: [D loss: 0.686566, acc: 0.511719]  [A loss: 1.143734, acc: 0.000000]\n",
            "273: [D loss: 0.663841, acc: 0.642578]  [A loss: 0.730906, acc: 0.417969]\n",
            "274: [D loss: 0.707598, acc: 0.503906]  [A loss: 1.153116, acc: 0.000000]\n",
            "275: [D loss: 0.647930, acc: 0.679688]  [A loss: 0.680117, acc: 0.562500]\n",
            "276: [D loss: 0.710876, acc: 0.501953]  [A loss: 1.290639, acc: 0.000000]\n",
            "277: [D loss: 0.673338, acc: 0.576172]  [A loss: 0.633264, acc: 0.769531]\n",
            "278: [D loss: 0.715329, acc: 0.501953]  [A loss: 1.157342, acc: 0.000000]\n",
            "279: [D loss: 0.666678, acc: 0.583984]  [A loss: 0.691826, acc: 0.503906]\n",
            "280: [D loss: 0.709205, acc: 0.505859]  [A loss: 1.112880, acc: 0.000000]\n",
            "281: [D loss: 0.667065, acc: 0.623047]  [A loss: 0.732467, acc: 0.429688]\n",
            "282: [D loss: 0.704063, acc: 0.513672]  [A loss: 1.099673, acc: 0.000000]\n",
            "283: [D loss: 0.667276, acc: 0.611328]  [A loss: 0.734086, acc: 0.386719]\n",
            "284: [D loss: 0.721248, acc: 0.505859]  [A loss: 1.081357, acc: 0.000000]\n",
            "285: [D loss: 0.680717, acc: 0.550781]  [A loss: 0.878739, acc: 0.089844]\n",
            "286: [D loss: 0.671312, acc: 0.541016]  [A loss: 0.915971, acc: 0.062500]\n",
            "287: [D loss: 0.675501, acc: 0.572266]  [A loss: 1.031205, acc: 0.015625]\n",
            "288: [D loss: 0.666819, acc: 0.589844]  [A loss: 0.797220, acc: 0.195312]\n",
            "289: [D loss: 0.683359, acc: 0.523438]  [A loss: 1.220245, acc: 0.000000]\n",
            "290: [D loss: 0.669488, acc: 0.591797]  [A loss: 0.651263, acc: 0.640625]\n",
            "291: [D loss: 0.733903, acc: 0.501953]  [A loss: 1.312909, acc: 0.000000]\n",
            "292: [D loss: 0.684708, acc: 0.560547]  [A loss: 0.607520, acc: 0.800781]\n",
            "293: [D loss: 0.730106, acc: 0.501953]  [A loss: 1.115992, acc: 0.003906]\n",
            "294: [D loss: 0.702138, acc: 0.486328]  [A loss: 0.763111, acc: 0.250000]\n",
            "295: [D loss: 0.678541, acc: 0.537109]  [A loss: 0.959731, acc: 0.093750]\n",
            "296: [D loss: 0.726170, acc: 0.476562]  [A loss: 0.939744, acc: 0.027344]\n",
            "297: [D loss: 0.668977, acc: 0.595703]  [A loss: 0.835380, acc: 0.113281]\n",
            "298: [D loss: 0.682505, acc: 0.531250]  [A loss: 0.928469, acc: 0.039062]\n",
            "299: [D loss: 0.673862, acc: 0.570312]  [A loss: 0.906351, acc: 0.019531]\n",
            "300: [D loss: 0.679453, acc: 0.533203]  [A loss: 0.901553, acc: 0.078125]\n",
            "301: [D loss: 0.688048, acc: 0.537109]  [A loss: 0.950583, acc: 0.019531]\n",
            "302: [D loss: 0.672355, acc: 0.568359]  [A loss: 0.976591, acc: 0.015625]\n",
            "303: [D loss: 0.663750, acc: 0.619141]  [A loss: 0.906468, acc: 0.046875]\n",
            "304: [D loss: 0.678494, acc: 0.548828]  [A loss: 1.012790, acc: 0.003906]\n",
            "305: [D loss: 0.662701, acc: 0.615234]  [A loss: 0.889992, acc: 0.046875]\n",
            "306: [D loss: 0.677630, acc: 0.568359]  [A loss: 1.014195, acc: 0.003906]\n",
            "307: [D loss: 0.674315, acc: 0.628906]  [A loss: 0.905567, acc: 0.054688]\n",
            "308: [D loss: 0.684120, acc: 0.542969]  [A loss: 1.134830, acc: 0.007812]\n",
            "309: [D loss: 0.665238, acc: 0.609375]  [A loss: 0.768911, acc: 0.312500]\n",
            "310: [D loss: 0.713093, acc: 0.507812]  [A loss: 1.370946, acc: 0.000000]\n",
            "311: [D loss: 0.687692, acc: 0.550781]  [A loss: 0.579790, acc: 0.832031]\n",
            "312: [D loss: 0.760665, acc: 0.501953]  [A loss: 1.319424, acc: 0.000000]\n",
            "313: [D loss: 0.716211, acc: 0.496094]  [A loss: 0.702965, acc: 0.480469]\n",
            "314: [D loss: 0.719418, acc: 0.490234]  [A loss: 1.059361, acc: 0.003906]\n",
            "315: [D loss: 0.681524, acc: 0.539062]  [A loss: 0.755015, acc: 0.324219]\n",
            "316: [D loss: 0.684125, acc: 0.525391]  [A loss: 0.901535, acc: 0.035156]\n",
            "317: [D loss: 0.670603, acc: 0.591797]  [A loss: 0.795230, acc: 0.152344]\n",
            "318: [D loss: 0.689176, acc: 0.519531]  [A loss: 0.944819, acc: 0.015625]\n",
            "319: [D loss: 0.670066, acc: 0.582031]  [A loss: 0.827524, acc: 0.121094]\n",
            "320: [D loss: 0.681407, acc: 0.548828]  [A loss: 0.970101, acc: 0.015625]\n",
            "321: [D loss: 0.670756, acc: 0.587891]  [A loss: 0.789871, acc: 0.218750]\n",
            "322: [D loss: 0.684179, acc: 0.527344]  [A loss: 1.032920, acc: 0.000000]\n",
            "323: [D loss: 0.661066, acc: 0.650391]  [A loss: 0.738238, acc: 0.375000]\n",
            "324: [D loss: 0.682527, acc: 0.517578]  [A loss: 1.181075, acc: 0.000000]\n",
            "325: [D loss: 0.666188, acc: 0.593750]  [A loss: 0.647953, acc: 0.652344]\n",
            "326: [D loss: 0.730839, acc: 0.500000]  [A loss: 1.189324, acc: 0.000000]\n",
            "327: [D loss: 0.665420, acc: 0.605469]  [A loss: 0.665554, acc: 0.613281]\n",
            "328: [D loss: 0.714489, acc: 0.507812]  [A loss: 1.117367, acc: 0.000000]\n",
            "329: [D loss: 0.675407, acc: 0.597656]  [A loss: 0.690120, acc: 0.523438]\n",
            "330: [D loss: 0.710320, acc: 0.498047]  [A loss: 1.110168, acc: 0.000000]\n",
            "331: [D loss: 0.676863, acc: 0.570312]  [A loss: 0.706589, acc: 0.433594]\n",
            "332: [D loss: 0.701245, acc: 0.511719]  [A loss: 1.003716, acc: 0.000000]\n",
            "333: [D loss: 0.667054, acc: 0.617188]  [A loss: 0.736269, acc: 0.359375]\n",
            "334: [D loss: 0.697140, acc: 0.525391]  [A loss: 1.043298, acc: 0.003906]\n",
            "335: [D loss: 0.677317, acc: 0.580078]  [A loss: 0.676877, acc: 0.597656]\n",
            "336: [D loss: 0.709635, acc: 0.498047]  [A loss: 1.030374, acc: 0.003906]\n",
            "337: [D loss: 0.666724, acc: 0.636719]  [A loss: 0.731421, acc: 0.375000]\n",
            "338: [D loss: 0.689520, acc: 0.513672]  [A loss: 0.978209, acc: 0.031250]\n",
            "339: [D loss: 0.676652, acc: 0.580078]  [A loss: 0.843313, acc: 0.101562]\n",
            "340: [D loss: 0.676122, acc: 0.539062]  [A loss: 0.881603, acc: 0.093750]\n",
            "341: [D loss: 0.672856, acc: 0.595703]  [A loss: 0.888099, acc: 0.078125]\n",
            "342: [D loss: 0.670087, acc: 0.591797]  [A loss: 0.930531, acc: 0.023438]\n",
            "343: [D loss: 0.668305, acc: 0.593750]  [A loss: 0.892350, acc: 0.066406]\n",
            "344: [D loss: 0.684956, acc: 0.544922]  [A loss: 1.004051, acc: 0.023438]\n",
            "345: [D loss: 0.664856, acc: 0.609375]  [A loss: 0.802024, acc: 0.234375]\n",
            "346: [D loss: 0.692256, acc: 0.525391]  [A loss: 1.157054, acc: 0.000000]\n",
            "347: [D loss: 0.690200, acc: 0.546875]  [A loss: 0.656759, acc: 0.613281]\n",
            "348: [D loss: 0.728455, acc: 0.500000]  [A loss: 1.228455, acc: 0.000000]\n",
            "349: [D loss: 0.678909, acc: 0.562500]  [A loss: 0.591501, acc: 0.796875]\n",
            "350: [D loss: 0.803821, acc: 0.494141]  [A loss: 1.075624, acc: 0.015625]\n",
            "351: [D loss: 0.674365, acc: 0.568359]  [A loss: 0.748328, acc: 0.347656]\n",
            "352: [D loss: 0.694458, acc: 0.529297]  [A loss: 0.934326, acc: 0.027344]\n",
            "353: [D loss: 0.681311, acc: 0.548828]  [A loss: 0.778480, acc: 0.234375]\n",
            "354: [D loss: 0.682455, acc: 0.546875]  [A loss: 0.901933, acc: 0.054688]\n",
            "355: [D loss: 0.673700, acc: 0.566406]  [A loss: 0.818832, acc: 0.156250]\n",
            "356: [D loss: 0.676108, acc: 0.541016]  [A loss: 0.983087, acc: 0.019531]\n",
            "357: [D loss: 0.685600, acc: 0.546875]  [A loss: 0.756186, acc: 0.312500]\n",
            "358: [D loss: 0.694731, acc: 0.517578]  [A loss: 1.087272, acc: 0.000000]\n",
            "359: [D loss: 0.664810, acc: 0.634766]  [A loss: 0.713107, acc: 0.464844]\n",
            "360: [D loss: 0.701036, acc: 0.507812]  [A loss: 1.060340, acc: 0.011719]\n",
            "361: [D loss: 0.673237, acc: 0.556641]  [A loss: 0.699586, acc: 0.445312]\n",
            "362: [D loss: 0.700578, acc: 0.509766]  [A loss: 1.097234, acc: 0.007812]\n",
            "363: [D loss: 0.675823, acc: 0.580078]  [A loss: 0.692427, acc: 0.464844]\n",
            "364: [D loss: 0.692973, acc: 0.503906]  [A loss: 1.043525, acc: 0.011719]\n",
            "365: [D loss: 0.675047, acc: 0.568359]  [A loss: 0.730941, acc: 0.390625]\n",
            "366: [D loss: 0.724348, acc: 0.503906]  [A loss: 1.012538, acc: 0.042969]\n",
            "367: [D loss: 0.683314, acc: 0.550781]  [A loss: 0.872686, acc: 0.085938]\n",
            "368: [D loss: 0.673713, acc: 0.576172]  [A loss: 0.845991, acc: 0.128906]\n",
            "369: [D loss: 0.669127, acc: 0.562500]  [A loss: 0.967899, acc: 0.015625]\n",
            "370: [D loss: 0.674105, acc: 0.589844]  [A loss: 0.806469, acc: 0.183594]\n",
            "371: [D loss: 0.695063, acc: 0.527344]  [A loss: 1.112022, acc: 0.000000]\n",
            "372: [D loss: 0.694348, acc: 0.535156]  [A loss: 0.718281, acc: 0.382812]\n",
            "373: [D loss: 0.696997, acc: 0.517578]  [A loss: 1.103096, acc: 0.019531]\n",
            "374: [D loss: 0.682710, acc: 0.552734]  [A loss: 0.670842, acc: 0.585938]\n",
            "375: [D loss: 0.716660, acc: 0.500000]  [A loss: 1.012291, acc: 0.011719]\n",
            "376: [D loss: 0.674527, acc: 0.580078]  [A loss: 0.723913, acc: 0.414062]\n",
            "377: [D loss: 0.688150, acc: 0.533203]  [A loss: 0.999525, acc: 0.019531]\n",
            "378: [D loss: 0.678741, acc: 0.558594]  [A loss: 0.734779, acc: 0.398438]\n",
            "379: [D loss: 0.702172, acc: 0.496094]  [A loss: 0.993439, acc: 0.003906]\n",
            "380: [D loss: 0.663380, acc: 0.630859]  [A loss: 0.715690, acc: 0.425781]\n",
            "381: [D loss: 0.687183, acc: 0.517578]  [A loss: 1.058116, acc: 0.000000]\n",
            "382: [D loss: 0.684720, acc: 0.544922]  [A loss: 0.681591, acc: 0.558594]\n",
            "383: [D loss: 0.708235, acc: 0.501953]  [A loss: 1.081821, acc: 0.000000]\n",
            "384: [D loss: 0.684336, acc: 0.535156]  [A loss: 0.716268, acc: 0.519531]\n",
            "385: [D loss: 0.705129, acc: 0.513672]  [A loss: 0.940102, acc: 0.003906]\n",
            "386: [D loss: 0.673008, acc: 0.580078]  [A loss: 0.778580, acc: 0.257812]\n",
            "387: [D loss: 0.699551, acc: 0.513672]  [A loss: 1.036739, acc: 0.000000]\n",
            "388: [D loss: 0.678300, acc: 0.587891]  [A loss: 0.699622, acc: 0.476562]\n",
            "389: [D loss: 0.691053, acc: 0.511719]  [A loss: 0.981104, acc: 0.000000]\n",
            "390: [D loss: 0.670177, acc: 0.609375]  [A loss: 0.744273, acc: 0.343750]\n",
            "391: [D loss: 0.690015, acc: 0.517578]  [A loss: 0.964556, acc: 0.050781]\n",
            "392: [D loss: 0.681100, acc: 0.566406]  [A loss: 0.768206, acc: 0.269531]\n",
            "393: [D loss: 0.676707, acc: 0.523438]  [A loss: 0.940245, acc: 0.027344]\n",
            "394: [D loss: 0.670572, acc: 0.595703]  [A loss: 0.773840, acc: 0.265625]\n",
            "395: [D loss: 0.700890, acc: 0.535156]  [A loss: 0.993198, acc: 0.011719]\n",
            "396: [D loss: 0.666807, acc: 0.623047]  [A loss: 0.732088, acc: 0.394531]\n",
            "397: [D loss: 0.712389, acc: 0.519531]  [A loss: 1.030909, acc: 0.007812]\n",
            "398: [D loss: 0.675489, acc: 0.599609]  [A loss: 0.724459, acc: 0.421875]\n",
            "399: [D loss: 0.704873, acc: 0.503906]  [A loss: 1.072949, acc: 0.003906]\n",
            "400: [D loss: 0.682892, acc: 0.574219]  [A loss: 0.699518, acc: 0.519531]\n",
            "401: [D loss: 0.689556, acc: 0.513672]  [A loss: 1.015903, acc: 0.019531]\n",
            "402: [D loss: 0.694848, acc: 0.521484]  [A loss: 0.759012, acc: 0.382812]\n",
            "403: [D loss: 0.688609, acc: 0.537109]  [A loss: 0.935904, acc: 0.019531]\n",
            "404: [D loss: 0.678676, acc: 0.535156]  [A loss: 0.804626, acc: 0.179688]\n",
            "405: [D loss: 0.681651, acc: 0.541016]  [A loss: 0.935926, acc: 0.054688]\n",
            "406: [D loss: 0.680090, acc: 0.560547]  [A loss: 0.862639, acc: 0.132812]\n",
            "407: [D loss: 0.684056, acc: 0.537109]  [A loss: 0.852167, acc: 0.105469]\n",
            "408: [D loss: 0.681779, acc: 0.556641]  [A loss: 0.875989, acc: 0.082031]\n",
            "409: [D loss: 0.670083, acc: 0.603516]  [A loss: 0.908486, acc: 0.054688]\n",
            "410: [D loss: 0.672863, acc: 0.580078]  [A loss: 0.826335, acc: 0.175781]\n",
            "411: [D loss: 0.684702, acc: 0.556641]  [A loss: 0.945517, acc: 0.046875]\n",
            "412: [D loss: 0.682239, acc: 0.566406]  [A loss: 0.788496, acc: 0.222656]\n",
            "413: [D loss: 0.692845, acc: 0.529297]  [A loss: 1.109068, acc: 0.003906]\n",
            "414: [D loss: 0.663224, acc: 0.603516]  [A loss: 0.647627, acc: 0.660156]\n",
            "415: [D loss: 0.716309, acc: 0.519531]  [A loss: 1.177273, acc: 0.000000]\n",
            "416: [D loss: 0.680050, acc: 0.544922]  [A loss: 0.615261, acc: 0.796875]\n",
            "417: [D loss: 0.733819, acc: 0.500000]  [A loss: 1.009230, acc: 0.019531]\n",
            "418: [D loss: 0.670923, acc: 0.582031]  [A loss: 0.728166, acc: 0.437500]\n",
            "419: [D loss: 0.689084, acc: 0.541016]  [A loss: 0.929735, acc: 0.031250]\n",
            "420: [D loss: 0.677516, acc: 0.568359]  [A loss: 0.762327, acc: 0.296875]\n",
            "421: [D loss: 0.693321, acc: 0.519531]  [A loss: 0.943625, acc: 0.035156]\n",
            "422: [D loss: 0.673025, acc: 0.597656]  [A loss: 0.798321, acc: 0.203125]\n",
            "423: [D loss: 0.684315, acc: 0.560547]  [A loss: 0.887258, acc: 0.085938]\n",
            "424: [D loss: 0.679058, acc: 0.568359]  [A loss: 0.799876, acc: 0.183594]\n",
            "425: [D loss: 0.687806, acc: 0.548828]  [A loss: 0.893185, acc: 0.058594]\n",
            "426: [D loss: 0.669167, acc: 0.621094]  [A loss: 0.864654, acc: 0.132812]\n",
            "427: [D loss: 0.684315, acc: 0.564453]  [A loss: 0.885098, acc: 0.085938]\n",
            "428: [D loss: 0.682932, acc: 0.560547]  [A loss: 0.897576, acc: 0.042969]\n",
            "429: [D loss: 0.658533, acc: 0.654297]  [A loss: 0.821954, acc: 0.128906]\n",
            "430: [D loss: 0.674422, acc: 0.570312]  [A loss: 0.988779, acc: 0.019531]\n",
            "431: [D loss: 0.668608, acc: 0.613281]  [A loss: 0.771009, acc: 0.281250]\n",
            "432: [D loss: 0.709561, acc: 0.513672]  [A loss: 1.159266, acc: 0.003906]\n",
            "433: [D loss: 0.672391, acc: 0.595703]  [A loss: 0.670422, acc: 0.617188]\n",
            "434: [D loss: 0.711821, acc: 0.503906]  [A loss: 1.265961, acc: 0.000000]\n",
            "435: [D loss: 0.686414, acc: 0.525391]  [A loss: 0.592254, acc: 0.851562]\n",
            "436: [D loss: 0.722977, acc: 0.503906]  [A loss: 1.015335, acc: 0.007812]\n",
            "437: [D loss: 0.676195, acc: 0.548828]  [A loss: 0.730206, acc: 0.398438]\n",
            "438: [D loss: 0.702296, acc: 0.535156]  [A loss: 0.892985, acc: 0.070312]\n",
            "439: [D loss: 0.681047, acc: 0.566406]  [A loss: 0.833026, acc: 0.105469]\n",
            "440: [D loss: 0.678537, acc: 0.552734]  [A loss: 0.826601, acc: 0.140625]\n",
            "441: [D loss: 0.683459, acc: 0.548828]  [A loss: 0.854138, acc: 0.089844]\n",
            "442: [D loss: 0.678766, acc: 0.556641]  [A loss: 0.847400, acc: 0.121094]\n",
            "443: [D loss: 0.673233, acc: 0.568359]  [A loss: 0.928847, acc: 0.050781]\n",
            "444: [D loss: 0.673578, acc: 0.564453]  [A loss: 0.801571, acc: 0.210938]\n",
            "445: [D loss: 0.686204, acc: 0.541016]  [A loss: 0.968543, acc: 0.027344]\n",
            "446: [D loss: 0.676945, acc: 0.599609]  [A loss: 0.753824, acc: 0.312500]\n",
            "447: [D loss: 0.684261, acc: 0.517578]  [A loss: 1.015201, acc: 0.007812]\n",
            "448: [D loss: 0.669479, acc: 0.605469]  [A loss: 0.712347, acc: 0.433594]\n",
            "449: [D loss: 0.697409, acc: 0.505859]  [A loss: 1.051928, acc: 0.019531]\n",
            "450: [D loss: 0.683216, acc: 0.554688]  [A loss: 0.707227, acc: 0.496094]\n",
            "451: [D loss: 0.722759, acc: 0.517578]  [A loss: 1.004919, acc: 0.023438]\n",
            "452: [D loss: 0.671028, acc: 0.593750]  [A loss: 0.782733, acc: 0.273438]\n",
            "453: [D loss: 0.701781, acc: 0.511719]  [A loss: 1.057558, acc: 0.000000]\n",
            "454: [D loss: 0.686621, acc: 0.552734]  [A loss: 0.750721, acc: 0.308594]\n",
            "455: [D loss: 0.683245, acc: 0.517578]  [A loss: 0.954406, acc: 0.039062]\n",
            "456: [D loss: 0.685716, acc: 0.564453]  [A loss: 0.828233, acc: 0.175781]\n",
            "457: [D loss: 0.671721, acc: 0.591797]  [A loss: 0.848073, acc: 0.128906]\n",
            "458: [D loss: 0.665764, acc: 0.605469]  [A loss: 0.858767, acc: 0.121094]\n",
            "459: [D loss: 0.669224, acc: 0.603516]  [A loss: 0.878118, acc: 0.109375]\n",
            "460: [D loss: 0.677479, acc: 0.570312]  [A loss: 0.939617, acc: 0.031250]\n",
            "461: [D loss: 0.668393, acc: 0.582031]  [A loss: 0.821087, acc: 0.171875]\n",
            "462: [D loss: 0.672692, acc: 0.576172]  [A loss: 0.922315, acc: 0.039062]\n",
            "463: [D loss: 0.680947, acc: 0.554688]  [A loss: 0.784008, acc: 0.226562]\n",
            "464: [D loss: 0.679903, acc: 0.558594]  [A loss: 0.985845, acc: 0.027344]\n",
            "465: [D loss: 0.665076, acc: 0.611328]  [A loss: 0.793723, acc: 0.269531]\n",
            "466: [D loss: 0.684816, acc: 0.539062]  [A loss: 1.068541, acc: 0.003906]\n",
            "467: [D loss: 0.669967, acc: 0.605469]  [A loss: 0.665664, acc: 0.597656]\n",
            "468: [D loss: 0.700297, acc: 0.507812]  [A loss: 1.207469, acc: 0.000000]\n",
            "469: [D loss: 0.687012, acc: 0.542969]  [A loss: 0.601515, acc: 0.816406]\n",
            "470: [D loss: 0.727180, acc: 0.503906]  [A loss: 1.018258, acc: 0.019531]\n",
            "471: [D loss: 0.669608, acc: 0.587891]  [A loss: 0.703742, acc: 0.464844]\n",
            "472: [D loss: 0.686827, acc: 0.511719]  [A loss: 0.973349, acc: 0.046875]\n",
            "473: [D loss: 0.670219, acc: 0.597656]  [A loss: 0.772971, acc: 0.277344]\n",
            "474: [D loss: 0.681669, acc: 0.529297]  [A loss: 0.917392, acc: 0.027344]\n",
            "475: [D loss: 0.672332, acc: 0.591797]  [A loss: 0.806552, acc: 0.203125]\n",
            "476: [D loss: 0.679586, acc: 0.556641]  [A loss: 0.904866, acc: 0.058594]\n",
            "477: [D loss: 0.678730, acc: 0.554688]  [A loss: 0.816533, acc: 0.144531]\n",
            "478: [D loss: 0.680540, acc: 0.576172]  [A loss: 0.931163, acc: 0.031250]\n",
            "479: [D loss: 0.666209, acc: 0.617188]  [A loss: 0.776326, acc: 0.292969]\n",
            "480: [D loss: 0.690628, acc: 0.541016]  [A loss: 0.984885, acc: 0.042969]\n",
            "481: [D loss: 0.671647, acc: 0.580078]  [A loss: 0.842851, acc: 0.187500]\n",
            "482: [D loss: 0.681177, acc: 0.560547]  [A loss: 0.938016, acc: 0.027344]\n",
            "483: [D loss: 0.668246, acc: 0.607422]  [A loss: 0.795194, acc: 0.261719]\n",
            "484: [D loss: 0.681989, acc: 0.556641]  [A loss: 1.044931, acc: 0.019531]\n",
            "485: [D loss: 0.657005, acc: 0.638672]  [A loss: 0.695600, acc: 0.585938]\n",
            "486: [D loss: 0.704303, acc: 0.513672]  [A loss: 1.052822, acc: 0.007812]\n",
            "487: [D loss: 0.666827, acc: 0.593750]  [A loss: 0.718290, acc: 0.453125]\n",
            "488: [D loss: 0.689854, acc: 0.533203]  [A loss: 1.092885, acc: 0.003906]\n",
            "489: [D loss: 0.668726, acc: 0.585938]  [A loss: 0.691035, acc: 0.535156]\n",
            "490: [D loss: 0.718774, acc: 0.496094]  [A loss: 1.081310, acc: 0.003906]\n",
            "491: [D loss: 0.690967, acc: 0.503906]  [A loss: 0.721866, acc: 0.390625]\n",
            "492: [D loss: 0.691067, acc: 0.535156]  [A loss: 0.941672, acc: 0.070312]\n",
            "493: [D loss: 0.682274, acc: 0.558594]  [A loss: 0.786944, acc: 0.207031]\n",
            "494: [D loss: 0.678394, acc: 0.544922]  [A loss: 0.876307, acc: 0.074219]\n",
            "495: [D loss: 0.683171, acc: 0.562500]  [A loss: 0.854914, acc: 0.074219]\n",
            "496: [D loss: 0.680406, acc: 0.556641]  [A loss: 0.849803, acc: 0.164062]\n",
            "497: [D loss: 0.681103, acc: 0.542969]  [A loss: 0.927535, acc: 0.042969]\n",
            "498: [D loss: 0.667246, acc: 0.615234]  [A loss: 0.764225, acc: 0.277344]\n",
            "499: [D loss: 0.682207, acc: 0.550781]  [A loss: 0.941971, acc: 0.062500]\n",
            "500: [D loss: 0.671894, acc: 0.603516]  [A loss: 0.810495, acc: 0.195312]\n",
            "501: [D loss: 0.682981, acc: 0.552734]  [A loss: 0.959989, acc: 0.031250]\n",
            "502: [D loss: 0.667952, acc: 0.619141]  [A loss: 0.759248, acc: 0.320312]\n",
            "503: [D loss: 0.684842, acc: 0.544922]  [A loss: 1.005765, acc: 0.015625]\n",
            "504: [D loss: 0.662758, acc: 0.619141]  [A loss: 0.726790, acc: 0.500000]\n",
            "505: [D loss: 0.715970, acc: 0.513672]  [A loss: 1.112466, acc: 0.003906]\n",
            "506: [D loss: 0.673622, acc: 0.560547]  [A loss: 0.677701, acc: 0.539062]\n",
            "507: [D loss: 0.701992, acc: 0.515625]  [A loss: 0.999887, acc: 0.039062]\n",
            "508: [D loss: 0.670443, acc: 0.591797]  [A loss: 0.729365, acc: 0.363281]\n",
            "509: [D loss: 0.698150, acc: 0.535156]  [A loss: 0.958779, acc: 0.031250]\n",
            "510: [D loss: 0.666416, acc: 0.589844]  [A loss: 0.749541, acc: 0.343750]\n",
            "511: [D loss: 0.690477, acc: 0.519531]  [A loss: 0.940034, acc: 0.062500]\n",
            "512: [D loss: 0.663030, acc: 0.601562]  [A loss: 0.833534, acc: 0.156250]\n",
            "513: [D loss: 0.682876, acc: 0.560547]  [A loss: 0.965161, acc: 0.031250]\n",
            "514: [D loss: 0.661269, acc: 0.625000]  [A loss: 0.737626, acc: 0.343750]\n",
            "515: [D loss: 0.691997, acc: 0.519531]  [A loss: 1.020317, acc: 0.011719]\n",
            "516: [D loss: 0.684354, acc: 0.580078]  [A loss: 0.780427, acc: 0.261719]\n",
            "517: [D loss: 0.684717, acc: 0.513672]  [A loss: 0.885591, acc: 0.058594]\n",
            "518: [D loss: 0.668303, acc: 0.621094]  [A loss: 0.772556, acc: 0.269531]\n",
            "519: [D loss: 0.686306, acc: 0.554688]  [A loss: 0.938014, acc: 0.027344]\n",
            "520: [D loss: 0.678770, acc: 0.542969]  [A loss: 0.822540, acc: 0.210938]\n",
            "521: [D loss: 0.673262, acc: 0.558594]  [A loss: 0.952994, acc: 0.058594]\n",
            "522: [D loss: 0.667351, acc: 0.617188]  [A loss: 0.813154, acc: 0.160156]\n",
            "523: [D loss: 0.680890, acc: 0.554688]  [A loss: 0.973943, acc: 0.027344]\n",
            "524: [D loss: 0.657589, acc: 0.615234]  [A loss: 0.825759, acc: 0.222656]\n",
            "525: [D loss: 0.681300, acc: 0.566406]  [A loss: 0.942253, acc: 0.042969]\n",
            "526: [D loss: 0.675629, acc: 0.572266]  [A loss: 0.889782, acc: 0.136719]\n",
            "527: [D loss: 0.701496, acc: 0.527344]  [A loss: 0.985632, acc: 0.042969]\n",
            "528: [D loss: 0.665702, acc: 0.587891]  [A loss: 0.848408, acc: 0.171875]\n",
            "529: [D loss: 0.682684, acc: 0.562500]  [A loss: 1.044105, acc: 0.011719]\n",
            "530: [D loss: 0.678227, acc: 0.558594]  [A loss: 0.678005, acc: 0.582031]\n",
            "531: [D loss: 0.693304, acc: 0.505859]  [A loss: 1.111769, acc: 0.027344]\n",
            "532: [D loss: 0.687233, acc: 0.558594]  [A loss: 0.743664, acc: 0.410156]\n",
            "533: [D loss: 0.689826, acc: 0.546875]  [A loss: 0.988919, acc: 0.019531]\n",
            "534: [D loss: 0.671626, acc: 0.585938]  [A loss: 0.732033, acc: 0.406250]\n",
            "535: [D loss: 0.702008, acc: 0.539062]  [A loss: 1.035764, acc: 0.003906]\n",
            "536: [D loss: 0.676754, acc: 0.578125]  [A loss: 0.741589, acc: 0.402344]\n",
            "537: [D loss: 0.688898, acc: 0.533203]  [A loss: 0.969467, acc: 0.039062]\n",
            "538: [D loss: 0.680971, acc: 0.576172]  [A loss: 0.745439, acc: 0.320312]\n",
            "539: [D loss: 0.669066, acc: 0.546875]  [A loss: 0.919195, acc: 0.109375]\n",
            "540: [D loss: 0.662917, acc: 0.619141]  [A loss: 0.796468, acc: 0.199219]\n",
            "541: [D loss: 0.677517, acc: 0.568359]  [A loss: 0.892828, acc: 0.082031]\n",
            "542: [D loss: 0.670671, acc: 0.585938]  [A loss: 0.866435, acc: 0.148438]\n",
            "543: [D loss: 0.679982, acc: 0.576172]  [A loss: 0.935125, acc: 0.039062]\n",
            "544: [D loss: 0.677943, acc: 0.568359]  [A loss: 0.848628, acc: 0.101562]\n",
            "545: [D loss: 0.677560, acc: 0.544922]  [A loss: 0.958752, acc: 0.039062]\n",
            "546: [D loss: 0.670585, acc: 0.593750]  [A loss: 0.824802, acc: 0.195312]\n",
            "547: [D loss: 0.677388, acc: 0.552734]  [A loss: 0.951011, acc: 0.050781]\n",
            "548: [D loss: 0.677072, acc: 0.554688]  [A loss: 0.776046, acc: 0.273438]\n",
            "549: [D loss: 0.688116, acc: 0.566406]  [A loss: 1.031391, acc: 0.003906]\n",
            "550: [D loss: 0.669474, acc: 0.623047]  [A loss: 0.716259, acc: 0.441406]\n",
            "551: [D loss: 0.690682, acc: 0.533203]  [A loss: 1.067804, acc: 0.019531]\n",
            "552: [D loss: 0.669047, acc: 0.589844]  [A loss: 0.676687, acc: 0.605469]\n",
            "553: [D loss: 0.717938, acc: 0.509766]  [A loss: 1.088520, acc: 0.015625]\n",
            "554: [D loss: 0.674323, acc: 0.576172]  [A loss: 0.680337, acc: 0.593750]\n",
            "555: [D loss: 0.710141, acc: 0.501953]  [A loss: 0.994393, acc: 0.031250]\n",
            "556: [D loss: 0.675739, acc: 0.552734]  [A loss: 0.763635, acc: 0.296875]\n",
            "557: [D loss: 0.676323, acc: 0.564453]  [A loss: 0.914574, acc: 0.042969]\n",
            "558: [D loss: 0.670133, acc: 0.601562]  [A loss: 0.805783, acc: 0.222656]\n",
            "559: [D loss: 0.677049, acc: 0.531250]  [A loss: 0.862596, acc: 0.128906]\n",
            "560: [D loss: 0.662409, acc: 0.617188]  [A loss: 0.832919, acc: 0.195312]\n",
            "561: [D loss: 0.682279, acc: 0.548828]  [A loss: 0.895661, acc: 0.058594]\n",
            "562: [D loss: 0.679095, acc: 0.591797]  [A loss: 0.889411, acc: 0.066406]\n",
            "563: [D loss: 0.682572, acc: 0.562500]  [A loss: 0.867747, acc: 0.097656]\n",
            "564: [D loss: 0.656375, acc: 0.615234]  [A loss: 0.841296, acc: 0.171875]\n",
            "565: [D loss: 0.667287, acc: 0.613281]  [A loss: 0.894056, acc: 0.097656]\n",
            "566: [D loss: 0.669688, acc: 0.585938]  [A loss: 0.846139, acc: 0.195312]\n",
            "567: [D loss: 0.676054, acc: 0.566406]  [A loss: 0.940380, acc: 0.066406]\n",
            "568: [D loss: 0.671600, acc: 0.560547]  [A loss: 0.779050, acc: 0.269531]\n",
            "569: [D loss: 0.690247, acc: 0.544922]  [A loss: 1.092607, acc: 0.007812]\n",
            "570: [D loss: 0.668881, acc: 0.583984]  [A loss: 0.677759, acc: 0.539062]\n",
            "571: [D loss: 0.722730, acc: 0.525391]  [A loss: 1.088208, acc: 0.003906]\n",
            "572: [D loss: 0.672612, acc: 0.576172]  [A loss: 0.666841, acc: 0.644531]\n",
            "573: [D loss: 0.701828, acc: 0.501953]  [A loss: 1.001320, acc: 0.082031]\n",
            "574: [D loss: 0.673163, acc: 0.601562]  [A loss: 0.834856, acc: 0.164062]\n",
            "575: [D loss: 0.685535, acc: 0.550781]  [A loss: 0.887470, acc: 0.085938]\n",
            "576: [D loss: 0.670297, acc: 0.560547]  [A loss: 0.862761, acc: 0.109375]\n",
            "577: [D loss: 0.682847, acc: 0.554688]  [A loss: 0.864179, acc: 0.121094]\n",
            "578: [D loss: 0.670450, acc: 0.576172]  [A loss: 0.859500, acc: 0.148438]\n",
            "579: [D loss: 0.676522, acc: 0.570312]  [A loss: 0.890060, acc: 0.101562]\n",
            "580: [D loss: 0.670988, acc: 0.580078]  [A loss: 0.845275, acc: 0.152344]\n",
            "581: [D loss: 0.669507, acc: 0.574219]  [A loss: 0.909468, acc: 0.085938]\n",
            "582: [D loss: 0.672284, acc: 0.566406]  [A loss: 0.808258, acc: 0.253906]\n",
            "583: [D loss: 0.691239, acc: 0.550781]  [A loss: 1.026537, acc: 0.015625]\n",
            "584: [D loss: 0.668934, acc: 0.609375]  [A loss: 0.745291, acc: 0.347656]\n",
            "585: [D loss: 0.691522, acc: 0.556641]  [A loss: 0.972972, acc: 0.078125]\n",
            "586: [D loss: 0.682206, acc: 0.572266]  [A loss: 0.824906, acc: 0.210938]\n",
            "587: [D loss: 0.664904, acc: 0.589844]  [A loss: 0.946782, acc: 0.035156]\n",
            "588: [D loss: 0.666425, acc: 0.603516]  [A loss: 0.761537, acc: 0.300781]\n",
            "589: [D loss: 0.694596, acc: 0.523438]  [A loss: 1.073768, acc: 0.007812]\n",
            "590: [D loss: 0.682212, acc: 0.570312]  [A loss: 0.695625, acc: 0.539062]\n",
            "591: [D loss: 0.691871, acc: 0.537109]  [A loss: 1.001758, acc: 0.035156]\n",
            "592: [D loss: 0.669201, acc: 0.587891]  [A loss: 0.753865, acc: 0.359375]\n",
            "593: [D loss: 0.685451, acc: 0.541016]  [A loss: 0.970484, acc: 0.039062]\n",
            "594: [D loss: 0.665336, acc: 0.589844]  [A loss: 0.774385, acc: 0.304688]\n",
            "595: [D loss: 0.709804, acc: 0.501953]  [A loss: 1.042448, acc: 0.011719]\n",
            "596: [D loss: 0.679173, acc: 0.582031]  [A loss: 0.745834, acc: 0.375000]\n",
            "597: [D loss: 0.682587, acc: 0.505859]  [A loss: 0.882938, acc: 0.109375]\n",
            "598: [D loss: 0.668281, acc: 0.572266]  [A loss: 0.802760, acc: 0.238281]\n",
            "599: [D loss: 0.684392, acc: 0.539062]  [A loss: 0.898434, acc: 0.093750]\n",
            "600: [D loss: 0.666119, acc: 0.609375]  [A loss: 0.861146, acc: 0.121094]\n",
            "601: [D loss: 0.671342, acc: 0.578125]  [A loss: 0.865070, acc: 0.140625]\n",
            "602: [D loss: 0.684746, acc: 0.570312]  [A loss: 0.919377, acc: 0.093750]\n",
            "603: [D loss: 0.678838, acc: 0.544922]  [A loss: 0.861991, acc: 0.132812]\n",
            "604: [D loss: 0.675666, acc: 0.582031]  [A loss: 0.942130, acc: 0.074219]\n",
            "605: [D loss: 0.666554, acc: 0.609375]  [A loss: 0.840459, acc: 0.160156]\n",
            "606: [D loss: 0.660519, acc: 0.595703]  [A loss: 0.914257, acc: 0.085938]\n",
            "607: [D loss: 0.662675, acc: 0.623047]  [A loss: 0.855534, acc: 0.171875]\n",
            "608: [D loss: 0.674758, acc: 0.562500]  [A loss: 1.069001, acc: 0.011719]\n",
            "609: [D loss: 0.677032, acc: 0.570312]  [A loss: 0.720529, acc: 0.437500]\n",
            "610: [D loss: 0.693250, acc: 0.523438]  [A loss: 1.102745, acc: 0.011719]\n",
            "611: [D loss: 0.669246, acc: 0.580078]  [A loss: 0.698276, acc: 0.535156]\n",
            "612: [D loss: 0.720586, acc: 0.500000]  [A loss: 1.078184, acc: 0.003906]\n",
            "613: [D loss: 0.677040, acc: 0.568359]  [A loss: 0.781899, acc: 0.289062]\n",
            "614: [D loss: 0.684473, acc: 0.542969]  [A loss: 0.941569, acc: 0.093750]\n",
            "615: [D loss: 0.664625, acc: 0.621094]  [A loss: 0.817550, acc: 0.171875]\n",
            "616: [D loss: 0.679652, acc: 0.560547]  [A loss: 0.898838, acc: 0.050781]\n",
            "617: [D loss: 0.668368, acc: 0.583984]  [A loss: 0.839604, acc: 0.171875]\n",
            "618: [D loss: 0.681800, acc: 0.533203]  [A loss: 0.935050, acc: 0.078125]\n",
            "619: [D loss: 0.674880, acc: 0.574219]  [A loss: 0.878902, acc: 0.105469]\n",
            "620: [D loss: 0.674641, acc: 0.533203]  [A loss: 0.845984, acc: 0.156250]\n",
            "621: [D loss: 0.670224, acc: 0.562500]  [A loss: 0.918867, acc: 0.093750]\n",
            "622: [D loss: 0.671697, acc: 0.566406]  [A loss: 0.790964, acc: 0.257812]\n",
            "623: [D loss: 0.681050, acc: 0.550781]  [A loss: 0.975905, acc: 0.031250]\n",
            "624: [D loss: 0.671994, acc: 0.601562]  [A loss: 0.808095, acc: 0.230469]\n",
            "625: [D loss: 0.683190, acc: 0.554688]  [A loss: 0.992640, acc: 0.046875]\n",
            "626: [D loss: 0.670031, acc: 0.576172]  [A loss: 0.778352, acc: 0.343750]\n",
            "627: [D loss: 0.695910, acc: 0.541016]  [A loss: 1.032605, acc: 0.035156]\n",
            "628: [D loss: 0.678434, acc: 0.564453]  [A loss: 0.770733, acc: 0.308594]\n",
            "629: [D loss: 0.700191, acc: 0.515625]  [A loss: 1.045140, acc: 0.039062]\n",
            "630: [D loss: 0.680806, acc: 0.570312]  [A loss: 0.700845, acc: 0.488281]\n",
            "631: [D loss: 0.697232, acc: 0.531250]  [A loss: 0.971333, acc: 0.046875]\n",
            "632: [D loss: 0.670060, acc: 0.558594]  [A loss: 0.762517, acc: 0.296875]\n",
            "633: [D loss: 0.684999, acc: 0.544922]  [A loss: 1.012082, acc: 0.023438]\n",
            "634: [D loss: 0.677370, acc: 0.535156]  [A loss: 0.799080, acc: 0.253906]\n",
            "635: [D loss: 0.688295, acc: 0.556641]  [A loss: 0.914997, acc: 0.089844]\n",
            "636: [D loss: 0.654179, acc: 0.626953]  [A loss: 0.777521, acc: 0.289062]\n",
            "637: [D loss: 0.721091, acc: 0.523438]  [A loss: 1.045815, acc: 0.007812]\n",
            "638: [D loss: 0.677880, acc: 0.572266]  [A loss: 0.785794, acc: 0.289062]\n",
            "639: [D loss: 0.693766, acc: 0.548828]  [A loss: 0.972246, acc: 0.035156]\n",
            "640: [D loss: 0.669983, acc: 0.582031]  [A loss: 0.771454, acc: 0.289062]\n",
            "641: [D loss: 0.680615, acc: 0.566406]  [A loss: 0.946672, acc: 0.085938]\n",
            "642: [D loss: 0.667443, acc: 0.599609]  [A loss: 0.873406, acc: 0.121094]\n",
            "643: [D loss: 0.677014, acc: 0.544922]  [A loss: 0.918131, acc: 0.066406]\n",
            "644: [D loss: 0.680283, acc: 0.558594]  [A loss: 0.816864, acc: 0.218750]\n",
            "645: [D loss: 0.681018, acc: 0.546875]  [A loss: 0.945143, acc: 0.097656]\n",
            "646: [D loss: 0.676148, acc: 0.546875]  [A loss: 0.843571, acc: 0.234375]\n",
            "647: [D loss: 0.683331, acc: 0.562500]  [A loss: 0.957095, acc: 0.035156]\n",
            "648: [D loss: 0.675279, acc: 0.572266]  [A loss: 0.791600, acc: 0.253906]\n",
            "649: [D loss: 0.676451, acc: 0.562500]  [A loss: 0.976689, acc: 0.031250]\n",
            "650: [D loss: 0.676850, acc: 0.605469]  [A loss: 0.781257, acc: 0.273438]\n",
            "651: [D loss: 0.681346, acc: 0.546875]  [A loss: 0.968618, acc: 0.046875]\n",
            "652: [D loss: 0.656903, acc: 0.634766]  [A loss: 0.765889, acc: 0.312500]\n",
            "653: [D loss: 0.682255, acc: 0.548828]  [A loss: 1.084288, acc: 0.031250]\n",
            "654: [D loss: 0.674143, acc: 0.570312]  [A loss: 0.696261, acc: 0.535156]\n",
            "655: [D loss: 0.738364, acc: 0.501953]  [A loss: 1.010757, acc: 0.011719]\n",
            "656: [D loss: 0.691539, acc: 0.550781]  [A loss: 0.802006, acc: 0.214844]\n",
            "657: [D loss: 0.697749, acc: 0.501953]  [A loss: 0.934945, acc: 0.027344]\n",
            "658: [D loss: 0.665244, acc: 0.638672]  [A loss: 0.735901, acc: 0.410156]\n",
            "659: [D loss: 0.692058, acc: 0.548828]  [A loss: 0.961823, acc: 0.046875]\n",
            "660: [D loss: 0.652428, acc: 0.644531]  [A loss: 0.783513, acc: 0.261719]\n",
            "661: [D loss: 0.696157, acc: 0.558594]  [A loss: 1.000476, acc: 0.023438]\n",
            "662: [D loss: 0.680403, acc: 0.582031]  [A loss: 0.789762, acc: 0.257812]\n",
            "663: [D loss: 0.673638, acc: 0.550781]  [A loss: 0.943679, acc: 0.058594]\n",
            "664: [D loss: 0.663386, acc: 0.587891]  [A loss: 0.803569, acc: 0.230469]\n",
            "665: [D loss: 0.685599, acc: 0.554688]  [A loss: 0.924355, acc: 0.054688]\n",
            "666: [D loss: 0.663872, acc: 0.628906]  [A loss: 0.812916, acc: 0.207031]\n",
            "667: [D loss: 0.676565, acc: 0.533203]  [A loss: 0.965959, acc: 0.050781]\n",
            "668: [D loss: 0.681383, acc: 0.570312]  [A loss: 0.814389, acc: 0.218750]\n",
            "669: [D loss: 0.681038, acc: 0.539062]  [A loss: 0.957768, acc: 0.078125]\n",
            "670: [D loss: 0.673335, acc: 0.587891]  [A loss: 0.761378, acc: 0.316406]\n",
            "671: [D loss: 0.695545, acc: 0.529297]  [A loss: 1.011294, acc: 0.019531]\n",
            "672: [D loss: 0.667509, acc: 0.578125]  [A loss: 0.819018, acc: 0.222656]\n",
            "673: [D loss: 0.674280, acc: 0.562500]  [A loss: 0.958424, acc: 0.042969]\n",
            "674: [D loss: 0.669393, acc: 0.589844]  [A loss: 0.805342, acc: 0.218750]\n",
            "675: [D loss: 0.675641, acc: 0.558594]  [A loss: 0.932702, acc: 0.058594]\n",
            "676: [D loss: 0.674042, acc: 0.605469]  [A loss: 0.830030, acc: 0.183594]\n",
            "677: [D loss: 0.677074, acc: 0.552734]  [A loss: 1.012007, acc: 0.039062]\n",
            "678: [D loss: 0.663926, acc: 0.593750]  [A loss: 0.734986, acc: 0.382812]\n",
            "679: [D loss: 0.701513, acc: 0.521484]  [A loss: 1.101665, acc: 0.035156]\n",
            "680: [D loss: 0.674222, acc: 0.566406]  [A loss: 0.765825, acc: 0.304688]\n",
            "681: [D loss: 0.705916, acc: 0.531250]  [A loss: 1.057156, acc: 0.031250]\n",
            "682: [D loss: 0.680007, acc: 0.544922]  [A loss: 0.733451, acc: 0.410156]\n",
            "683: [D loss: 0.686146, acc: 0.546875]  [A loss: 0.978588, acc: 0.085938]\n",
            "684: [D loss: 0.676142, acc: 0.570312]  [A loss: 0.878110, acc: 0.097656]\n",
            "685: [D loss: 0.688173, acc: 0.541016]  [A loss: 0.950736, acc: 0.078125]\n",
            "686: [D loss: 0.670049, acc: 0.605469]  [A loss: 0.854754, acc: 0.148438]\n",
            "687: [D loss: 0.669603, acc: 0.578125]  [A loss: 0.899689, acc: 0.085938]\n",
            "688: [D loss: 0.666586, acc: 0.599609]  [A loss: 0.868122, acc: 0.125000]\n",
            "689: [D loss: 0.672869, acc: 0.580078]  [A loss: 0.958454, acc: 0.054688]\n",
            "690: [D loss: 0.676394, acc: 0.560547]  [A loss: 0.834481, acc: 0.171875]\n",
            "691: [D loss: 0.687081, acc: 0.537109]  [A loss: 0.984764, acc: 0.031250]\n",
            "692: [D loss: 0.659620, acc: 0.597656]  [A loss: 0.732191, acc: 0.429688]\n",
            "693: [D loss: 0.716149, acc: 0.507812]  [A loss: 1.105973, acc: 0.007812]\n",
            "694: [D loss: 0.674747, acc: 0.560547]  [A loss: 0.663421, acc: 0.628906]\n",
            "695: [D loss: 0.711928, acc: 0.519531]  [A loss: 1.027502, acc: 0.027344]\n",
            "696: [D loss: 0.657669, acc: 0.611328]  [A loss: 0.695034, acc: 0.476562]\n",
            "697: [D loss: 0.713771, acc: 0.527344]  [A loss: 0.972680, acc: 0.035156]\n",
            "698: [D loss: 0.677437, acc: 0.548828]  [A loss: 0.775145, acc: 0.300781]\n",
            "699: [D loss: 0.679239, acc: 0.556641]  [A loss: 0.870007, acc: 0.136719]\n",
            "700: [D loss: 0.666979, acc: 0.578125]  [A loss: 0.810476, acc: 0.253906]\n",
            "701: [D loss: 0.667244, acc: 0.578125]  [A loss: 0.892409, acc: 0.085938]\n",
            "702: [D loss: 0.681867, acc: 0.525391]  [A loss: 0.862380, acc: 0.144531]\n",
            "703: [D loss: 0.675407, acc: 0.572266]  [A loss: 0.843969, acc: 0.167969]\n",
            "704: [D loss: 0.667336, acc: 0.562500]  [A loss: 0.906313, acc: 0.121094]\n",
            "705: [D loss: 0.665078, acc: 0.587891]  [A loss: 0.875772, acc: 0.128906]\n",
            "706: [D loss: 0.661915, acc: 0.591797]  [A loss: 0.875410, acc: 0.148438]\n",
            "707: [D loss: 0.668204, acc: 0.583984]  [A loss: 0.898421, acc: 0.089844]\n",
            "708: [D loss: 0.677408, acc: 0.554688]  [A loss: 0.893793, acc: 0.148438]\n",
            "709: [D loss: 0.668842, acc: 0.574219]  [A loss: 0.910203, acc: 0.089844]\n",
            "710: [D loss: 0.689319, acc: 0.556641]  [A loss: 1.008591, acc: 0.027344]\n",
            "711: [D loss: 0.662203, acc: 0.607422]  [A loss: 0.722821, acc: 0.441406]\n",
            "712: [D loss: 0.695178, acc: 0.519531]  [A loss: 1.135899, acc: 0.023438]\n",
            "713: [D loss: 0.676923, acc: 0.556641]  [A loss: 0.668227, acc: 0.574219]\n",
            "714: [D loss: 0.740775, acc: 0.511719]  [A loss: 1.062433, acc: 0.011719]\n",
            "715: [D loss: 0.673014, acc: 0.589844]  [A loss: 0.737643, acc: 0.398438]\n",
            "716: [D loss: 0.690281, acc: 0.529297]  [A loss: 0.947278, acc: 0.046875]\n",
            "717: [D loss: 0.667270, acc: 0.597656]  [A loss: 0.812907, acc: 0.218750]\n",
            "718: [D loss: 0.676486, acc: 0.548828]  [A loss: 0.887410, acc: 0.101562]\n",
            "719: [D loss: 0.667862, acc: 0.599609]  [A loss: 0.851242, acc: 0.203125]\n",
            "720: [D loss: 0.676364, acc: 0.554688]  [A loss: 0.916649, acc: 0.113281]\n",
            "721: [D loss: 0.663328, acc: 0.583984]  [A loss: 0.864210, acc: 0.144531]\n",
            "722: [D loss: 0.676525, acc: 0.585938]  [A loss: 0.897175, acc: 0.113281]\n",
            "723: [D loss: 0.686260, acc: 0.546875]  [A loss: 0.886513, acc: 0.148438]\n",
            "724: [D loss: 0.670525, acc: 0.580078]  [A loss: 0.857680, acc: 0.140625]\n",
            "725: [D loss: 0.682356, acc: 0.572266]  [A loss: 0.912689, acc: 0.097656]\n",
            "726: [D loss: 0.673892, acc: 0.587891]  [A loss: 0.808507, acc: 0.261719]\n",
            "727: [D loss: 0.672826, acc: 0.578125]  [A loss: 0.943062, acc: 0.066406]\n",
            "728: [D loss: 0.670189, acc: 0.601562]  [A loss: 0.824582, acc: 0.246094]\n",
            "729: [D loss: 0.693878, acc: 0.529297]  [A loss: 1.019479, acc: 0.078125]\n",
            "730: [D loss: 0.685139, acc: 0.542969]  [A loss: 0.744691, acc: 0.375000]\n",
            "731: [D loss: 0.697011, acc: 0.527344]  [A loss: 1.033098, acc: 0.042969]\n",
            "732: [D loss: 0.665338, acc: 0.587891]  [A loss: 0.684474, acc: 0.574219]\n",
            "733: [D loss: 0.722284, acc: 0.517578]  [A loss: 1.023012, acc: 0.031250]\n",
            "734: [D loss: 0.683559, acc: 0.562500]  [A loss: 0.724441, acc: 0.421875]\n",
            "735: [D loss: 0.704299, acc: 0.529297]  [A loss: 0.988763, acc: 0.058594]\n",
            "736: [D loss: 0.669850, acc: 0.601562]  [A loss: 0.778193, acc: 0.273438]\n",
            "737: [D loss: 0.683506, acc: 0.525391]  [A loss: 0.930374, acc: 0.089844]\n",
            "738: [D loss: 0.668563, acc: 0.609375]  [A loss: 0.806622, acc: 0.222656]\n",
            "739: [D loss: 0.667250, acc: 0.570312]  [A loss: 0.951260, acc: 0.054688]\n",
            "740: [D loss: 0.664680, acc: 0.599609]  [A loss: 0.834898, acc: 0.214844]\n",
            "741: [D loss: 0.701112, acc: 0.537109]  [A loss: 1.012308, acc: 0.027344]\n",
            "742: [D loss: 0.678120, acc: 0.585938]  [A loss: 0.745236, acc: 0.390625]\n",
            "743: [D loss: 0.682910, acc: 0.554688]  [A loss: 0.987803, acc: 0.023438]\n",
            "744: [D loss: 0.658591, acc: 0.638672]  [A loss: 0.783220, acc: 0.269531]\n",
            "745: [D loss: 0.708114, acc: 0.550781]  [A loss: 0.968346, acc: 0.046875]\n",
            "746: [D loss: 0.681655, acc: 0.546875]  [A loss: 0.780723, acc: 0.296875]\n",
            "747: [D loss: 0.679594, acc: 0.542969]  [A loss: 0.963236, acc: 0.039062]\n",
            "748: [D loss: 0.671652, acc: 0.568359]  [A loss: 0.857389, acc: 0.152344]\n",
            "749: [D loss: 0.685074, acc: 0.539062]  [A loss: 0.881183, acc: 0.125000]\n",
            "750: [D loss: 0.682331, acc: 0.535156]  [A loss: 0.882182, acc: 0.125000]\n",
            "751: [D loss: 0.674559, acc: 0.570312]  [A loss: 0.934301, acc: 0.078125]\n",
            "752: [D loss: 0.679409, acc: 0.562500]  [A loss: 0.836942, acc: 0.187500]\n",
            "753: [D loss: 0.667932, acc: 0.587891]  [A loss: 0.918856, acc: 0.066406]\n",
            "754: [D loss: 0.667705, acc: 0.625000]  [A loss: 0.929533, acc: 0.082031]\n",
            "755: [D loss: 0.686373, acc: 0.560547]  [A loss: 0.896293, acc: 0.109375]\n",
            "756: [D loss: 0.669243, acc: 0.578125]  [A loss: 0.857673, acc: 0.164062]\n",
            "757: [D loss: 0.687165, acc: 0.550781]  [A loss: 0.961613, acc: 0.046875]\n",
            "758: [D loss: 0.675869, acc: 0.574219]  [A loss: 0.801857, acc: 0.230469]\n",
            "759: [D loss: 0.676262, acc: 0.570312]  [A loss: 1.038356, acc: 0.039062]\n",
            "760: [D loss: 0.668843, acc: 0.582031]  [A loss: 0.709115, acc: 0.464844]\n",
            "761: [D loss: 0.731685, acc: 0.517578]  [A loss: 1.029948, acc: 0.011719]\n",
            "762: [D loss: 0.683353, acc: 0.554688]  [A loss: 0.767487, acc: 0.312500]\n",
            "763: [D loss: 0.699268, acc: 0.525391]  [A loss: 1.083172, acc: 0.007812]\n",
            "764: [D loss: 0.690188, acc: 0.523438]  [A loss: 0.689288, acc: 0.550781]\n",
            "765: [D loss: 0.722851, acc: 0.494141]  [A loss: 1.061038, acc: 0.019531]\n",
            "766: [D loss: 0.673292, acc: 0.601562]  [A loss: 0.709065, acc: 0.437500]\n",
            "767: [D loss: 0.707553, acc: 0.529297]  [A loss: 0.948249, acc: 0.074219]\n",
            "768: [D loss: 0.679612, acc: 0.541016]  [A loss: 0.769681, acc: 0.261719]\n",
            "769: [D loss: 0.694600, acc: 0.537109]  [A loss: 0.911548, acc: 0.085938]\n",
            "770: [D loss: 0.666662, acc: 0.625000]  [A loss: 0.767000, acc: 0.320312]\n",
            "771: [D loss: 0.690223, acc: 0.531250]  [A loss: 0.923026, acc: 0.085938]\n",
            "772: [D loss: 0.670802, acc: 0.626953]  [A loss: 0.758036, acc: 0.335938]\n",
            "773: [D loss: 0.671859, acc: 0.574219]  [A loss: 0.922659, acc: 0.093750]\n",
            "774: [D loss: 0.676174, acc: 0.570312]  [A loss: 0.782866, acc: 0.273438]\n",
            "775: [D loss: 0.677681, acc: 0.550781]  [A loss: 0.895142, acc: 0.121094]\n",
            "776: [D loss: 0.665995, acc: 0.605469]  [A loss: 0.804399, acc: 0.250000]\n",
            "777: [D loss: 0.673911, acc: 0.568359]  [A loss: 0.999057, acc: 0.035156]\n",
            "778: [D loss: 0.674358, acc: 0.574219]  [A loss: 0.747398, acc: 0.363281]\n",
            "779: [D loss: 0.690369, acc: 0.531250]  [A loss: 0.979984, acc: 0.050781]\n",
            "780: [D loss: 0.671664, acc: 0.564453]  [A loss: 0.772432, acc: 0.304688]\n",
            "781: [D loss: 0.683477, acc: 0.531250]  [A loss: 0.958872, acc: 0.054688]\n",
            "782: [D loss: 0.665973, acc: 0.587891]  [A loss: 0.751844, acc: 0.335938]\n",
            "783: [D loss: 0.705394, acc: 0.533203]  [A loss: 0.980614, acc: 0.039062]\n",
            "784: [D loss: 0.671605, acc: 0.583984]  [A loss: 0.800062, acc: 0.273438]\n",
            "785: [D loss: 0.680404, acc: 0.562500]  [A loss: 0.945715, acc: 0.070312]\n",
            "786: [D loss: 0.657581, acc: 0.611328]  [A loss: 0.799101, acc: 0.250000]\n",
            "787: [D loss: 0.693091, acc: 0.558594]  [A loss: 0.912700, acc: 0.082031]\n",
            "788: [D loss: 0.666097, acc: 0.646484]  [A loss: 0.783736, acc: 0.316406]\n",
            "789: [D loss: 0.689542, acc: 0.523438]  [A loss: 0.972688, acc: 0.046875]\n",
            "790: [D loss: 0.690756, acc: 0.525391]  [A loss: 0.765670, acc: 0.308594]\n",
            "791: [D loss: 0.693863, acc: 0.496094]  [A loss: 1.009889, acc: 0.035156]\n",
            "792: [D loss: 0.666858, acc: 0.607422]  [A loss: 0.769868, acc: 0.312500]\n",
            "793: [D loss: 0.712379, acc: 0.517578]  [A loss: 0.990161, acc: 0.054688]\n",
            "794: [D loss: 0.689374, acc: 0.527344]  [A loss: 0.767252, acc: 0.312500]\n",
            "795: [D loss: 0.687696, acc: 0.546875]  [A loss: 0.965585, acc: 0.015625]\n",
            "796: [D loss: 0.662947, acc: 0.599609]  [A loss: 0.759366, acc: 0.328125]\n",
            "797: [D loss: 0.713076, acc: 0.527344]  [A loss: 1.034895, acc: 0.027344]\n",
            "798: [D loss: 0.668166, acc: 0.582031]  [A loss: 0.755033, acc: 0.339844]\n",
            "799: [D loss: 0.682141, acc: 0.550781]  [A loss: 0.912654, acc: 0.109375]\n",
            "800: [D loss: 0.679906, acc: 0.566406]  [A loss: 0.798777, acc: 0.261719]\n",
            "801: [D loss: 0.684591, acc: 0.568359]  [A loss: 0.886925, acc: 0.093750]\n",
            "802: [D loss: 0.675587, acc: 0.556641]  [A loss: 0.863877, acc: 0.136719]\n",
            "803: [D loss: 0.670973, acc: 0.574219]  [A loss: 0.888941, acc: 0.113281]\n",
            "804: [D loss: 0.667212, acc: 0.566406]  [A loss: 0.798516, acc: 0.273438]\n",
            "805: [D loss: 0.686556, acc: 0.539062]  [A loss: 0.961080, acc: 0.046875]\n",
            "806: [D loss: 0.678685, acc: 0.558594]  [A loss: 0.783573, acc: 0.253906]\n",
            "807: [D loss: 0.696738, acc: 0.519531]  [A loss: 1.073241, acc: 0.007812]\n",
            "808: [D loss: 0.687668, acc: 0.566406]  [A loss: 0.652979, acc: 0.636719]\n",
            "809: [D loss: 0.713966, acc: 0.507812]  [A loss: 0.988465, acc: 0.011719]\n",
            "810: [D loss: 0.665475, acc: 0.617188]  [A loss: 0.746899, acc: 0.339844]\n",
            "811: [D loss: 0.697623, acc: 0.544922]  [A loss: 0.918831, acc: 0.066406]\n",
            "812: [D loss: 0.680695, acc: 0.544922]  [A loss: 0.817202, acc: 0.179688]\n",
            "813: [D loss: 0.672227, acc: 0.568359]  [A loss: 0.827166, acc: 0.210938]\n",
            "814: [D loss: 0.677295, acc: 0.552734]  [A loss: 0.881649, acc: 0.144531]\n",
            "815: [D loss: 0.686514, acc: 0.554688]  [A loss: 0.892937, acc: 0.097656]\n",
            "816: [D loss: 0.676162, acc: 0.591797]  [A loss: 0.831900, acc: 0.167969]\n",
            "817: [D loss: 0.677428, acc: 0.585938]  [A loss: 0.857719, acc: 0.136719]\n",
            "818: [D loss: 0.682090, acc: 0.562500]  [A loss: 0.879765, acc: 0.125000]\n",
            "819: [D loss: 0.688367, acc: 0.535156]  [A loss: 0.899173, acc: 0.128906]\n",
            "820: [D loss: 0.680089, acc: 0.570312]  [A loss: 0.795285, acc: 0.277344]\n",
            "821: [D loss: 0.686364, acc: 0.537109]  [A loss: 0.989304, acc: 0.039062]\n",
            "822: [D loss: 0.666410, acc: 0.585938]  [A loss: 0.736141, acc: 0.394531]\n",
            "823: [D loss: 0.699054, acc: 0.519531]  [A loss: 0.987930, acc: 0.058594]\n",
            "824: [D loss: 0.666758, acc: 0.589844]  [A loss: 0.712083, acc: 0.441406]\n",
            "825: [D loss: 0.694117, acc: 0.539062]  [A loss: 1.021398, acc: 0.023438]\n",
            "826: [D loss: 0.684152, acc: 0.539062]  [A loss: 0.659134, acc: 0.617188]\n",
            "827: [D loss: 0.722101, acc: 0.490234]  [A loss: 1.072471, acc: 0.027344]\n",
            "828: [D loss: 0.668883, acc: 0.615234]  [A loss: 0.722063, acc: 0.433594]\n",
            "829: [D loss: 0.710981, acc: 0.517578]  [A loss: 0.899343, acc: 0.082031]\n",
            "830: [D loss: 0.690674, acc: 0.542969]  [A loss: 0.818446, acc: 0.164062]\n",
            "831: [D loss: 0.683303, acc: 0.541016]  [A loss: 0.832644, acc: 0.164062]\n",
            "832: [D loss: 0.677984, acc: 0.558594]  [A loss: 0.827562, acc: 0.203125]\n",
            "833: [D loss: 0.681952, acc: 0.564453]  [A loss: 0.831392, acc: 0.171875]\n",
            "834: [D loss: 0.672325, acc: 0.582031]  [A loss: 0.851035, acc: 0.164062]\n",
            "835: [D loss: 0.658965, acc: 0.609375]  [A loss: 0.818298, acc: 0.218750]\n",
            "836: [D loss: 0.706737, acc: 0.537109]  [A loss: 0.899890, acc: 0.058594]\n",
            "837: [D loss: 0.681561, acc: 0.546875]  [A loss: 0.881571, acc: 0.097656]\n",
            "838: [D loss: 0.670299, acc: 0.595703]  [A loss: 0.823238, acc: 0.164062]\n",
            "839: [D loss: 0.675827, acc: 0.583984]  [A loss: 0.865384, acc: 0.128906]\n",
            "840: [D loss: 0.687484, acc: 0.541016]  [A loss: 0.901262, acc: 0.101562]\n",
            "841: [D loss: 0.675266, acc: 0.570312]  [A loss: 0.861950, acc: 0.136719]\n",
            "842: [D loss: 0.685315, acc: 0.544922]  [A loss: 0.909770, acc: 0.082031]\n",
            "843: [D loss: 0.674177, acc: 0.576172]  [A loss: 0.852153, acc: 0.171875]\n",
            "844: [D loss: 0.686882, acc: 0.552734]  [A loss: 0.932163, acc: 0.085938]\n",
            "845: [D loss: 0.690116, acc: 0.529297]  [A loss: 0.800849, acc: 0.230469]\n",
            "846: [D loss: 0.675314, acc: 0.544922]  [A loss: 0.900427, acc: 0.148438]\n",
            "847: [D loss: 0.697041, acc: 0.548828]  [A loss: 0.819800, acc: 0.257812]\n",
            "848: [D loss: 0.683887, acc: 0.544922]  [A loss: 1.029539, acc: 0.027344]\n",
            "849: [D loss: 0.671212, acc: 0.580078]  [A loss: 0.681723, acc: 0.578125]\n",
            "850: [D loss: 0.715458, acc: 0.511719]  [A loss: 1.149335, acc: 0.015625]\n",
            "851: [D loss: 0.674939, acc: 0.580078]  [A loss: 0.662058, acc: 0.597656]\n",
            "852: [D loss: 0.718687, acc: 0.505859]  [A loss: 0.991370, acc: 0.019531]\n",
            "853: [D loss: 0.679101, acc: 0.572266]  [A loss: 0.737402, acc: 0.433594]\n",
            "854: [D loss: 0.702355, acc: 0.517578]  [A loss: 0.999590, acc: 0.035156]\n",
            "855: [D loss: 0.678150, acc: 0.560547]  [A loss: 0.712937, acc: 0.484375]\n",
            "856: [D loss: 0.707398, acc: 0.523438]  [A loss: 0.941521, acc: 0.042969]\n",
            "857: [D loss: 0.657729, acc: 0.630859]  [A loss: 0.798997, acc: 0.234375]\n",
            "858: [D loss: 0.677527, acc: 0.576172]  [A loss: 0.892159, acc: 0.136719]\n",
            "859: [D loss: 0.664367, acc: 0.593750]  [A loss: 0.794544, acc: 0.277344]\n",
            "860: [D loss: 0.695139, acc: 0.556641]  [A loss: 0.887271, acc: 0.093750]\n",
            "861: [D loss: 0.666736, acc: 0.589844]  [A loss: 0.811870, acc: 0.218750]\n",
            "862: [D loss: 0.691174, acc: 0.556641]  [A loss: 0.895344, acc: 0.078125]\n",
            "863: [D loss: 0.680786, acc: 0.587891]  [A loss: 0.842113, acc: 0.183594]\n",
            "864: [D loss: 0.684884, acc: 0.550781]  [A loss: 0.832055, acc: 0.171875]\n",
            "865: [D loss: 0.661950, acc: 0.599609]  [A loss: 0.797529, acc: 0.289062]\n",
            "866: [D loss: 0.690714, acc: 0.519531]  [A loss: 0.921723, acc: 0.097656]\n",
            "867: [D loss: 0.670032, acc: 0.613281]  [A loss: 0.784075, acc: 0.277344]\n",
            "868: [D loss: 0.690302, acc: 0.533203]  [A loss: 0.993597, acc: 0.062500]\n",
            "869: [D loss: 0.682091, acc: 0.572266]  [A loss: 0.786968, acc: 0.289062]\n",
            "870: [D loss: 0.675383, acc: 0.558594]  [A loss: 0.943917, acc: 0.050781]\n",
            "871: [D loss: 0.673138, acc: 0.582031]  [A loss: 0.763481, acc: 0.316406]\n",
            "872: [D loss: 0.700565, acc: 0.519531]  [A loss: 1.001969, acc: 0.046875]\n",
            "873: [D loss: 0.680700, acc: 0.593750]  [A loss: 0.697404, acc: 0.507812]\n",
            "874: [D loss: 0.711352, acc: 0.509766]  [A loss: 1.040238, acc: 0.027344]\n",
            "875: [D loss: 0.686468, acc: 0.556641]  [A loss: 0.706856, acc: 0.441406]\n",
            "876: [D loss: 0.696525, acc: 0.539062]  [A loss: 0.943017, acc: 0.046875]\n",
            "877: [D loss: 0.676214, acc: 0.597656]  [A loss: 0.785721, acc: 0.269531]\n",
            "878: [D loss: 0.674588, acc: 0.603516]  [A loss: 0.876092, acc: 0.117188]\n",
            "879: [D loss: 0.674871, acc: 0.564453]  [A loss: 0.861677, acc: 0.152344]\n",
            "880: [D loss: 0.682903, acc: 0.546875]  [A loss: 0.849401, acc: 0.136719]\n",
            "881: [D loss: 0.679358, acc: 0.580078]  [A loss: 0.868403, acc: 0.148438]\n",
            "882: [D loss: 0.655948, acc: 0.634766]  [A loss: 0.765893, acc: 0.296875]\n",
            "883: [D loss: 0.690927, acc: 0.542969]  [A loss: 0.994678, acc: 0.042969]\n",
            "884: [D loss: 0.685671, acc: 0.552734]  [A loss: 0.690023, acc: 0.566406]\n",
            "885: [D loss: 0.692487, acc: 0.531250]  [A loss: 0.973763, acc: 0.031250]\n",
            "886: [D loss: 0.677076, acc: 0.597656]  [A loss: 0.856668, acc: 0.187500]\n",
            "887: [D loss: 0.678571, acc: 0.566406]  [A loss: 0.852378, acc: 0.183594]\n",
            "888: [D loss: 0.691604, acc: 0.523438]  [A loss: 0.946818, acc: 0.058594]\n",
            "889: [D loss: 0.679143, acc: 0.585938]  [A loss: 0.718078, acc: 0.425781]\n",
            "890: [D loss: 0.725347, acc: 0.507812]  [A loss: 0.973440, acc: 0.062500]\n",
            "891: [D loss: 0.678393, acc: 0.582031]  [A loss: 0.825605, acc: 0.203125]\n",
            "892: [D loss: 0.711988, acc: 0.498047]  [A loss: 0.989306, acc: 0.019531]\n",
            "893: [D loss: 0.670288, acc: 0.597656]  [A loss: 0.685697, acc: 0.546875]\n",
            "894: [D loss: 0.717210, acc: 0.519531]  [A loss: 1.030142, acc: 0.042969]\n",
            "895: [D loss: 0.674521, acc: 0.550781]  [A loss: 0.689061, acc: 0.511719]\n",
            "896: [D loss: 0.740403, acc: 0.517578]  [A loss: 0.902324, acc: 0.062500]\n",
            "897: [D loss: 0.688591, acc: 0.529297]  [A loss: 0.777698, acc: 0.234375]\n",
            "898: [D loss: 0.696633, acc: 0.529297]  [A loss: 0.886277, acc: 0.093750]\n",
            "899: [D loss: 0.683795, acc: 0.564453]  [A loss: 0.786750, acc: 0.242188]\n",
            "900: [D loss: 0.699441, acc: 0.515625]  [A loss: 0.898307, acc: 0.085938]\n",
            "901: [D loss: 0.680883, acc: 0.533203]  [A loss: 0.776269, acc: 0.292969]\n",
            "902: [D loss: 0.697676, acc: 0.537109]  [A loss: 0.933358, acc: 0.050781]\n",
            "903: [D loss: 0.665489, acc: 0.599609]  [A loss: 0.737051, acc: 0.359375]\n",
            "904: [D loss: 0.678725, acc: 0.544922]  [A loss: 0.934504, acc: 0.074219]\n",
            "905: [D loss: 0.680377, acc: 0.542969]  [A loss: 0.765296, acc: 0.304688]\n",
            "906: [D loss: 0.675419, acc: 0.537109]  [A loss: 0.856440, acc: 0.203125]\n",
            "907: [D loss: 0.682632, acc: 0.548828]  [A loss: 0.791327, acc: 0.250000]\n",
            "908: [D loss: 0.667538, acc: 0.568359]  [A loss: 0.877923, acc: 0.121094]\n",
            "909: [D loss: 0.674931, acc: 0.593750]  [A loss: 0.857310, acc: 0.175781]\n",
            "910: [D loss: 0.677216, acc: 0.572266]  [A loss: 0.878424, acc: 0.117188]\n",
            "911: [D loss: 0.670833, acc: 0.572266]  [A loss: 0.871988, acc: 0.160156]\n",
            "912: [D loss: 0.686836, acc: 0.550781]  [A loss: 0.878679, acc: 0.113281]\n",
            "913: [D loss: 0.700172, acc: 0.507812]  [A loss: 0.894065, acc: 0.109375]\n",
            "914: [D loss: 0.685896, acc: 0.505859]  [A loss: 0.895372, acc: 0.121094]\n",
            "915: [D loss: 0.672772, acc: 0.609375]  [A loss: 0.831440, acc: 0.195312]\n",
            "916: [D loss: 0.677172, acc: 0.558594]  [A loss: 0.934361, acc: 0.113281]\n",
            "917: [D loss: 0.691485, acc: 0.548828]  [A loss: 0.815991, acc: 0.203125]\n",
            "918: [D loss: 0.697137, acc: 0.525391]  [A loss: 0.942723, acc: 0.082031]\n",
            "919: [D loss: 0.691659, acc: 0.544922]  [A loss: 0.782734, acc: 0.304688]\n",
            "920: [D loss: 0.690549, acc: 0.525391]  [A loss: 1.078081, acc: 0.007812]\n",
            "921: [D loss: 0.672803, acc: 0.568359]  [A loss: 0.674355, acc: 0.613281]\n",
            "922: [D loss: 0.732547, acc: 0.501953]  [A loss: 1.090913, acc: 0.023438]\n",
            "923: [D loss: 0.685244, acc: 0.529297]  [A loss: 0.660273, acc: 0.601562]\n",
            "924: [D loss: 0.719934, acc: 0.503906]  [A loss: 0.898161, acc: 0.058594]\n",
            "925: [D loss: 0.685566, acc: 0.576172]  [A loss: 0.795092, acc: 0.246094]\n",
            "926: [D loss: 0.697545, acc: 0.505859]  [A loss: 0.947376, acc: 0.085938]\n",
            "927: [D loss: 0.667919, acc: 0.585938]  [A loss: 0.745078, acc: 0.367188]\n",
            "928: [D loss: 0.680091, acc: 0.572266]  [A loss: 0.868203, acc: 0.121094]\n",
            "929: [D loss: 0.681913, acc: 0.556641]  [A loss: 0.842668, acc: 0.167969]\n",
            "930: [D loss: 0.707623, acc: 0.511719]  [A loss: 0.875901, acc: 0.117188]\n",
            "931: [D loss: 0.693274, acc: 0.541016]  [A loss: 0.917089, acc: 0.085938]\n",
            "932: [D loss: 0.682054, acc: 0.541016]  [A loss: 0.791589, acc: 0.273438]\n",
            "933: [D loss: 0.695876, acc: 0.544922]  [A loss: 0.941137, acc: 0.078125]\n",
            "934: [D loss: 0.679852, acc: 0.572266]  [A loss: 0.748953, acc: 0.402344]\n",
            "935: [D loss: 0.703217, acc: 0.519531]  [A loss: 0.933645, acc: 0.082031]\n",
            "936: [D loss: 0.675690, acc: 0.572266]  [A loss: 0.791417, acc: 0.261719]\n",
            "937: [D loss: 0.693419, acc: 0.535156]  [A loss: 0.877221, acc: 0.125000]\n",
            "938: [D loss: 0.680920, acc: 0.566406]  [A loss: 0.803776, acc: 0.226562]\n",
            "939: [D loss: 0.677456, acc: 0.542969]  [A loss: 0.891499, acc: 0.093750]\n",
            "940: [D loss: 0.678137, acc: 0.558594]  [A loss: 0.793953, acc: 0.226562]\n",
            "941: [D loss: 0.684162, acc: 0.546875]  [A loss: 0.899022, acc: 0.105469]\n",
            "942: [D loss: 0.676003, acc: 0.591797]  [A loss: 0.787298, acc: 0.214844]\n",
            "943: [D loss: 0.676307, acc: 0.564453]  [A loss: 0.942825, acc: 0.066406]\n",
            "944: [D loss: 0.658265, acc: 0.605469]  [A loss: 0.745547, acc: 0.394531]\n",
            "945: [D loss: 0.687203, acc: 0.560547]  [A loss: 0.938796, acc: 0.062500]\n",
            "946: [D loss: 0.685539, acc: 0.558594]  [A loss: 0.748831, acc: 0.406250]\n",
            "947: [D loss: 0.694813, acc: 0.519531]  [A loss: 1.013854, acc: 0.042969]\n",
            "948: [D loss: 0.675851, acc: 0.556641]  [A loss: 0.706447, acc: 0.480469]\n",
            "949: [D loss: 0.704241, acc: 0.527344]  [A loss: 0.984918, acc: 0.046875]\n",
            "950: [D loss: 0.679080, acc: 0.595703]  [A loss: 0.772054, acc: 0.335938]\n",
            "951: [D loss: 0.715369, acc: 0.505859]  [A loss: 0.909188, acc: 0.093750]\n",
            "952: [D loss: 0.666661, acc: 0.597656]  [A loss: 0.754342, acc: 0.343750]\n",
            "953: [D loss: 0.693871, acc: 0.537109]  [A loss: 0.949789, acc: 0.066406]\n",
            "954: [D loss: 0.687952, acc: 0.541016]  [A loss: 0.836447, acc: 0.179688]\n",
            "955: [D loss: 0.693366, acc: 0.539062]  [A loss: 0.919063, acc: 0.070312]\n",
            "956: [D loss: 0.673189, acc: 0.595703]  [A loss: 0.809051, acc: 0.222656]\n",
            "957: [D loss: 0.692227, acc: 0.544922]  [A loss: 0.937905, acc: 0.097656]\n",
            "958: [D loss: 0.682042, acc: 0.576172]  [A loss: 0.768730, acc: 0.292969]\n",
            "959: [D loss: 0.688604, acc: 0.544922]  [A loss: 0.863285, acc: 0.156250]\n",
            "960: [D loss: 0.678188, acc: 0.572266]  [A loss: 0.835045, acc: 0.167969]\n",
            "961: [D loss: 0.685039, acc: 0.558594]  [A loss: 0.901969, acc: 0.117188]\n",
            "962: [D loss: 0.679126, acc: 0.554688]  [A loss: 0.796043, acc: 0.222656]\n",
            "963: [D loss: 0.692529, acc: 0.542969]  [A loss: 0.980964, acc: 0.046875]\n",
            "964: [D loss: 0.679771, acc: 0.566406]  [A loss: 0.735049, acc: 0.378906]\n",
            "965: [D loss: 0.715101, acc: 0.498047]  [A loss: 1.040726, acc: 0.070312]\n",
            "966: [D loss: 0.681356, acc: 0.564453]  [A loss: 0.728279, acc: 0.378906]\n",
            "967: [D loss: 0.718344, acc: 0.513672]  [A loss: 0.990151, acc: 0.046875]\n",
            "968: [D loss: 0.689718, acc: 0.535156]  [A loss: 0.796679, acc: 0.261719]\n",
            "969: [D loss: 0.711857, acc: 0.509766]  [A loss: 0.995807, acc: 0.027344]\n",
            "970: [D loss: 0.674285, acc: 0.574219]  [A loss: 0.660281, acc: 0.597656]\n",
            "971: [D loss: 0.725397, acc: 0.521484]  [A loss: 0.963288, acc: 0.031250]\n",
            "972: [D loss: 0.678847, acc: 0.560547]  [A loss: 0.706031, acc: 0.472656]\n",
            "973: [D loss: 0.695618, acc: 0.537109]  [A loss: 0.944290, acc: 0.050781]\n",
            "974: [D loss: 0.684423, acc: 0.542969]  [A loss: 0.735277, acc: 0.367188]\n",
            "975: [D loss: 0.694051, acc: 0.529297]  [A loss: 0.854071, acc: 0.136719]\n",
            "976: [D loss: 0.671776, acc: 0.564453]  [A loss: 0.827051, acc: 0.195312]\n",
            "977: [D loss: 0.683862, acc: 0.544922]  [A loss: 0.888806, acc: 0.101562]\n",
            "978: [D loss: 0.681680, acc: 0.566406]  [A loss: 0.849761, acc: 0.136719]\n",
            "979: [D loss: 0.685997, acc: 0.554688]  [A loss: 0.884067, acc: 0.121094]\n",
            "980: [D loss: 0.678586, acc: 0.562500]  [A loss: 0.804342, acc: 0.210938]\n",
            "981: [D loss: 0.697177, acc: 0.517578]  [A loss: 0.951499, acc: 0.097656]\n",
            "982: [D loss: 0.684295, acc: 0.580078]  [A loss: 0.806148, acc: 0.218750]\n",
            "983: [D loss: 0.684294, acc: 0.527344]  [A loss: 0.885409, acc: 0.113281]\n",
            "984: [D loss: 0.675116, acc: 0.564453]  [A loss: 0.783661, acc: 0.332031]\n",
            "985: [D loss: 0.695839, acc: 0.527344]  [A loss: 0.855389, acc: 0.167969]\n",
            "986: [D loss: 0.682050, acc: 0.580078]  [A loss: 0.895623, acc: 0.101562]\n",
            "987: [D loss: 0.689204, acc: 0.564453]  [A loss: 0.756219, acc: 0.347656]\n",
            "988: [D loss: 0.705278, acc: 0.513672]  [A loss: 1.072594, acc: 0.015625]\n",
            "989: [D loss: 0.679447, acc: 0.585938]  [A loss: 0.668734, acc: 0.574219]\n",
            "990: [D loss: 0.710172, acc: 0.517578]  [A loss: 1.012732, acc: 0.027344]\n",
            "991: [D loss: 0.686064, acc: 0.552734]  [A loss: 0.671468, acc: 0.582031]\n",
            "992: [D loss: 0.709322, acc: 0.501953]  [A loss: 0.930028, acc: 0.078125]\n",
            "993: [D loss: 0.681415, acc: 0.552734]  [A loss: 0.772795, acc: 0.316406]\n",
            "994: [D loss: 0.689933, acc: 0.519531]  [A loss: 0.871531, acc: 0.109375]\n",
            "995: [D loss: 0.677309, acc: 0.574219]  [A loss: 0.767581, acc: 0.285156]\n",
            "996: [D loss: 0.685135, acc: 0.560547]  [A loss: 0.913443, acc: 0.097656]\n",
            "997: [D loss: 0.685839, acc: 0.554688]  [A loss: 0.755704, acc: 0.363281]\n",
            "998: [D loss: 0.685668, acc: 0.556641]  [A loss: 0.961462, acc: 0.082031]\n",
            "999: [D loss: 0.691076, acc: 0.550781]  [A loss: 0.762517, acc: 0.285156]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxkXAHYm_Xmm",
        "colab_type": "code",
        "outputId": "298fb429-3c48-4d24-d1d5-5f270dee1ba3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        }
      },
      "source": [
        "# Generate batch of synthetic MNIST images\n",
        "timer.elapsed_time()\n",
        "mnist_dcgan.plot_images(fake=True)\n",
        "mnist_dcgan.plot_images(fake=False, save2file=True)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Elapsed: 9.512916302680969 min \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsgAAALICAYAAABiqwZ2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyAAAUFAgAwBA4f8DmqyQT1YbTQQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x720 with 16 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcnC-jooCpqA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}