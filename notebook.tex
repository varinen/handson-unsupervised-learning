
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{Chap03}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{c+c1}{\PYZsh{} Import libraries}
        \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}Main\PYZsq{}\PYZsq{}\PYZsq{}}
        \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
        \PY{k+kn}{import} \PY{n+nn}{os}\PY{o}{,} \PY{n+nn}{time}\PY{o}{,} \PY{n+nn}{pickle}\PY{o}{,} \PY{n+nn}{gzip}
        
        \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}Data Viz\PYZsq{}\PYZsq{}\PYZsq{}}
        \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
        \PY{k+kn}{import} \PY{n+nn}{seaborn} \PY{k}{as} \PY{n+nn}{sns}
        \PY{n}{color} \PY{o}{=} \PY{n}{sns}\PY{o}{.}\PY{n}{color\PYZus{}palette}\PY{p}{(}\PY{p}{)}
        \PY{k+kn}{import} \PY{n+nn}{matplotlib} \PY{k}{as} \PY{n+nn}{mpl}
        
        \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
\end{Verbatim}


    \subsection{The MNIST Digist Database}\label{the-mnist-digist-database}

The dataset we use in this project is the dataset of handwritten digits.
It is taken as a pickle file. The dataset consists of 70 thousands
observations. Each entry is a 28 by 28 pixel image represented by a
784-dimensional vector.

The dataset is split into 50000 trining elements, 10000 validation and
10000 test observations.

The labels for the objservations are between 0 and 9, i.e., there are 10
classes.

Each feature component (pixel) is a float value between 0.0 and 1.0 that
is the intensity of the pixel, 0.0 is black, 1.0 is white.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{n}{current\PYZus{}path} \PY{o}{=} \PY{n}{os}\PY{o}{.}\PY{n}{getcwd}\PY{p}{(}\PY{p}{)}
        \PY{n}{file} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}\PYZbs{}}\PY{l+s+s1}{datasets}\PY{l+s+se}{\PYZbs{}\PYZbs{}}\PY{l+s+s1}{mnist\PYZus{}data}\PY{l+s+se}{\PYZbs{}\PYZbs{}}\PY{l+s+s1}{mnist.pkl.gz}\PY{l+s+s1}{\PYZsq{}}
        
        \PY{k}{with} \PY{n}{gzip}\PY{o}{.}\PY{n}{open}\PY{p}{(}\PY{n}{current\PYZus{}path} \PY{o}{+} \PY{n}{file}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rb}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{k}{as} \PY{n}{f}\PY{p}{:}
            \PY{n}{train\PYZus{}set}\PY{p}{,} \PY{n}{validation\PYZus{}set}\PY{p}{,} \PY{n}{test\PYZus{}set} \PY{o}{=} \PY{n}{pickle}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{n}{f}\PY{p}{,} \PY{n}{encoding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{latin1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        
        
        \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train} \PY{o}{=} \PY{n}{train\PYZus{}set}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{train\PYZus{}set}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
        \PY{n}{X\PYZus{}validation}\PY{p}{,} \PY{n}{y\PYZus{}validation} \PY{o}{=} \PY{n}{validation\PYZus{}set}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{validation\PYZus{}set}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
        \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{test\PYZus{}set}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{test\PYZus{}set}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{c+c1}{\PYZsh{} Verify shape of datasets}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Shape of X\PYZus{}train: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Shape of y\PYZus{}train: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Shape of X\PYZus{}validation: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{X\PYZus{}validation}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Shape of y\PYZus{}validation: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{y\PYZus{}validation}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Shape of X\PYZus{}test: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Shape of y\PYZus{}test: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Shape of X\_train:  (50000, 784)
Shape of y\_train:  (50000,)
Shape of X\_validation:  (10000, 784)
Shape of y\_validation:  (10000,)
Shape of X\_test:  (10000, 784)
Shape of y\_test:  (10000,)

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{n}{train\PYZus{}index} \PY{o}{=} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}\PY{p}{)}
        \PY{n}{validation\PYZus{}index} \PY{o}{=} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}\PY{p}{,} \PYZbs{}
                                 \PY{n+nb}{len}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}\PY{o}{+}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X\PYZus{}validation}\PY{p}{)}\PY{p}{)}
        \PY{n}{test\PYZus{}index} \PY{o}{=} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}\PY{o}{+}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X\PYZus{}validation}\PY{p}{)}\PY{p}{,} \PYZbs{}
                           \PY{n+nb}{len}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}\PY{o}{+}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X\PYZus{}validation}\PY{p}{)}\PY{o}{+}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Train index: }\PY{l+s+si}{\PYZob{}train\PYZus{}index\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation index: }\PY{l+s+si}{\PYZob{}validation\PYZus{}index\PYZcb{}}\PY{l+s+s1}{\PYZsq{}} \PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test index: }\PY{l+s+si}{\PYZob{}test\PYZus{}index\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Train index: range(0, 50000)
Validation index: range(50000, 60000)
Test index: range(60000, 70000)

    \end{Verbatim}

    In this step we create \texttt{pandas} DataFrames out of \texttt{numpy}
arrays with index values that we set up before. The labels are converted
into \texttt{pandas} Series:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{n}{type\PYZus{}x\PYZus{}train\PYZus{}orig} \PY{o}{=} \PY{n+nb}{type}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
        \PY{n}{type\PYZus{}y\PYZus{}train\PYZus{}orig} \PY{o}{=} \PY{n+nb}{type}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{type\PYZus{}x\PYZus{}train\PYZus{}orig}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{type\PYZus{}y\PYZus{}train\PYZus{}orig}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
<class 'numpy.ndarray'>
<class 'numpy.ndarray'>

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{train\PYZus{}index}\PY{p}{)}
        \PY{n}{y\PYZus{}train} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{Series}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{y\PYZus{}train}\PY{p}{,}\PY{n}{index}\PY{o}{=}\PY{n}{train\PYZus{}index}\PY{p}{)}
        
        \PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{type}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{type}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{index}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
<class 'pandas.core.series.Series'>
<class 'pandas.core.frame.DataFrame'>
RangeIndex(start=0, stop=50000, step=1)

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{n}{X\PYZus{}validation} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}validation}\PY{p}{,}\PY{n}{index}\PY{o}{=}\PY{n}{validation\PYZus{}index}\PY{p}{)}
        \PY{n}{y\PYZus{}validation} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{Series}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{y\PYZus{}validation}\PY{p}{,}\PY{n}{index}\PY{o}{=}\PY{n}{validation\PYZus{}index}\PY{p}{)}
        
        \PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}test}\PY{p}{,}\PY{n}{index}\PY{o}{=}\PY{n}{test\PYZus{}index}\PY{p}{)}
        \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{Series}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{y\PYZus{}test}\PY{p}{,}\PY{n}{index}\PY{o}{=}\PY{n}{test\PYZus{}index}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{c+c1}{\PYZsh{} Describe the training matrix}
        \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{describe}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}8}]:}            0        1        2        3        4        5        6        7    \textbackslash{}
        count  50000.0  50000.0  50000.0  50000.0  50000.0  50000.0  50000.0  50000.0   
        mean       0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   
        std        0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   
        min        0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   
        25\%        0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   
        50\%        0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   
        75\%        0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   
        max        0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   
        
                   8        9     {\ldots}              774           775           776  \textbackslash{}
        count  50000.0  50000.0   {\ldots}     50000.000000  50000.000000  50000.000000   
        mean       0.0      0.0   {\ldots}         0.000739      0.000354      0.000204   
        std        0.0      0.0   {\ldots}         0.022778      0.015422      0.012079   
        min        0.0      0.0   {\ldots}         0.000000      0.000000      0.000000   
        25\%        0.0      0.0   {\ldots}         0.000000      0.000000      0.000000   
        50\%        0.0      0.0   {\ldots}         0.000000      0.000000      0.000000   
        75\%        0.0      0.0   {\ldots}         0.000000      0.000000      0.000000   
        max        0.0      0.0   {\ldots}         0.992188      0.992188      0.988281   
        
                        777           778           779      780      781      782  \textbackslash{}
        count  50000.000000  50000.000000  50000.000000  50000.0  50000.0  50000.0   
        mean       0.000090      0.000071      0.000009      0.0      0.0      0.0   
        std        0.007217      0.007181      0.001483      0.0      0.0      0.0   
        min        0.000000      0.000000      0.000000      0.0      0.0      0.0   
        25\%        0.000000      0.000000      0.000000      0.0      0.0      0.0   
        50\%        0.000000      0.000000      0.000000      0.0      0.0      0.0   
        75\%        0.000000      0.000000      0.000000      0.0      0.0      0.0   
        max        0.988281      0.992188      0.242188      0.0      0.0      0.0   
        
                   783  
        count  50000.0  
        mean       0.0  
        std        0.0  
        min        0.0  
        25\%        0.0  
        50\%        0.0  
        75\%        0.0  
        max        0.0  
        
        [8 rows x 784 columns]
\end{Verbatim}
            
    We can see, that the first and the last features are black pixels: their
mean, min, and max values are all 0.0. That is to be expected. A higher
value variety can be seen in the middle of the feature list.

Let's output the first labels in the training dataset:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{n}{y\PYZus{}train}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}9}]:} 0    5
        1    0
        2    4
        3    1
        4    9
        dtype: int64
\end{Verbatim}
            
    We can look at the images on the dataset by plotting their vectors:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{k}{def} \PY{n+nf}{view\PYZus{}digit}\PY{p}{(}\PY{n}{example}\PY{p}{)}\PY{p}{:}
             \PY{n}{label} \PY{o}{=} \PY{n}{y\PYZus{}train}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{n}{example}\PY{p}{]}
             \PY{n}{image} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{n}{example}\PY{p}{,} \PY{p}{:}\PY{p}{]}\PY{o}{.}\PY{n}{values}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{28}\PY{p}{,} \PY{l+m+mi}{28}\PY{p}{]}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{n}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Example: }\PY{l+s+si}{\PYZob{}example\PYZcb{}}\PY{l+s+s1}{ Label: }\PY{l+s+si}{\PYZob{}label\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{image}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{n}{plt}\PY{o}{.}\PY{n}{get\PYZus{}cmap}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gray}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{n}{view\PYZus{}digit}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_14_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{Dimensionality Reduction
Algorithms}\label{dimensionality-reduction-algorithms}

The dimensionality reduction algorithms can be divided into tw main
groups: \textbf{linear projection} and \textbf{manifold learning} (or
\textbf{nonlinear dimensionality reduction}).

Examples of the first group are \textbf{priciple component analysis}
(PCA), \textbf{singular value decomposition} (SVD), and \textbf{random
projections}.

Examples of the second group include: - \textbf{isomap}: this technique
learns curved (or geodesic) distance between points rather than the
Euclidean distance. - \textbf{multidimensional scaling} (MDS) -
\textbf{local linear embeddings} - \textbf{t-distributed stochastic
neighbor embedding} (t-SNE) - \textbf{dictionary learning} -
\textbf{random trees embedding} - \textbf{independent component
analysis}

    \subsubsection{PCA}\label{pca}

We first expore several versions of PCA: standard, incremental, sparse,
and kernel.

The goal of PCA is to reduce the number of features (dimensions) while
retaining as much variation (salient information) as possible.

This is done by finding groups of correlated features. They are combined
so that a lower number of linearly uuncorrelated features are created.
The process involves projecting higher dimensional data into lower
dimensional space.

It is possible to reconstruct the original features from the reduced
versions, although not completely.

The reduced features are called \emph{principal components}.

The MNIST dataset has 784 features. We will try to reduce this number by
applying PCA. Although some information will be lost, the remaining will
still represent the original data close enough. In the same time, the
lower number of features will allow us to perform clustering and anomaly
detection much more effective.

\paragraph{Feature Scaling}\label{feature-scaling}

Feature scaling is essential for the correct work of PCA algorithms that
are very sensitive to the original features' ranges. In the MNIST
dataset all features are already in the same range of 0.0 to 1.0, so we
can skip this step.

\subsubsection{Apply PCA}\label{apply-pca}

We import \texttt{PCA} and set up the hyper parameters. The parameter
\texttt{whiten} is explained in the PCA docs:
https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{c+c1}{\PYZsh{} Principal Component Analysis}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{decomposition} \PY{k}{import} \PY{n}{PCA}
         
         \PY{n}{n\PYZus{}components} \PY{o}{=} \PY{l+m+mi}{784}
         \PY{n}{whiten} \PY{o}{=} \PY{k+kc}{False}
         \PY{n}{random\PYZus{}state} \PY{o}{=} \PY{l+m+mi}{2018}
         
         \PY{n}{pca} \PY{o}{=} \PY{n}{PCA}\PY{p}{(}\PY{n}{n\PYZus{}components}\PY{o}{=}\PY{n}{n\PYZus{}components}\PY{p}{,} \PY{n}{whiten}\PY{o}{=}\PY{n}{whiten}\PY{p}{,} \PYZbs{}
                   \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{random\PYZus{}state}\PY{p}{)}
         
         \PY{n}{X\PYZus{}train\PYZus{}PCA} \PY{o}{=} \PY{n}{pca}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
         \PY{n}{X\PYZus{}train\PYZus{}PCA} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}train\PYZus{}PCA}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{train\PYZus{}index}\PY{p}{)}
\end{Verbatim}


    With \texttt{n\_components} set to 784 we do not reduce the
dimensionality, we only transform the data. Still 100\% of the variance
must be retained:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{c+c1}{\PYZsh{} Percentage of Variance Captured by 784 principal components}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Variance Explained by all 784 principal components: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PYZbs{}
               \PY{n+nb}{sum}\PY{p}{(}\PY{n}{pca}\PY{o}{.}\PY{n}{explained\PYZus{}variance\PYZus{}ratio\PYZus{}}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Variance Explained by all 784 principal components:  0.9999999999999998

    \end{Verbatim}

    This transformation allows us to evaluate the importance of features:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{c+c1}{\PYZsh{} Percentage of Variance Captured by X principal components}
         \PY{n}{importanceOfPrincipalComponents} \PY{o}{=} \PYZbs{}
             \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{pca}\PY{o}{.}\PY{n}{explained\PYZus{}variance\PYZus{}ratio\PYZus{}}\PY{p}{)}
         
         
         \PY{n}{importanceOfPrincipalComponents} \PY{o}{=} \PY{n}{importanceOfPrincipalComponents}\PY{o}{.}\PY{n}{T}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Variance Captured by First 10 Principal Components: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
               \PY{n}{importanceOfPrincipalComponents}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{9}\PY{p}{]}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{.}\PY{n}{values}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Variance Captured by First 20 Principal Components: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
               \PY{n}{importanceOfPrincipalComponents}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{19}\PY{p}{]}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{.}\PY{n}{values}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Variance Captured by First 50 Principal Components: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
               \PY{n}{importanceOfPrincipalComponents}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{49}\PY{p}{]}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{.}\PY{n}{values}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Variance Captured by First 100 Principal Components: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
               \PY{n}{importanceOfPrincipalComponents}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{99}\PY{p}{]}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{.}\PY{n}{values}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Variance Captured by First 200 Principal Components: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
               \PY{n}{importanceOfPrincipalComponents}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{199}\PY{p}{]}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{.}\PY{n}{values}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Variance Captured by First 300 Principal Components: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
               \PY{n}{importanceOfPrincipalComponents}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{299}\PY{p}{]}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{.}\PY{n}{values}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Variance Captured by First 10 Principal Components:  [0.48876238]
Variance Captured by First 20 Principal Components:  [0.64398025]
Variance Captured by First 50 Principal Components:  [0.8248609]
Variance Captured by First 100 Principal Components:  [0.91465857]
Variance Captured by First 200 Principal Components:  [0.96650076]
Variance Captured by First 300 Principal Components:  [0.9862489]

    \end{Verbatim}

    One can see, that with the first 100 most important components we
already capture 91\% variance and with the first 300 - 98\%.

We can visualize the importance of the first 10 comnponents using this
chart. The \emph{y} axis represent the proportion of the explained
variance.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{n}{sns}\PY{o}{.}\PY{n}{set}\PY{p}{(}\PY{n}{rc}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{figure.figsize}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{\PYZcb{}}\PY{p}{)}
         \PY{n}{sns}\PY{o}{.}\PY{n}{barplot}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{importanceOfPrincipalComponents}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{9}\PY{p}{]}\PY{p}{,}\PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{k}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{;}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_23_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Lets try mapping our dataset using two dimensions provided by the first
two most important components.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} \PY{k}{def} \PY{n+nf}{scatterPlot}\PY{p}{(}\PY{n}{xDF}\PY{p}{,} \PY{n}{yDF}\PY{p}{,} \PY{n}{algoName}\PY{p}{)}\PY{p}{:}
             \PY{c+c1}{\PYZsh{} take othe first two columns only}
             \PY{n}{tempDF} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{xDF}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{xDF}\PY{o}{.}\PY{n}{index}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} join the labels}
             \PY{n}{tempDF} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{concat}\PY{p}{(}\PY{p}{(}\PY{n}{tempDF}\PY{p}{,}\PY{n}{yDF}\PY{p}{)}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{join}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{inner}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
             \PY{n}{tempDF}\PY{o}{.}\PY{n}{columns} \PY{o}{=} \PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{First Vector}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Second Vector}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Label}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
             
             \PY{n}{sns}\PY{o}{.}\PY{n}{lmplot}\PY{p}{(}\PY{n}{x}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{First Vector}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{y}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Second Vector}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{hue}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Label}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PYZbs{}
                        \PY{n}{data}\PY{o}{=}\PY{n}{tempDF}\PY{p}{,} \PY{n}{fit\PYZus{}reg}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
             \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{gca}\PY{p}{(}\PY{p}{)}
             \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Separation of Observations using }\PY{l+s+s2}{\PYZdq{}}\PY{o}{+}\PY{n}{algoName}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}17}]:} \PY{n}{scatterPlot}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}PCA}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{PCA}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_26_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    We can see that the fist two components do quite a good job separating
the labels. This separation is learned without using the original
labels. This is the power of PCA.

We can compare this graph with a similar one created out of the
originial, non-PCA-transformed data. We take two features representing
pixels in the center of the image:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} \PY{n}{X\PYZus{}train\PYZus{}scatter} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{p}{[}\PY{l+m+mi}{350}\PY{p}{,}\PY{l+m+mi}{406}\PY{p}{]}\PY{p}{]}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{index}\PY{p}{)}
         \PY{n}{X\PYZus{}train\PYZus{}scatter} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{concat}\PY{p}{(}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}scatter}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{)}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{join}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{inner}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n}{X\PYZus{}train\PYZus{}scatter}\PY{o}{.}\PY{n}{columns} \PY{o}{=} \PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{First Vector}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Second Vector}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Label}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
         \PY{n}{sns}\PY{o}{.}\PY{n}{lmplot}\PY{p}{(}\PY{n}{x}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{First Vector}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{y}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Second Vector}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{hue}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Label}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}train\PYZus{}scatter}\PY{p}{,} \PY{n}{fit\PYZus{}reg}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
         \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{gca}\PY{p}{(}\PY{p}{)}
         \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Separation of Observations Using Original Feature Set}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{;}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_28_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    The label separation is much less obvious, almost indistinguishable from
noise.

    \subsubsection{Incremental PCA}\label{incremental-pca}

For datasets that are too large to fit in memory, we can use PCA
incrementally. We define small batches of data (manually or
automatically). This batch-processed PCA is called \textbf{incremental
PCA}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{c+c1}{\PYZsh{} Incremental PCA}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{decomposition} \PY{k}{import} \PY{n}{IncrementalPCA}
         
         \PY{n}{n\PYZus{}components} \PY{o}{=} \PY{l+m+mi}{784}
         \PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{k+kc}{None}
         
         \PY{n}{incrementalPCA} \PY{o}{=} \PY{n}{IncrementalPCA}\PY{p}{(}\PY{n}{n\PYZus{}components}\PY{o}{=}\PY{n}{n\PYZus{}components}\PY{p}{,} \PYZbs{}
                                         \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{n}{batch\PYZus{}size}\PY{p}{)}
         
         \PY{n}{X\PYZus{}train\PYZus{}incrementalPCA} \PY{o}{=} \PY{n}{incrementalPCA}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
         
         \PY{n}{X\PYZus{}train\PYZus{}incrementalPCA} \PY{o}{=} \PYZbs{}
             \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}train\PYZus{}incrementalPCA}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{train\PYZus{}index}\PY{p}{)}
         
         
         \PY{n}{X\PYZus{}validation\PYZus{}incrementalPCA} \PY{o}{=} \PY{n}{incrementalPCA}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}validation}\PY{p}{)}
         
         \PY{n}{X\PYZus{}validation\PYZus{}incrementalPCA} \PY{o}{=} \PYZbs{}
             \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}validation\PYZus{}incrementalPCA}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{validation\PYZus{}index}\PY{p}{)}
         
         \PY{n}{scatterPlot}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}incrementalPCA}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Incremental PCA}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_31_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{Sparse PCA}\label{sparse-pca}

The regular PCA looks for linear combinations of all the input variables
and reduced the original space as densely as possible.

For some machine learning algorithms, some degree of sparsity may be
preferred. There is a version of PCA that controls the sparsity using
the parameter \texttt{alpha}: \textbf{sparse PCA}.

The sparse PCA searches for linear combinations in just some of the
input variables. The original feature space is reduced not as densely
ans with normal PCA.

Sparse PCA runs more slowly than the regular. So we train it only on the
first 10000 samples because we only need to demonstrate it.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}20}]:} \PY{c+c1}{\PYZsh{} Sparse PCA}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{decomposition} \PY{k}{import} \PY{n}{SparsePCA}
         
         \PY{n}{n\PYZus{}components} \PY{o}{=} \PY{l+m+mi}{100}
         \PY{n}{alpha} \PY{o}{=} \PY{l+m+mf}{0.0001}
         \PY{n}{random\PYZus{}state} \PY{o}{=} \PY{l+m+mi}{2018}
         \PY{n}{n\PYZus{}jobs} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}
         
         \PY{n}{sparsePCA} \PY{o}{=} \PY{n}{SparsePCA}\PY{p}{(}\PY{n}{n\PYZus{}components}\PY{o}{=}\PY{n}{n\PYZus{}components}\PY{p}{,} \PYZbs{}
                         \PY{n}{alpha}\PY{o}{=}\PY{n}{alpha}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{random\PYZus{}state}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{n}{n\PYZus{}jobs}\PY{p}{)}
         
         \PY{n}{sparsePCA}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{10000}\PY{p}{,}\PY{p}{:}\PY{p}{]}\PY{p}{)}
         \PY{n}{X\PYZus{}train\PYZus{}sparsePCA} \PY{o}{=} \PY{n}{sparsePCA}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
         \PY{n}{X\PYZus{}train\PYZus{}sparsePCA} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}train\PYZus{}sparsePCA}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{train\PYZus{}index}\PY{p}{)}
         
         \PY{n}{X\PYZus{}validation\PYZus{}sparsePCA} \PY{o}{=} \PY{n}{sparsePCA}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}validation}\PY{p}{)}
         \PY{n}{X\PYZus{}validation\PYZus{}sparsePCA} \PY{o}{=} \PYZbs{}
             \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}validation\PYZus{}sparsePCA}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{validation\PYZus{}index}\PY{p}{)}
         
         \PY{n}{scatterPlot}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}sparsePCA}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Sparse PCA}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_33_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{Kernel PCA}\label{kernel-pca}

Normaly PCA, incremental PCA, sparse PCA linearly project datapoints
into a lower dimensional space. In case when data is not linearly
separable, we need a non-linear projection. Kernel PCA runs a non-linear
function that computes similarity between two points. Kernel PCA maps
the implicit feature space where the majority of the data points lie and
creates this feature space in a much lower number of dimensions.

For a kernel PCA we need to specify the number of components, a kernel
function, the kernel coefficient (\emph{gamma}). The most popular kernel
is \textbf{radial basis function} kernel (RBF).

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}22}]:} \PY{c+c1}{\PYZsh{} KErnel PCA}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{decomposition} \PY{k}{import} \PY{n}{KernelPCA}
         
         \PY{n}{n\PYZus{}components} \PY{o}{=} \PY{l+m+mi}{100}
         \PY{n}{kernel} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rbf}\PY{l+s+s1}{\PYZsq{}}
         \PY{n}{gamma} \PY{o}{=} \PY{k+kc}{None}
         \PY{n}{random\PYZus{}state} \PY{o}{=} \PY{l+m+mi}{2018}
         
         \PY{n}{n\PYZus{}jobs} \PY{o}{=} \PY{l+m+mi}{1}
         
         \PY{n}{kernelPCA} \PY{o}{=} \PY{n}{KernelPCA}\PY{p}{(}\PY{n}{n\PYZus{}components}\PY{o}{=}\PY{n}{n\PYZus{}components}\PY{p}{,} \PY{n}{kernel}\PY{o}{=}\PY{n}{kernel}\PY{p}{,}\PYZbs{}
                              \PY{n}{gamma}\PY{o}{=}\PY{n}{gamma}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{n}{n\PYZus{}jobs}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{random\PYZus{}state}\PY{p}{)}
         
         \PY{n}{kernelPCA}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{10000}\PY{p}{,}\PY{p}{:}\PY{p}{]}\PY{p}{)}
         \PY{n}{X\PYZus{}train\PYZus{}kernelPCA} \PY{o}{=} \PY{n}{kernelPCA}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
         \PY{n}{X\PYZus{}train\PYZus{}kernelPCA} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}train\PYZus{}kernelPCA}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{train\PYZus{}index}\PY{p}{)}
         
         \PY{n}{X\PYZus{}validation\PYZus{}kernelPCA} \PY{o}{=} \PY{n}{kernelPCA}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}validation}\PY{p}{)}
         \PY{n}{X\PYZus{}validation\PYZus{}kernelPCA} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}validation}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{validation\PYZus{}index}\PY{p}{)}
         
         \PY{n}{scatterPlot}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}kernelPCA}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Kernel PCA}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_35_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    The separation we see in the scatter plot for the kernel PCA does not
differ much from the one generated by the normal PCA. The use of kernel
PCA in this case does not offer much improvement.

\subsection{Singular Value
Decomposition}\label{singular-value-decomposition}

The idea behind the SVD is to reduce the rank of the original matrix by
learning the underlying structure of the data and building a matrix of a
smaller rank, This way it is possible to recreate the original matrix by
using a linear combination of some of the vectors in the smaller rank
matrix.

To generate a smaller rank matrix, SVD keeps the vectors in the original
matrix that have the most information (highest singular value). This
way, the smaller matrix will contain the most important elements of the
original feature space.

Compare this to the PCA. PCA uses the eigen-values decomposition of the
covariance matrix to perform dimensionality reduction. SVD uses the
singular value decomposition. Calculating PCA involves performing SVD.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}26}]:} \PY{c+c1}{\PYZsh{} Singular Value Decomposition}
         
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{decomposition} \PY{k}{import} \PY{n}{TruncatedSVD}
         
         \PY{n}{n\PYZus{}components} \PY{o}{=} \PY{l+m+mi}{200}
         \PY{n}{algorithm} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{randomized}\PY{l+s+s1}{\PYZsq{}}
         \PY{n}{n\PYZus{}iter} \PY{o}{=} \PY{l+m+mi}{5}
         \PY{n}{random\PYZus{}state} \PY{o}{=} \PY{l+m+mi}{2018}
         
         
         \PY{n}{svd} \PY{o}{=} \PY{n}{TruncatedSVD}\PY{p}{(}\PY{n}{n\PYZus{}components}\PY{o}{=}\PY{n}{n\PYZus{}components}\PY{p}{,} \PY{n}{algorithm}\PY{o}{=}\PY{n}{algorithm}\PY{p}{,}\PYZbs{}
                           \PY{n}{n\PYZus{}iter}\PY{o}{=}\PY{n}{n\PYZus{}iter}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{random\PYZus{}state}\PY{p}{)}
         
         \PY{n}{X\PYZus{}train\PYZus{}svd} \PY{o}{=} \PY{n}{svd}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
         \PY{n}{X\PYZus{}train\PYZus{}svd} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}train\PYZus{}svd}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{train\PYZus{}index}\PY{p}{)}
         
         \PY{n}{X\PYZus{}validation\PYZus{}svd} \PY{o}{=} \PY{n}{svd}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}validation}\PY{p}{)}
         \PY{n}{X\PYZus{}validation\PYZus{}svd} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}validation\PYZus{}svd}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{validation\PYZus{}index}\PY{p}{)}
         
         \PY{n}{scatterPlot}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}svd}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Singular Value Decomposition}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_37_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{Random Projection}\label{random-projection}

Dimentionality reduction can be achieved by using a technique called
random projection. This is based on the \emph{Johnson-Lindenstrauss
lemma}. According to it, points in a high-dimensional space can be
embedded into a much lower-dimensional space so that distances between
the points are nearly preserved.

\subsubsection{Gaussian Random
Projection}\label{gaussian-random-projection}

There are two versions of random projection: the \textbf{Gaussian random
projection} and the \textbf{sparse random projection}.

In Gaussian random projection, we can set the number of the target
components or we can use the hyperparameter called \emph{eps}. This
parameter controls the quality of the transformation and its smaller
values will generate a higher number of dimensions.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}28}]:} \PY{c+c1}{\PYZsh{} Gaussian random projection}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{random\PYZus{}projection} \PY{k}{import} \PY{n}{GaussianRandomProjection}
         
         \PY{n}{n\PYZus{}components} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{auto}\PY{l+s+s1}{\PYZsq{}}
         \PY{n}{eps} \PY{o}{=} \PY{l+m+mf}{0.5}
         \PY{n}{random\PYZus{}state} \PY{o}{=} \PY{l+m+mi}{2018}
         
         \PY{n}{GRP} \PY{o}{=} \PY{n}{GaussianRandomProjection}\PY{p}{(}\PY{n}{n\PYZus{}components}\PY{o}{=}\PY{n}{n\PYZus{}components}\PY{p}{,} \PY{n}{eps}\PY{o}{=}\PY{n}{eps}\PY{p}{,}\PYZbs{}
                                       \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{random\PYZus{}state}\PY{p}{)}
         
         \PY{n}{X\PYZus{}train\PYZus{}GRP} \PY{o}{=} \PY{n}{GRP}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
         \PY{n}{X\PYZus{}train\PYZus{}GRP} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}train\PYZus{}GRP}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{train\PYZus{}index}\PY{p}{)}
         
         \PY{n}{X\PYZus{}validation\PYZus{}GRP} \PY{o}{=} \PY{n}{GRP}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}validation}\PY{p}{)}
         \PY{n}{X\PYZus{}validation\PYZus{}GRP} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}validation\PYZus{}GRP}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{validation\PYZus{}index}\PY{p}{)}
         
         \PY{n}{scatterPlot}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}GRP}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Gaussian Random Projection}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_39_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    The scatter plot for the Gaussian random projection differs from those
produced by PCA. This is because random projection is an entirely
different type of dimensionality reduction.

\subsubsection{Sparse Random Projection}\label{sparse-random-projection}

Just as there is a sparse version of PCA, there is a sparse version for
random projection. It retains some degree of sparsity in the transformed
dataset and is generally more efficient. It transforms data faster than
the Gaussian random projection.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}29}]:} \PY{c+c1}{\PYZsh{}Sparse Random Projection}
         
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{random\PYZus{}projection} \PY{k}{import} \PY{n}{SparseRandomProjection}
         
         \PY{n}{n\PYZus{}components} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{auto}\PY{l+s+s1}{\PYZsq{}}
         \PY{n}{density} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{auto}\PY{l+s+s1}{\PYZsq{}}
         \PY{n}{eps} \PY{o}{=} \PY{l+m+mf}{0.5}
         \PY{n}{dense\PYZus{}output} \PY{o}{=} \PY{k+kc}{False}
         \PY{n}{random\PYZus{}state} \PY{o}{=} \PY{l+m+mi}{2018}
         
         \PY{n}{SRP} \PY{o}{=} \PY{n}{SparseRandomProjection}\PY{p}{(}\PY{n}{n\PYZus{}components}\PY{o}{=}\PY{n}{n\PYZus{}components}\PY{p}{,} \PY{n}{density}\PY{o}{=}\PY{n}{density}\PY{p}{,}\PYZbs{}
                                     \PY{n}{eps}\PY{o}{=}\PY{n}{eps}\PY{p}{,} \PY{n}{dense\PYZus{}output}\PY{o}{=}\PY{n}{dense\PYZus{}output}\PY{p}{,}\PYZbs{}
                                     \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{random\PYZus{}state}\PY{p}{)}
         
         \PY{n}{X\PYZus{}train\PYZus{}SRP} \PY{o}{=} \PY{n}{SRP}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
         \PY{n}{X\PYZus{}train\PYZus{}SRP} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}train\PYZus{}SRP}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{train\PYZus{}index}\PY{p}{)}
         
         \PY{n}{X\PYZus{}validation\PYZus{}SRP} \PY{o}{=} \PY{n}{SRP}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{X\PYZus{}validation}\PY{p}{)}
         \PY{n}{X\PYZus{}validation\PYZus{}SRP} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}validation\PYZus{}SRP}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{validation\PYZus{}index}\PY{p}{)}
         
         \PY{n}{scatterPlot}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}SRP}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Sparse Random Projection}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_41_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{Isomap}\label{isomap}

Besides methods that use linear projections, there is a family on
non-linear methods. Thise are known as \textbf{manifold learning}.

The basic form of manifold learning is \textbf{Isomap} (\emph{isometric
mapping}). Similar to Kernel PCA, Isomap learns a new low-dimensional
embedding of the original feature set by calculating pairwise distances
of all points, where the distance is \textbf{curved} or \textbf{geodesic
distance} instead of Euclidean distance.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}31}]:} \PY{c+c1}{\PYZsh{} Isomap}
         
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{manifold} \PY{k}{import} \PY{n}{Isomap}
         
         \PY{n}{n\PYZus{}neighbors} \PY{o}{=} \PY{l+m+mi}{5}
         \PY{n}{n\PYZus{}components} \PY{o}{=} \PY{l+m+mi}{10}
         \PY{n}{n\PYZus{}jobs} \PY{o}{=} \PY{l+m+mi}{4}
         
         \PY{n}{isomap} \PY{o}{=} \PY{n}{Isomap}\PY{p}{(}\PY{n}{n\PYZus{}neighbors}\PY{o}{=}\PY{n}{n\PYZus{}neighbors}\PY{p}{,} \PYZbs{}
                         \PY{n}{n\PYZus{}components}\PY{o}{=}\PY{n}{n\PYZus{}components}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{n}{n\PYZus{}jobs}\PY{p}{)}
         
         \PY{n}{isomap}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{5000}\PY{p}{,}\PY{p}{:}\PY{p}{]}\PY{p}{)}
         \PY{n}{X\PYZus{}train\PYZus{}isomap} \PY{o}{=} \PY{n}{isomap}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
         \PY{n}{X\PYZus{}train\PYZus{}isomap} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}train\PYZus{}isomap}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{train\PYZus{}index}\PY{p}{)}
         
         \PY{n}{X\PYZus{}validation\PYZus{}isomap} \PY{o}{=} \PY{n}{isomap}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}validation}\PY{p}{)}
         \PY{n}{X\PYZus{}validation\PYZus{}isomap} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}validation\PYZus{}isomap}\PY{p}{,} \PYZbs{}
                                            \PY{n}{index}\PY{o}{=}\PY{n}{validation\PYZus{}index}\PY{p}{)}
         
         \PY{n}{scatterPlot}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}isomap}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Isomap}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_43_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{Multidimensional Scaling}\label{multidimensional-scaling}

Multidimensional Scaling learns the similarity between points and uses
this similarity to build a lower-dimensional space.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}34}]:} \PY{c+c1}{\PYZsh{} Multidimensional Scaling}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{manifold} \PY{k}{import} \PY{n}{MDS}
         
         \PY{n}{n\PYZus{}components} \PY{o}{=} \PY{l+m+mi}{2}
         \PY{n}{n\PYZus{}init} \PY{o}{=} \PY{l+m+mi}{12}
         \PY{n}{max\PYZus{}iter} \PY{o}{=} \PY{l+m+mi}{1200}
         \PY{n}{metric} \PY{o}{=} \PY{k+kc}{True}
         \PY{n}{n\PYZus{}jobs} \PY{o}{=} \PY{l+m+mi}{4}
         \PY{n}{random\PYZus{}state} \PY{o}{=} \PY{l+m+mi}{2018}
         
         \PY{n}{mds} \PY{o}{=} \PY{n}{MDS}\PY{p}{(}\PY{n}{n\PYZus{}components}\PY{o}{=}\PY{n}{n\PYZus{}components}\PY{p}{,} \PY{n}{n\PYZus{}init}\PY{o}{=}\PY{n}{n\PYZus{}init}\PY{p}{,} \PY{n}{max\PYZus{}iter}\PY{o}{=}\PY{n}{max\PYZus{}iter}\PY{p}{,} \PYZbs{}
                   \PY{n}{metric}\PY{o}{=}\PY{n}{metric}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{n}{n\PYZus{}jobs}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{random\PYZus{}state}\PY{p}{)}
         
         \PY{n}{X\PYZus{}train\PYZus{}mds} \PY{o}{=} \PY{n}{mds}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{1000}\PY{p}{,}\PY{p}{:}\PY{p}{]}\PY{p}{)}
         \PY{n}{X\PYZus{}train\PYZus{}mds} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}train\PYZus{}mds}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{train\PYZus{}index}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{1001}\PY{p}{]}\PY{p}{)}
         
         \PY{n}{scatterPlot}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}mds}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Multidimensional Scaling}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
C:\textbackslash{}ProgramData\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}sklearn\textbackslash{}metrics\textbackslash{}pairwise.py:257: RuntimeWarning: invalid value encountered in sqrt
  return distances if squared else np.sqrt(distances, out=distances)

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_45_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{Locally Linear Embedding}\label{locally-linear-embedding}

Locally Linear Embedding preserves the distances between pojnts in local
neighborhoods as it projects data to a lower-dimensional space. LLE
discovers non-linear structure in the original, high-dimensional data by
segmenting the data into smaller components (that is, neighborhoods of
points) and modelling each components as a linear embedding.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}35}]:} \PY{c+c1}{\PYZsh{} Locally Linear Embeddings}
         
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{manifold} \PY{k}{import} \PY{n}{LocallyLinearEmbedding}
         
         \PY{n}{n\PYZus{}neighbors} \PY{o}{=} \PY{l+m+mi}{10}
         \PY{n}{n\PYZus{}components} \PY{o}{=} \PY{l+m+mi}{2}
         \PY{n}{method} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{modified}\PY{l+s+s1}{\PYZsq{}}
         \PY{n}{n\PYZus{}jobs} \PY{o}{=} \PY{l+m+mi}{4}
         \PY{n}{random\PYZus{}state} \PY{o}{=} \PY{l+m+mi}{2018}
         
         \PY{n}{lle} \PY{o}{=} \PY{n}{LocallyLinearEmbedding}\PY{p}{(}\PY{n}{n\PYZus{}neighbors}\PY{o}{=}\PY{n}{n\PYZus{}neighbors}\PY{p}{,} \PYZbs{}
                                      \PY{n}{n\PYZus{}components}\PY{o}{=}\PY{n}{n\PYZus{}components}\PY{p}{,} \PY{n}{method}\PY{o}{=}\PY{n}{method}\PY{p}{,}\PYZbs{}
                                     \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{random\PYZus{}state}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{n}{n\PYZus{}jobs}\PY{p}{)}
         
         \PY{n}{lle}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{5000}\PY{p}{,}\PY{p}{:}\PY{p}{]}\PY{p}{)}
         
         \PY{n}{X\PYZus{}train\PYZus{}lle} \PY{o}{=} \PY{n}{lle}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
         \PY{n}{X\PYZus{}train\PYZus{}lle} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}train\PYZus{}lle}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{train\PYZus{}index}\PY{p}{)}
         
         \PY{n}{X\PYZus{}validation\PYZus{}lle} \PY{o}{=} \PY{n}{lle}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}validation}\PY{p}{)}
         \PY{n}{X\PYZus{}validation\PYZus{}lle} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}validation}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{validation\PYZus{}index}\PY{p}{)}
         
         \PY{n}{scatterPlot}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}lle}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Locally Linear Embedding}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_47_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{t-Distributed Stochastic Neighbor
Embedding}\label{t-distributed-stochastic-neighbor-embedding}

Another nonlinear dimensionality reduction method is \emph{t-distributed
Stochastic Neighbor Embedding} (\emph{t-SNE}). It is used for
visualisation of high-dimensional data. t\_SNE models each
high-dimensional point in a two- or three-dimensional space. Points that
are similar are modelled close to each other, while dissimilar points
are modelled father away.

t-SNE constructs two probability distributions, one over pairs of points
in the original space, and another over pair of points in the
low-dimensional space. In effect, similar points will have high
probability, and dissimilar points will have low probability.
Specifically, t-SNE minimized the \emph{Killback-Leibler divergence}
between two probability distributions.

In the real-world applications of t-SNE it is best to apply some form of
dimensionality reduction first (e.g., PCA) to reduce the number of
dimensions before applying t-SNE. This will reduce the noise in features
and speed up the computation.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}37}]:} \PY{c+c1}{\PYZsh{} t\PYZhy{}SNE}
         
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{manifold} \PY{k}{import} \PY{n}{TSNE}
         
         \PY{n}{n\PYZus{}components} \PY{o}{=} \PY{l+m+mi}{2}
         \PY{n}{learning\PYZus{}rate} \PY{o}{=} \PY{l+m+mi}{300}
         \PY{n}{perplexity} \PY{o}{=} \PY{l+m+mi}{30}
         \PY{n}{early\PYZus{}exaggeration} \PY{o}{=} \PY{l+m+mi}{12}
         \PY{n}{init} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{random}\PY{l+s+s1}{\PYZsq{}}
         \PY{n}{random\PYZus{}state} \PY{o}{=} \PY{l+m+mi}{2018}
         
         \PY{n}{tSNE} \PY{o}{=} \PY{n}{TSNE}\PY{p}{(}\PY{n}{n\PYZus{}components}\PY{o}{=}\PY{n}{n\PYZus{}components}\PY{p}{,} \PY{n}{learning\PYZus{}rate}\PY{o}{=}\PY{n}{learning\PYZus{}rate}\PY{p}{,}\PYZbs{}
                    \PY{n}{perplexity}\PY{o}{=}\PY{n}{perplexity}\PY{p}{,} \PY{n}{early\PYZus{}exaggeration}\PY{o}{=}\PY{n}{early\PYZus{}exaggeration}\PY{p}{,}\PYZbs{}
                    \PY{n}{init}\PY{o}{=}\PY{n}{init}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{random\PYZus{}state}\PY{p}{)}
         
         \PY{n}{X\PYZus{}train\PYZus{}tSNE} \PY{o}{=} \PY{n}{tSNE}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}PCA}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{5000}\PY{p}{,}\PY{p}{:}\PY{l+m+mi}{9}\PY{p}{]}\PY{p}{)}
         \PY{n}{X\PYZus{}train\PYZus{}tSNE} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}train\PYZus{}tSNE}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{train\PYZus{}index}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{5001}\PY{p}{]}\PY{p}{)}
         
         \PY{n}{scatterPlot}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}tSNE}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{t\PYZhy{}SNE}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_49_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \section{Other Dimensionality Reduction
Methods}\label{other-dimensionality-reduction-methods}

There are methods that do not rely on any sort of distance or geometric
metric.

\subsection{Dictionary Learning}\label{dictionary-learning}

\textbf{Dictionary Learning} is a method that learns a sparse
representation of the original data. The resulting matrix is called
\emph{dictionary} and vectors that populate it are called \emph{atoms}.
These atoms are binary vectors (i.e., with values of ones and zeros).
Each original feature can be reconstructed as a weighted sum of these
atoms.

If we have \emph{d} features in the original dataset and \emph{n} atoms
in the dictionary, we will have an \emph{undercomplete} dictionary when
\(n < d\) and an \emph{overcomplete} dictionary when \(n>d\). With an
undercomplete dictionary we achieve dimensionality reduction.

To represent images in a two-dimensional scatterplot, we will have to
learn a very dense dictionary. In practice, we would use a sparser
version.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}38}]:} \PY{c+c1}{\PYZsh{} Mini\PYZhy{}batch dictionary learning}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{decomposition} \PY{k}{import} \PY{n}{MiniBatchDictionaryLearning}
         
         \PY{n}{n\PYZus{}components} \PY{o}{=} \PY{l+m+mi}{50}
         \PY{n}{alpha} \PY{o}{=} \PY{l+m+mi}{1}
         \PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{200}
         \PY{n}{n\PYZus{}iter} \PY{o}{=} \PY{l+m+mi}{25}
         \PY{n}{random\PYZus{}state} \PY{o}{=} \PY{l+m+mi}{2018}
         
         \PY{n}{miniBatchDictLearning} \PY{o}{=} \PY{n}{MiniBatchDictionaryLearning}\PY{p}{(} \PYZbs{}
                                 \PY{n}{n\PYZus{}components}\PY{o}{=}\PY{n}{n\PYZus{}components}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{n}{alpha}\PY{p}{,} \PYZbs{}
                                 \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{n}{n\PYZus{}iter}\PY{o}{=}\PY{n}{n\PYZus{}iter}\PY{p}{,} \PYZbs{}
                                 \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{random\PYZus{}state}\PY{p}{)}
         
         \PY{n}{miniBatchDictLearning}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{l+m+mi}{10000}\PY{p}{]}\PY{p}{)}
         \PY{n}{X\PYZus{}train\PYZus{}miniBatchDictLearning} \PY{o}{=} \PY{n}{miniBatchDictLearning}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
         \PY{n}{X\PYZus{}train\PYZus{}miniBatchDictLearning} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(} \PYZbs{}
             \PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}train\PYZus{}miniBatchDictLearning}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{train\PYZus{}index}\PY{p}{)}
         
         \PY{n}{X\PYZus{}validation\PYZus{}miniBatchDictLearning} \PY{o}{=} \PYZbs{}
             \PY{n}{miniBatchDictLearning}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}validation}\PY{p}{)}
         \PY{n}{X\PYZus{}validation\PYZus{}miniBatchDictLearning} \PY{o}{=} \PYZbs{}
             \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}validation\PYZus{}miniBatchDictLearning}\PY{p}{,} \PYZbs{}
             \PY{n}{index}\PY{o}{=}\PY{n}{validation\PYZus{}index}\PY{p}{)}
         
         \PY{n}{scatterPlot}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}miniBatchDictLearning}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PYZbs{}
                     \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Mini\PYZhy{}batch Dictionary Learning}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_51_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{Independent Component
Analysis}\label{independent-component-analysis}

One problem with unlabeled data is that many individual signals are
blended in together. By applying independent component analysis, we
separate these signals into their individual components. We can then
reconstruct the originaol features by adding together some combinations
of the individual components. ICA is commonly used in signal processing
tasks such as, identifying individual voices in a busy coffeehouse.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}39}]:} \PY{c+c1}{\PYZsh{} Independent Component Analysis}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{decomposition} \PY{k}{import} \PY{n}{FastICA}
         
         \PY{n}{n\PYZus{}components} \PY{o}{=} \PY{l+m+mi}{25}
         \PY{n}{algorithm} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{parallel}\PY{l+s+s1}{\PYZsq{}}
         \PY{n}{whiten} \PY{o}{=} \PY{k+kc}{True}
         \PY{n}{max\PYZus{}iter} \PY{o}{=} \PY{l+m+mi}{100}
         \PY{n}{random\PYZus{}state} \PY{o}{=} \PY{l+m+mi}{2018}
         
         \PY{n}{fastICA} \PY{o}{=} \PY{n}{FastICA}\PY{p}{(}\PY{n}{n\PYZus{}components}\PY{o}{=}\PY{n}{n\PYZus{}components}\PY{p}{,} \PY{n}{algorithm}\PY{o}{=}\PY{n}{algorithm}\PY{p}{,} \PYZbs{}
                           \PY{n}{whiten}\PY{o}{=}\PY{n}{whiten}\PY{p}{,} \PY{n}{max\PYZus{}iter}\PY{o}{=}\PY{n}{max\PYZus{}iter}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{random\PYZus{}state}\PY{p}{)}
         
         \PY{n}{X\PYZus{}train\PYZus{}fastICA} \PY{o}{=} \PY{n}{fastICA}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
         \PY{n}{X\PYZus{}train\PYZus{}fastICA} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}train\PYZus{}fastICA}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{train\PYZus{}index}\PY{p}{)}
         
         \PY{n}{X\PYZus{}validation\PYZus{}fastICA} \PY{o}{=} \PY{n}{fastICA}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}validation}\PY{p}{)}
         \PY{n}{X\PYZus{}validation\PYZus{}fastICA} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}validation\PYZus{}fastICA}\PY{p}{,} \PYZbs{}
                                             \PY{n}{index}\PY{o}{=}\PY{n}{validation\PYZus{}index}\PY{p}{)}
         
         \PY{n}{scatterPlot}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}fastICA}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Independent Component Analysis}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_53_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \section{Conclusion}\label{conclusion}

In this chapter, we have looked into linear and nonlinear methods of
dimensionality reduction. This methods allow us to extract the most
meaningful part of data and represent the original dataset with a lower
number of features (dimensions). This is achieved without using any
labeled data. The result allows to meaningfully separate digits with
only two dimensions.


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
