
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{Chap04}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \section{Anomaly detection}\label{anomaly-detection}

In this chapter, we will use dimensionality reduction algorithm for the
task of anomaly detection. We will develop an anomaly detection solution
for the task of capturing fraud in a credit card transaction dataset.

When we use supervised learning to solve this problem, we rely upon
provided labels that allow the learning algorithm to distinguish between
genuine and fraudulent transactions. With time, however, fraud patterns
change and the algorithm has to be constantly retrained to capture new
fraud trends. For this reason, using a non-supervised learning algorithm
for fraud detection is very popular.

\subsubsection{Prepare the data}\label{prepare-the-data}

The credit card dataset contains 284804 transactions, of which 492 are
fraudulent. We will not use labels to train the algorithm, instead, we
will evaluate the algorithm using known labels. We split the dataset
into a training and a test set using a balanced split.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{c+c1}{\PYZsh{} Required imports}
        \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}Main\PYZsq{}\PYZsq{}\PYZsq{}}
        \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
        \PY{k+kn}{import} \PY{n+nn}{os}\PY{o}{,} \PY{n+nn}{time}
        \PY{k+kn}{import} \PY{n+nn}{pickle}\PY{o}{,} \PY{n+nn}{gzip}
        
        \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}Data Viz\PYZsq{}\PYZsq{}\PYZsq{}}
        \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
        \PY{k+kn}{import} \PY{n+nn}{seaborn} \PY{k}{as} \PY{n+nn}{sns}
        \PY{n}{color} \PY{o}{=} \PY{n}{sns}\PY{o}{.}\PY{n}{color\PYZus{}palette}\PY{p}{(}\PY{p}{)}
        \PY{k+kn}{import} \PY{n+nn}{matplotlib} \PY{k}{as} \PY{n+nn}{mpl}
        
        \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
        
        \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}Data Prep and Model Evaluation\PYZsq{}\PYZsq{}\PYZsq{}}
        \PY{k+kn}{from} \PY{n+nn}{sklearn} \PY{k}{import} \PY{n}{preprocessing} \PY{k}{as} \PY{n}{pp}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{train\PYZus{}test\PYZus{}split} 
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{precision\PYZus{}recall\PYZus{}curve}\PY{p}{,} \PY{n}{average\PYZus{}precision\PYZus{}score}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{roc\PYZus{}curve}\PY{p}{,} \PY{n}{auc}\PY{p}{,} \PY{n}{roc\PYZus{}auc\PYZus{}score}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{c+c1}{\PYZsh{} Load datasets}
        \PY{n}{current\PYZus{}path} \PY{o}{=} \PY{n}{os}\PY{o}{.}\PY{n}{getcwd}\PY{p}{(}\PY{p}{)}
        \PY{n}{file} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}\PYZbs{}}\PY{l+s+s1}{datasets}\PY{l+s+se}{\PYZbs{}\PYZbs{}}\PY{l+s+s1}{credit\PYZus{}card\PYZus{}data}\PY{l+s+se}{\PYZbs{}\PYZbs{}}\PY{l+s+s1}{credit\PYZus{}card.csv}\PY{l+s+s1}{\PYZsq{}}
        \PY{n}{data} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{n}{current\PYZus{}path} \PY{o}{+} \PY{n}{file}\PY{p}{)}
        
        \PY{n}{dataX} \PY{o}{=} \PY{n}{data}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Class}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
        \PY{n}{dataY} \PY{o}{=} \PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Class}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
        
        \PY{n}{featuresToScale} \PY{o}{=} \PY{n}{dataX}\PY{o}{.}\PY{n}{columns}
        \PY{n}{sX} \PY{o}{=} \PY{n}{pp}\PY{o}{.}\PY{n}{StandardScaler}\PY{p}{(}\PY{n}{copy}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
        \PY{n}{dataX}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{n}{featuresToScale}\PY{p}{]} \PY{o}{=} \PY{n}{sX}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{dataX}\PY{p}{[}\PY{n}{featuresToScale}\PY{p}{]}\PY{p}{)}
        
        \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PYZbs{}
            \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{dataX}\PY{p}{,} \PY{n}{dataY}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.33}\PY{p}{,} \PYZbs{}
                            \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{2018}\PY{p}{,} \PY{n}{stratify}\PY{o}{=}\PY{n}{dataY}\PY{p}{)}
\end{Verbatim}


    \subsubsection{Define Anomaly Score
Function}\label{define-anomaly-score-function}

Using anomaly detection to capture fraud means that we consider the
fraudulent transaction to be anomalies. That is, the more anomalous the
transaction is, the more likely it is fraudulent.

Dimensionality reduction algorithms reduce dimensions of the data while
trying to minimize the reconstruction error. They identify the most
salient features and keep them while discarding the less important
features. We can expect that anomalous entries will suffer the most when
the data is reconstructed. This means we can use the reconstruction
error as the score function for the anomaly detection.

The reconstruction error is the sum of the squared differences between
the original feature matrix and the matrix reconstructed by a
dimensionality reduction algorithm. We will scale the sum of squared
differences by the max-min range of the sum of the squared differences
for the entire dataset so that the reconstruction error is in the range
between 0 and 1.

The scoring function is then:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{k}{def} \PY{n+nf}{anomalyScores}\PY{p}{(}\PY{n}{originalDF}\PY{p}{,} \PY{n}{reducedDF}\PY{p}{)}\PY{p}{:}
            \PY{n}{loss} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{originalDF}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{reducedDF}\PY{p}{)}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
            \PY{n}{loss} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{Series}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{loss}\PY{p}{,}\PY{n}{index}\PY{o}{=}\PY{n}{originalDF}\PY{o}{.}\PY{n}{index}\PY{p}{)}
            \PY{n}{loss} \PY{o}{=} \PY{p}{(}\PY{n}{loss}\PY{o}{\PYZhy{}}\PY{n}{np}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{n}{loss}\PY{p}{)}\PY{p}{)}\PY{o}{/}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{loss}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{n}{np}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{n}{loss}\PY{p}{)}\PY{p}{)}
            \PY{k}{return} \PY{n}{loss}
\end{Verbatim}


    \subsubsection{Define Evaluation}\label{define-evaluation}

As mentioned before, we will not use the available labels for the
training. Instead, we will use them for the evaluation. The metrics we
will employ are the \textbf{precision-recall curve}, the \textbf{average
precision}, and the \textbf{auROC}.

To plot the results, we will use this function:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{k}{def} \PY{n+nf}{plotResults}\PY{p}{(}\PY{n}{trueLabels}\PY{p}{,} \PY{n}{anomalyScores}\PY{p}{,} \PY{n}{returnPreds} \PY{o}{=} \PY{k+kc}{False}\PY{p}{)}\PY{p}{:}
            \PY{n}{preds} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{concat}\PY{p}{(}\PY{p}{[}\PY{n}{trueLabels}\PY{p}{,} \PY{n}{anomalyScores}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
            \PY{n}{preds}\PY{o}{.}\PY{n}{columns} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{trueLabel}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{anomalyScore}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
            \PY{n}{precision}\PY{p}{,} \PY{n}{recall}\PY{p}{,} \PY{n}{thresholds} \PY{o}{=} \PYZbs{}
                \PY{n}{precision\PYZus{}recall\PYZus{}curve}\PY{p}{(}\PY{n}{preds}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{trueLabel}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}\PY{n}{preds}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{anomalyScore}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
            \PY{n}{average\PYZus{}precision} \PY{o}{=} \PYZbs{}
                \PY{n}{average\PYZus{}precision\PYZus{}score}\PY{p}{(}\PY{n}{preds}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{trueLabel}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}\PY{n}{preds}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{anomalyScore}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
            
            \PY{n}{plt}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{n}{recall}\PY{p}{,} \PY{n}{precision}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{k}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.7}\PY{p}{,} \PY{n}{where}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{post}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{fill\PYZus{}between}\PY{p}{(}\PY{n}{recall}\PY{p}{,} \PY{n}{precision}\PY{p}{,} \PY{n}{step}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{post}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.3}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{k}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        
            \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Recall}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Precision}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{ylim}\PY{p}{(}\PY{p}{[}\PY{l+m+mf}{0.0}\PY{p}{,} \PY{l+m+mf}{1.05}\PY{p}{]}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{xlim}\PY{p}{(}\PY{p}{[}\PY{l+m+mf}{0.0}\PY{p}{,} \PY{l+m+mf}{1.0}\PY{p}{]}\PY{p}{)}
            
            \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Precision\PYZhy{}Recall curve: Average Precision = }\PY{l+s+se}{\PYZbs{}}
        \PY{l+s+s1}{    }\PY{l+s+si}{\PYZob{}0:0.2f\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{average\PYZus{}precision}\PY{p}{)}\PY{p}{)}
        
            \PY{n}{fpr}\PY{p}{,} \PY{n}{tpr}\PY{p}{,} \PY{n}{thresholds} \PY{o}{=} \PY{n}{roc\PYZus{}curve}\PY{p}{(}\PY{n}{preds}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{trueLabel}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PYZbs{}
                                             \PY{n}{preds}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{anomalyScore}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
            \PY{n}{areaUnderROC} \PY{o}{=} \PY{n}{auc}\PY{p}{(}\PY{n}{fpr}\PY{p}{,} \PY{n}{tpr}\PY{p}{)}
        
            \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{fpr}\PY{p}{,} \PY{n}{tpr}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{lw}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ROC curve}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{k}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{lw}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{linestyle}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{xlim}\PY{p}{(}\PY{p}{[}\PY{l+m+mf}{0.0}\PY{p}{,} \PY{l+m+mf}{1.0}\PY{p}{]}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{ylim}\PY{p}{(}\PY{p}{[}\PY{l+m+mf}{0.0}\PY{p}{,} \PY{l+m+mf}{1.05}\PY{p}{]}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{False Positive Rate}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{True Positive Rate}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Receiver operating characteristic: }\PY{l+s+se}{\PYZbs{}}
        \PY{l+s+s1}{    Area under the curve = }\PY{l+s+si}{\PYZob{}0:0.2f\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{areaUnderROC}\PY{p}{)}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{lower right}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
            
            \PY{k}{if} \PY{n}{returnPreds}\PY{o}{==}\PY{k+kc}{True}\PY{p}{:}
                \PY{k}{return} \PY{n}{preds}
\end{Verbatim}


    \subsubsection{Note on the metrics}\label{note-on-the-metrics}

These metrics will help us to asses how good the unsupervised fraud
detection systems are at catching \textbf{known} pattern of fraud -
fraud that we have caught in the past and have labels for.

We will not be able to assess how good the unsupervised learning systems
are at catching \textbf{unknown} patterns of fraud. In other words, the
current dataset may have transactions that are fraudulent but have never
been discovered and are not labeled as a fraud.

Unfortunately, we cannot go to the financial company and ask them to
evaluate new instances of fraud our unsupervised system might find. This
shows, how difficult it really is to evaluate unsupervised systems by
using only already known patterns.

    \subsubsection{Define Plotting Function}\label{define-plotting-function}

This function will generate a scatter plot to display the separation of
points, the dimensionality reduction algorithms achieve in just two
dimensions.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{k}{def} \PY{n+nf}{scatterPlot}\PY{p}{(}\PY{n}{xDF}\PY{p}{,} \PY{n}{yDF}\PY{p}{,} \PY{n}{algoName}\PY{p}{)}\PY{p}{:}
            \PY{n}{tempDF} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{xDF}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{xDF}\PY{o}{.}\PY{n}{index}\PY{p}{)}
            \PY{n}{tempDF} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{concat}\PY{p}{(}\PY{p}{(}\PY{n}{tempDF}\PY{p}{,}\PY{n}{yDF}\PY{p}{)}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{join}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{inner}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
            \PY{n}{tempDF}\PY{o}{.}\PY{n}{columns} \PY{o}{=} \PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{First Vector}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Second Vector}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Label}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}
            \PY{n}{sns}\PY{o}{.}\PY{n}{lmplot}\PY{p}{(}\PY{n}{x}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{First Vector}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{y}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Second Vector}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{hue}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Label}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PYZbs{}
                       \PY{n}{data}\PY{o}{=}\PY{n}{tempDF}\PY{p}{,} \PY{n}{fit\PYZus{}reg}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
            \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{gca}\PY{p}{(}\PY{p}{)}
            \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Separation of Observations using }\PY{l+s+s2}{\PYZdq{}}\PY{o}{+}\PY{n}{algoName}\PY{p}{)}
\end{Verbatim}


    \subsection{Normal PCA Anomaly
Detection}\label{normal-pca-anomaly-detection}

We have seen, how powerful PCA can be when applied on the MNIST
handwritten digits dataset. It made it possible to visually separate
digits by using only two dimensions.

Now, we will let PCA learn the underlying structure of the credit card
transactions dataset. We remember that the available dataset has already
been processed by a PCA algorithm. However, it is not unusual to perform
a PCA on already dimensionally-reduced datasets.

When designing an anomaly detection system using a
dimensionality-reduction algorithm, we must keep in mind that the goal
is to have the highest reconstruction error for dataset entries that are
anomalous, i.e. fraudulent. The genuine transactions have to have the
reconstruction error as low as possible.

For PCA the reconstruction error depends on how many dimensions we are
going to keep. The lowest error will be when all dimensions are kept.
But in this case, we may not be able to distinguish fraud. If we keep to
few dimensions, the PCA might not be able to reconstruct the dataset
well enough and again the hight reconstruction error across the entire
dataset will not allow us to see what transactions are fraudulent. We
will have to find an optimum number of dimensions.

To demonstrate this point, we first try to apply PCA while keeping all
30 features.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{c+c1}{\PYZsh{} 30 principal components}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{decomposition} \PY{k}{import} \PY{n}{PCA}
        
        \PY{n}{n\PYZus{}components} \PY{o}{=} \PY{l+m+mi}{30}
        \PY{n}{whiten} \PY{o}{=} \PY{k+kc}{False}
        \PY{n}{random\PYZus{}state} \PY{o}{=} \PY{l+m+mi}{2018}
        
        \PY{n}{pca} \PY{o}{=} \PY{n}{PCA}\PY{p}{(}\PY{n}{n\PYZus{}components}\PY{o}{=}\PY{n}{n\PYZus{}components}\PY{p}{,} \PY{n}{whiten}\PY{o}{=}\PY{n}{whiten}\PY{p}{,} \PYZbs{}
                  \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{random\PYZus{}state}\PY{p}{)}
        
        \PY{n}{X\PYZus{}train\PYZus{}PCA} \PY{o}{=} \PY{n}{pca}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
        \PY{n}{X\PYZus{}train\PYZus{}PCA} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}train\PYZus{}PCA}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{index}\PY{p}{)}
        
        \PY{n}{X\PYZus{}train\PYZus{}PCA\PYZus{}inverse} \PY{o}{=} \PY{n}{pca}\PY{o}{.}\PY{n}{inverse\PYZus{}transform}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}PCA}\PY{p}{)}
        \PY{n}{X\PYZus{}train\PYZus{}PCA\PYZus{}inverse} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}train\PYZus{}PCA\PYZus{}inverse}\PY{p}{,} \PYZbs{}
                                           \PY{n}{index}\PY{o}{=}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{index}\PY{p}{)}
        
        \PY{n}{scatterPlot}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}PCA}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{PCA}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_11_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Now, let's calculate the precision-recall curve and ROC:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{n}{anomalyScoresPCA} \PY{o}{=} \PY{n}{anomalyScores}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}train\PYZus{}PCA\PYZus{}inverse}\PY{p}{)}
        \PY{n}{preds} \PY{o}{=} \PY{n}{plotResults}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{anomalyScoresPCA}\PY{p}{,} \PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_13_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_13_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    With an average precision of 0.08, this is a very poor solution for
fraud detection.

\subsubsection{Search for the Optimal Number of Principal
Components}\label{search-for-the-optimal-number-of-principal-components}

Let's reduce the number of principal components and evaluate the
results. We do not want the situation when the error is to low or too
high so that the rare and the normal transaction are indistinguishable.

We can experiment with different numbers of principal components. It
appears that 27 perform the best:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{c+c1}{\PYZsh{} 27 principal components}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{decomposition} \PY{k}{import} \PY{n}{PCA}
        
        \PY{n}{n\PYZus{}components} \PY{o}{=} \PY{l+m+mi}{27}
        \PY{n}{whiten} \PY{o}{=} \PY{k+kc}{False}
        \PY{n}{random\PYZus{}state} \PY{o}{=} \PY{l+m+mi}{2018}
        
        \PY{n}{pca} \PY{o}{=} \PY{n}{PCA}\PY{p}{(}\PY{n}{n\PYZus{}components}\PY{o}{=}\PY{n}{n\PYZus{}components}\PY{p}{,} \PY{n}{whiten}\PY{o}{=}\PY{n}{whiten}\PY{p}{,} \PYZbs{}
                  \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{random\PYZus{}state}\PY{p}{)}
        
        \PY{n}{X\PYZus{}train\PYZus{}PCA} \PY{o}{=} \PY{n}{pca}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
        \PY{n}{X\PYZus{}train\PYZus{}PCA} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}train\PYZus{}PCA}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{index}\PY{p}{)}
        
        \PY{n}{X\PYZus{}train\PYZus{}PCA\PYZus{}inverse} \PY{o}{=} \PY{n}{pca}\PY{o}{.}\PY{n}{inverse\PYZus{}transform}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}PCA}\PY{p}{)}
        \PY{n}{X\PYZus{}train\PYZus{}PCA\PYZus{}inverse} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}train\PYZus{}PCA\PYZus{}inverse}\PY{p}{,} \PYZbs{}
                                           \PY{n}{index}\PY{o}{=}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{index}\PY{p}{)}
        
        \PY{n}{scatterPlot}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}PCA}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{PCA}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_15_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{n}{anomalyScoresPCA} \PY{o}{=} \PY{n}{anomalyScores}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}train\PYZus{}PCA\PYZus{}inverse}\PY{p}{)}
         \PY{n}{preds} \PY{o}{=} \PY{n}{plotResults}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{anomalyScoresPCA}\PY{p}{,} \PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_16_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_16_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    We can see that the system can catch 80\% of fraud with 75\% precision.
Given that we did not use any labels, this is impressive. Particularly,
when we recall that we have inspected 190820 transactions of which only
330 are fraudulent.

The PCA gives us the reconstruction error for each of these 190820
transactions. If we sort the transaction by their reconstruction error
(anomaly score) in descending order and cut 350 topmost, we will be able
to see that 264 of them are actually fraudulent.

This is the precision of 75\%. The 264 caught transaction represent 80\%
of the true 350 fraudulent entries. This is 80\% recall.

This is the code that does this:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}32}]:} \PY{n}{preds}\PY{o}{.}\PY{n}{sort\PYZus{}values}\PY{p}{(}\PY{n}{by}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{anomalyScore}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{ascending}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,}\PY{n}{inplace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
         \PY{n}{cutoff} \PY{o}{=} \PY{l+m+mi}{350}
         \PY{n}{predsTop} \PY{o}{=} \PY{n}{preds}\PY{p}{[}\PY{p}{:}\PY{n}{cutoff}\PY{p}{]}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Precision: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{round}\PY{p}{(}
             \PY{n}{predsTop}\PY{o}{.}\PY{n}{anomalyScore}\PY{p}{[}\PY{n}{predsTop}\PY{o}{.}\PY{n}{trueLabel}\PY{o}{==}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{count}\PY{p}{(}\PY{p}{)}\PY{o}{/}\PY{n}{cutoff}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Recall: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{round}\PY{p}{(}
             \PY{n}{predsTop}\PY{o}{.}\PY{n}{anomalyScore}\PY{p}{[}\PY{n}{predsTop}\PY{o}{.}\PY{n}{trueLabel}\PY{o}{==}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{count}\PY{p}{(}\PY{p}{)} \PYZbs{}
             \PY{o}{/} \PY{n}{y\PYZus{}train}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Fraud Caught out of 350 Cases: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{predsTop}\PY{o}{.}\PY{n}{trueLabel}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Precision:  0.75
Recall:  0.8
Fraud Caught out of 350 Cases:  264

    \end{Verbatim}

    \subsection{Sparse PCA Anomaly
Detection}\label{sparse-pca-anomaly-detection}

Recall that Sparse PCA delivers a less dense solution than the normal
PCA. We will need to specify the number of components and the parameter
\textbf{alpha} that controls the degree of sparsity. We will have to
experiment with different values of these two parameters for the optimal
result.

The normal PCA has the \texttt{inverse\_transform} function that
reconstructs the dataset. The Sparse PCA does not have it, so we will
have to implement it ourselves.

Let's begin the Sparse PCA with 27 principal components and the default
alpha value of 0.0001:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{decomposition} \PY{k}{import} \PY{n}{SparsePCA}
         
         \PY{n}{n\PYZus{}components} \PY{o}{=} \PY{l+m+mi}{27}
         \PY{n}{alpha} \PY{o}{=} \PY{l+m+mf}{0.0001}
         \PY{n}{random\PYZus{}state} \PY{o}{=} \PY{l+m+mi}{2018}
         \PY{n}{n\PYZus{}jobs} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}
         
         \PY{n}{sparsePCA} \PY{o}{=} \PY{n}{SparsePCA}\PY{p}{(}\PY{n}{n\PYZus{}components}\PY{o}{=}\PY{n}{n\PYZus{}components}\PY{p}{,} \PYZbs{}
                              \PY{n}{alpha}\PY{o}{=}\PY{n}{alpha}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{random\PYZus{}state}\PY{p}{,} \PYZbs{}
                              \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{n}{n\PYZus{}jobs}\PY{p}{)}
         
         \PY{n}{sparsePCA}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{p}{:}\PY{p}{]}\PY{p}{)}
         \PY{n}{X\PYZus{}train\PYZus{}sparsePCA} \PY{o}{=} \PY{n}{sparsePCA}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
         \PY{n}{X\PYZus{}train\PYZus{}sparsePCA} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}train\PYZus{}sparsePCA}\PY{p}{,}
                                         \PY{n}{index}\PY{o}{=}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{index}\PY{p}{)}
         
         \PY{n}{scatterPlot}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}sparsePCA}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Sparse PCA}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_20_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Below you will see the code that reconstructs the reduced dataset to the
original features by matrix multiplication: the sparse PCA matrix
(\(190820 \times 27\)) and the sparse PCA components (\(27 \times 30\)).
We then add to the restored features their respective mean.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{n}{X\PYZus{}train\PYZus{}sparsePCA\PYZus{}inverse} \PY{o}{=} \PYZbs{}
         \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}sparsePCA}\PY{p}{)}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{sparsePCA}\PY{o}{.}\PY{n}{components\PYZus{}}\PY{p}{)} \PYZbs{}
         \PY{o}{+} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{X\PYZus{}train\PYZus{}PCA\PYZus{}inverse} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}train\PYZus{}PCA\PYZus{}inverse}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{index}\PY{p}{)}
         
         \PY{n}{anomalyScoresSparsePCA} \PY{o}{=} \PY{n}{anomalyScores}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}train\PYZus{}sparsePCA\PYZus{}inverse}\PY{p}{)}
         \PY{n}{preds} \PY{o}{=} \PY{n}{plotResults}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{anomalyScoresSparsePCA}\PY{p}{,} \PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_22_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_22_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    The results are identical to those of the normal PCA. This is expected
because normal and sparse PCA are very similar - the latter is just a
sparse representation of the former.

    \subsection{Kernel PCA Anomaly
Detection}\label{kernel-pca-anomaly-detection}

We can use a non-linear dimensionality reduction with kernel PCA. This
can be useful if the fraudulent cases are not linearly separable from
the genuine ones.

We will need to specify the number of components, the kernel (in our
case, 'rbf'), and the gamma (which is set to \(\frac{1}{n\_features}\)
by default, or, in our case, 1/30). We will need to set
\texttt{fit\_inverse\_transform} to \texttt{true} to apply the built-in
inverse transform function.

Kernel PCA is expensive to train, so we will train on just the first 200
samples.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{c+c1}{\PYZsh{} Kernel PCA}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{decomposition} \PY{k}{import} \PY{n}{KernelPCA}
         
         \PY{n}{n\PYZus{}components} \PY{o}{=} \PY{l+m+mi}{27}
         \PY{n}{kernel} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rbf}\PY{l+s+s1}{\PYZsq{}}
         \PY{n}{gamma} \PY{o}{=} \PY{k+kc}{None}
         \PY{n}{fit\PYZus{}inverse\PYZus{}transform} \PY{o}{=} \PY{k+kc}{True}
         \PY{n}{random\PYZus{}state} \PY{o}{=} \PY{l+m+mi}{2018}
         \PY{n}{n\PYZus{}jobs} \PY{o}{=} \PY{l+m+mi}{1}
         
         \PY{n}{kernelPCA} \PY{o}{=} \PY{n}{KernelPCA}\PY{p}{(}\PY{n}{n\PYZus{}components}\PY{o}{=}\PY{n}{n\PYZus{}components}\PY{p}{,} \PY{n}{kernel}\PY{o}{=}\PY{n}{kernel}\PY{p}{,} \PYZbs{}
                         \PY{n}{gamma}\PY{o}{=}\PY{n}{gamma}\PY{p}{,} \PY{n}{fit\PYZus{}inverse\PYZus{}transform}\PY{o}{=} \PYZbs{}
                         \PY{n}{fit\PYZus{}inverse\PYZus{}transform}\PY{p}{,} \PY{n}{n\PYZus{}jobs}\PY{o}{=}\PY{n}{n\PYZus{}jobs}\PY{p}{,} \PYZbs{}
                         \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{random\PYZus{}state}\PY{p}{)}
         
         \PY{n}{kernelPCA}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{200}\PY{p}{]}\PY{p}{)}
         \PY{n}{X\PYZus{}train\PYZus{}kernelPCA} \PY{o}{=} \PY{n}{kernelPCA}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
         \PY{n}{X\PYZus{}train\PYZus{}kernelPCA} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}train\PYZus{}kernelPCA}\PY{p}{,} \PYZbs{}
                                          \PY{n}{index}\PY{o}{=}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{index}\PY{p}{)}
         
         \PY{n}{X\PYZus{}train\PYZus{}kernelPCA\PYZus{}inverse} \PY{o}{=} \PY{n}{kernelPCA}\PY{o}{.}\PY{n}{inverse\PYZus{}transform}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}kernelPCA}\PY{p}{)}
         \PY{n}{X\PYZus{}train\PYZus{}kernelPCA\PYZus{}inverse} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}train\PYZus{}kernelPCA\PYZus{}inverse}\PY{p}{,} \PYZbs{}
                                                  \PY{n}{index}\PY{o}{=}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{index}\PY{p}{)}
         
         \PY{n}{scatterPlot}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}kernelPCA}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Kernel PCA}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_25_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{n}{anomalyScoresKernelPCA} \PY{o}{=} \PY{n}{anomalyScores}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}train\PYZus{}kernelPCA\PYZus{}inverse}\PY{p}{)}
         \PY{n}{preds} \PY{o}{=} \PY{n}{plotResults}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{anomalyScoresKernelPCA}\PY{p}{,} \PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_26_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_26_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    As we can see, the results provided by kernel PCA are far worse than
those from normal and sparse PCA. It is useful to experiment with
different algorithms, but we will not use kernel PCA for the final fraud
detection solution.

\subsection{Gaussian Random Projection Anomaly
Detection}\label{gaussian-random-projection-anomaly-detection}

Now, let's try to implement anomaly detection with Gaussian random
projection. Here, we will have to either set the desired number of
components or use the \emph{eps} parameter that controls the quality of
the embedding.

For now, we choose to set the number of components explicitly. Gaussian
random projections train quickly, so we can train on the entire training
set.

As with sparse PCA, we will have to derive our own inverse
transformation function.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{c+c1}{\PYZsh{} Gaussian Random Projection}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{random\PYZus{}projection} \PY{k}{import} \PY{n}{GaussianRandomProjection}
         
         \PY{n}{n\PYZus{}components} \PY{o}{=} \PY{l+m+mi}{27}
         \PY{n}{eps} \PY{o}{=} \PY{k+kc}{None}
         \PY{n}{random\PYZus{}state} \PY{o}{=} \PY{l+m+mi}{2018}
         
         \PY{n}{GRP} \PY{o}{=} \PY{n}{GaussianRandomProjection}\PY{p}{(}\PY{n}{n\PYZus{}components}\PY{o}{=}\PY{n}{n\PYZus{}components}\PY{p}{,} \PYZbs{}
                                        \PY{n}{eps}\PY{o}{=}\PY{n}{eps}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{random\PYZus{}state}\PY{p}{)}
         
         \PY{n}{X\PYZus{}train\PYZus{}GRP} \PY{o}{=} \PY{n}{GRP}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
         \PY{n}{X\PYZus{}train\PYZus{}GRP} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}train\PYZus{}GRP}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{index}\PY{p}{)}
         
         \PY{n}{scatterPlot}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}GRP}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Gaussian Random Projection}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_28_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} \PY{n}{X\PYZus{}train\PYZus{}GRP\PYZus{}inverse} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}GRP}\PY{p}{)}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{GRP}\PY{o}{.}\PY{n}{components\PYZus{}}\PY{p}{)}
         \PY{n}{X\PYZus{}train\PYZus{}GRP\PYZus{}inverse} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}train\PYZus{}GRP\PYZus{}inverse}\PY{p}{,} \PYZbs{}
                                            \PY{n}{index}\PY{o}{=}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{index}\PY{p}{)}
         
         \PY{n}{anomalyScoresGRP} \PY{o}{=} \PY{n}{anomalyScores}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}train\PYZus{}GRP\PYZus{}inverse}\PY{p}{)}
         \PY{n}{preds} \PY{o}{=} \PY{n}{plotResults}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{anomalyScoresGRP}\PY{p}{,} \PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_29_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_29_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    These results are not good enough and Gaussian random projection is not
a suitable candidate for the final solution.

    \subsection{Sparse Random Projection Anomaly
Detection}\label{sparse-random-projection-anomaly-detection}

Now, let's try sparse random projection for our task. We will set the
desired number of parameters and will have to implement our own inverse
transformation.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}17}]:} \PY{c+c1}{\PYZsh{} Sparse Random Projection}
         
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{random\PYZus{}projection} \PY{k}{import} \PY{n}{SparseRandomProjection}
         
         \PY{n}{n\PYZus{}components} \PY{o}{=} \PY{l+m+mi}{27}
         \PY{n}{density} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{auto}\PY{l+s+s1}{\PYZsq{}}
         \PY{n}{eps} \PY{o}{=} \PY{o}{.}\PY{l+m+mi}{01}
         \PY{n}{dense\PYZus{}output} \PY{o}{=} \PY{k+kc}{True}
         \PY{n}{random\PYZus{}state} \PY{o}{=} \PY{l+m+mi}{2018}
         
         \PY{n}{SRP} \PY{o}{=} \PY{n}{SparseRandomProjection}\PY{p}{(}\PY{n}{n\PYZus{}components}\PY{o}{=}\PY{n}{n\PYZus{}components}\PY{p}{,} \PYZbs{}
                 \PY{n}{density}\PY{o}{=}\PY{n}{density}\PY{p}{,} \PY{n}{eps}\PY{o}{=}\PY{n}{eps}\PY{p}{,} \PY{n}{dense\PYZus{}output}\PY{o}{=}\PY{n}{dense\PYZus{}output}\PY{p}{,} \PYZbs{}
                                         \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{random\PYZus{}state}\PY{p}{)}
         
         \PY{n}{X\PYZus{}train\PYZus{}SRP} \PY{o}{=} \PY{n}{SRP}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
         \PY{n}{X\PYZus{}train\PYZus{}SRP} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}train\PYZus{}SRP}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{index}\PY{p}{)}
         
         \PY{n}{scatterPlot}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}SRP}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Sparse Random Projection}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_32_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} \PY{n}{X\PYZus{}train\PYZus{}SRP\PYZus{}inverse} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}SRP}\PY{p}{)}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{SRP}\PY{o}{.}\PY{n}{components\PYZus{}}\PY{o}{.}\PY{n}{todense}\PY{p}{(}\PY{p}{)}\PY{p}{)}
         \PY{n}{X\PYZus{}train\PYZus{}SRP\PYZus{}inverse} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}train\PYZus{}SRP\PYZus{}inverse}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{index}\PY{p}{)}
         
         \PY{n}{anomalyScoresSRP} \PY{o}{=} \PY{n}{anomalyScores}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}train\PYZus{}SRP\PYZus{}inverse}\PY{p}{)}
         \PY{n}{plotResults}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{anomalyScoresSRP}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_33_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_33_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Another set of poor results. We turn sparse random projection down.

    \subsection{Nonlinear Anomaly
Detection}\label{nonlinear-anomaly-detection}

Until now we have tried several methods based on linear dimensionality
reduction: normal PCA, sparse PCA, Gaussian random projection and sparse
random projection. We also tried a non-linear method: kernel PCA. Out of
those the best results were given by PCA.

The non-linear (manifold learning) methods that are available as
open-source run very slowly, so we will have to skip them and try the
non-distance methods.

\subsection{Dictionary Learning Anomaly
Detection}\label{dictionary-learning-anomaly-detection}

Recall that with dictionary learning the algorithm creates a sparse
representation of the original data in a form of atom vectors in a
dictionary. We can use these atom vectors to reconstruct the original
dataset.

For the dimensionality reduction, we need to learn an undercomplete
dictionary. In our case, we will generate 28 vectors. We will feed the
learning algorithm in 10 batches each consisting of 200 samples.

We will have to use our own inverse transformation function.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{c+c1}{\PYZsh{} Mini\PYZhy{}batch dictionary learning}
         \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{decomposition} \PY{k}{import} \PY{n}{MiniBatchDictionaryLearning}
         
         \PY{n}{n\PYZus{}components} \PY{o}{=} \PY{l+m+mi}{28}
         \PY{n}{alpha} \PY{o}{=} \PY{l+m+mi}{1}
         \PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{200}
         \PY{n}{n\PYZus{}iter} \PY{o}{=} \PY{l+m+mi}{10}
         \PY{n}{random\PYZus{}state} \PY{o}{=} \PY{l+m+mi}{2018}
         
         \PY{n}{miniBatchDictLearning} \PY{o}{=} \PY{n}{MiniBatchDictionaryLearning}\PY{p}{(} \PYZbs{}
             \PY{n}{n\PYZus{}components}\PY{o}{=}\PY{n}{n\PYZus{}components}\PY{p}{,} \PY{n}{alpha}\PY{o}{=}\PY{n}{alpha}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{n}{batch\PYZus{}size}\PY{p}{,} \PYZbs{}
             \PY{n}{n\PYZus{}iter}\PY{o}{=}\PY{n}{n\PYZus{}iter}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{random\PYZus{}state}\PY{p}{)}
         
         \PY{n}{miniBatchDictLearning}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
         \PY{n}{X\PYZus{}train\PYZus{}miniBatchDictLearning} \PY{o}{=} \PYZbs{}
             \PY{n}{miniBatchDictLearning}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
         \PY{n}{X\PYZus{}train\PYZus{}miniBatchDictLearning} \PY{o}{=} \PYZbs{}
             \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}train\PYZus{}miniBatchDictLearning}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{index}\PY{p}{)}
         
         \PY{n}{scatterPlot}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}miniBatchDictLearning}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PYZbs{}
                     \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Mini\PYZhy{}batch Dictionary Learning}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_36_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}20}]:} \PY{n}{X\PYZus{}train\PYZus{}miniBatchDictLearning\PYZus{}inverse} \PY{o}{=} \PYZbs{}
             \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}miniBatchDictLearning}\PY{p}{)}\PY{o}{.} \PYZbs{}
             \PY{n}{dot}\PY{p}{(}\PY{n}{miniBatchDictLearning}\PY{o}{.}\PY{n}{components\PYZus{}}\PY{p}{)}
         
         \PY{n}{X\PYZus{}train\PYZus{}miniBatchDictLearning\PYZus{}inverse} \PY{o}{=} \PYZbs{}
             \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}train\PYZus{}miniBatchDictLearning\PYZus{}inverse}\PY{p}{,} \PYZbs{}
                          \PY{n}{index}\PY{o}{=}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{index}\PY{p}{)}
         
         \PY{n}{anomalyScoresMiniBatchDictLearning} \PY{o}{=} \PY{n}{anomalyScores}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PYZbs{}
             \PY{n}{X\PYZus{}train\PYZus{}miniBatchDictLearning\PYZus{}inverse}\PY{p}{)}
         \PY{n}{preds} \PY{o}{=} \PY{n}{plotResults}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{anomalyScoresMiniBatchDictLearning}\PY{p}{,} \PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_37_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_37_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    The results from the dictionary learning are much better than from
random projections but still are worse than those of PCA.

    \subsection{ICA Anomaly Detection}\label{ica-anomaly-detection}

Now, let's try the independent component analysis (ICA) for fraud
detection. We will need to specify the number of components, which we
set to 27. ICA has its own \texttt{inverse\_transform} function.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}21}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{decomposition} \PY{k}{import} \PY{n}{FastICA}
         
         \PY{n}{n\PYZus{}components} \PY{o}{=} \PY{l+m+mi}{27}
         \PY{n}{algorithm} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{parallel}\PY{l+s+s1}{\PYZsq{}}
         \PY{n}{whiten} \PY{o}{=} \PY{k+kc}{True}
         \PY{n}{max\PYZus{}iter} \PY{o}{=} \PY{l+m+mi}{200}
         \PY{n}{random\PYZus{}state} \PY{o}{=} \PY{l+m+mi}{2018}
         
         \PY{n}{fastICA} \PY{o}{=} \PY{n}{FastICA}\PY{p}{(}\PY{n}{n\PYZus{}components}\PY{o}{=}\PY{n}{n\PYZus{}components}\PY{p}{,} \PYZbs{}
             \PY{n}{algorithm}\PY{o}{=}\PY{n}{algorithm}\PY{p}{,} \PY{n}{whiten}\PY{o}{=}\PY{n}{whiten}\PY{p}{,} \PY{n}{max\PYZus{}iter}\PY{o}{=}\PY{n}{max\PYZus{}iter}\PY{p}{,} \PYZbs{}
             \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{random\PYZus{}state}\PY{p}{)}
         
         \PY{n}{X\PYZus{}train\PYZus{}fastICA} \PY{o}{=} \PY{n}{fastICA}\PY{o}{.}\PY{n}{fit\PYZus{}transform}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
         \PY{n}{X\PYZus{}train\PYZus{}fastICA} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}train\PYZus{}fastICA}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{index}\PY{p}{)}
         
         \PY{n}{X\PYZus{}train\PYZus{}fastICA\PYZus{}inverse} \PY{o}{=} \PY{n}{fastICA}\PY{o}{.}\PY{n}{inverse\PYZus{}transform}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}fastICA}\PY{p}{)}
         \PY{n}{X\PYZus{}train\PYZus{}fastICA\PYZus{}inverse} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}train\PYZus{}fastICA\PYZus{}inverse}\PY{p}{,} \PYZbs{}
                                                \PY{n}{index}\PY{o}{=}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{index}\PY{p}{)}
         
         \PY{n}{scatterPlot}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}fastICA}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Independent Component Analysis}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_40_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}22}]:} \PY{n}{anomalyScoresFastICA} \PY{o}{=} \PY{n}{anomalyScores}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}train\PYZus{}fastICA\PYZus{}inverse}\PY{p}{)}
         \PY{n}{plotResults}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{anomalyScoresFastICA}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_41_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_41_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    These results are close to those of the normal PCA. The detecting fraud
using ICA is a plausible alternative to PCA.

\subsection{Fraud Detection of the Test
Set}\label{fraud-detection-of-the-test-set}

Now, it is time to evaluate the fraud detection solution of the test set
that contains data never seen before by the algorithms. We evaluate the
three best solutions:

\begin{itemize}
\tightlist
\item
  normal PCA
\item
  ICA
\item
  dictionary learning
\end{itemize}

\subsubsection{Normal PCA and the Test
Set}\label{normal-pca-and-the-test-set}

We will use the PCA embedding that the PCA algorithm has learned from
the training set and use it to transform the test set. Then, we will use
the inverse transformation o recreate the original dimensions of the
test set.

By comparing the original test set and the reconstructed version we can
calculate the anomaly scores:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}23}]:} \PY{n}{X\PYZus{}test\PYZus{}PCA} \PY{o}{=} \PY{n}{pca}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
         \PY{n}{X\PYZus{}test\PYZus{}PCA} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}test\PYZus{}PCA}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{index}\PY{p}{)}
         
         \PY{n}{X\PYZus{}test\PYZus{}PCA\PYZus{}inverse} \PY{o}{=} \PY{n}{pca}\PY{o}{.}\PY{n}{inverse\PYZus{}transform}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}PCA}\PY{p}{)}
         \PY{n}{X\PYZus{}test\PYZus{}PCA\PYZus{}inverse} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}test\PYZus{}PCA\PYZus{}inverse}\PY{p}{,}
                                           \PY{n}{index}\PY{o}{=}\PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{index}\PY{p}{)}
         
         \PY{n}{scatterPlot}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}PCA}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{PCA}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_43_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}24}]:} \PY{n}{anomalyScoresPCA} \PY{o}{=} \PY{n}{anomalyScores}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{X\PYZus{}test\PYZus{}PCA\PYZus{}inverse}\PY{p}{)}
         \PY{n}{preds} \PY{o}{=} \PY{n}{plotResults}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{anomalyScoresPCA}\PY{p}{,} \PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_44_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_44_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    The results are very good: we can catch 80\% of fraud with 80\%
precision - all without using any labels.

    \subsubsection{ICA Anomaly Detection of the Test
Set}\label{ica-anomaly-detection-of-the-test-set}

Our next evaluation is done for the ICA solution:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}25}]:} \PY{c+c1}{\PYZsh{} Independent Component Analysis on Test Set}
         \PY{n}{X\PYZus{}test\PYZus{}fastICA} \PY{o}{=} \PY{n}{fastICA}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
         \PY{n}{X\PYZus{}test\PYZus{}fastICA} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}test\PYZus{}fastICA}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{index}\PY{p}{)}
         
         \PY{n}{X\PYZus{}test\PYZus{}fastICA\PYZus{}inverse} \PY{o}{=} \PY{n}{fastICA}\PY{o}{.}\PY{n}{inverse\PYZus{}transform}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}fastICA}\PY{p}{)}
         \PY{n}{X\PYZus{}test\PYZus{}fastICA\PYZus{}inverse} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}test\PYZus{}fastICA\PYZus{}inverse}\PY{p}{,} \PYZbs{}
                                               \PY{n}{index}\PY{o}{=}\PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{index}\PY{p}{)}
         
         \PY{n}{scatterPlot}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}fastICA}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Independent Component Analysis}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_47_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}26}]:} \PY{n}{anomalyScoresFastICA} \PY{o}{=} \PY{n}{anomalyScores}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{X\PYZus{}test\PYZus{}fastICA\PYZus{}inverse}\PY{p}{)}
         \PY{n}{plotResults}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{anomalyScoresFastICA}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_48_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_48_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    We get the same good results as with the normal PCA.

\subsubsection{Dictionary Learning Anomaly Detecion of the Test
Set}\label{dictionary-learning-anomaly-detecion-of-the-test-set}

Finally, we will test the dictionary learning solution:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}27}]:} \PY{n}{X\PYZus{}test\PYZus{}miniBatchDictLearning} \PY{o}{=} \PY{n}{miniBatchDictLearning}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
         \PY{n}{X\PYZus{}test\PYZus{}miniBatchDictLearning} \PY{o}{=} \PYZbs{}
             \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}test\PYZus{}miniBatchDictLearning}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{index}\PY{p}{)}
         
         \PY{n}{scatterPlot}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}miniBatchDictLearning}\PY{p}{,} \PY{n}{y\PYZus{}test}\PY{p}{,} \PYZbs{}
                     \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Mini\PYZhy{}batch Dictionary Learning}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_50_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}28}]:} \PY{n}{X\PYZus{}test\PYZus{}miniBatchDictLearning\PYZus{}inverse} \PY{o}{=} \PYZbs{}
             \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}miniBatchDictLearning}\PY{p}{)}\PY{o}{.} \PYZbs{}
             \PY{n}{dot}\PY{p}{(}\PY{n}{miniBatchDictLearning}\PY{o}{.}\PY{n}{components\PYZus{}}\PY{p}{)}
         
         \PY{n}{X\PYZus{}test\PYZus{}miniBatchDictLearning\PYZus{}inverse} \PY{o}{=} \PYZbs{}
             \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{X\PYZus{}test\PYZus{}miniBatchDictLearning\PYZus{}inverse}\PY{p}{,} \PYZbs{}
                          \PY{n}{index}\PY{o}{=}\PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{index}\PY{p}{)}
         
         \PY{n}{anomalyScoresMiniBatchDictLearning} \PY{o}{=} \PY{n}{anomalyScores}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,} \PYZbs{}
             \PY{n}{X\PYZus{}test\PYZus{}miniBatchDictLearning\PYZus{}inverse}\PY{p}{)}
         \PY{n}{preds} \PY{o}{=} \PY{n}{plotResults}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{anomalyScoresMiniBatchDictLearning}\PY{p}{,} \PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_51_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_51_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    The results are below those of the previous two solutions: we can catch
80\% of fraud but only with 20\% precision.

    \subsection{Conclusion}\label{conclusion}

    In this chapter, we've built fraud detection solutions using algorithms
that did not require labeled data to train. By using this kind of
unsupervised learning we were able to find two algorithms that performed
the best: normal PCA and ICA. While their results (80\% recall with 80\%
precision) were worse than those of the best-performing supervised
learning algorithms from the previous chapter (90\% recall at 80\%
precision), these unsupervised learning solutions will perform better on
data with new patterns of fraud where the supervised learning solutions
will see their performance worsen over time.


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
